% Copyright 2021 Daniel Nikpayuk
\documentclass[twoside]{article}
\usepackage[letterpaper,left=2cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{asymptote}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% latex symbols:

\newcommand{\vn}{\ensuremath{\varnothing}}

\newcommand{\RA}{\Rightarrow}
\newcommand{\lra}{\longrightarrow}
\newcommand{\then}{\ensuremath{\quad\Longrightarrow\quad}}
\newcommand{\qmapsto}{\ensuremath{\quad \mapsto \quad}}
\newcommand{\mapsfrom}{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}

\newcommand{\defeq}{\ensuremath{\ :=\ }}
\newcommand{\qeq}{\ensuremath{\quad =\quad}}
\newcommand{\qqeq}{\ensuremath{\qquad =\qquad}}
\newcommand{\qdefeq}{\ensuremath{\quad :=\quad}}
\newcommand{\qqdefeq}{\ensuremath{\qquad :=\qquad}}

\newcommand{\quadbar}{\ensuremath{\quad |\quad}}
\newcommand{\quadcomma}{\ensuremath{\quad ,\quad}}
\newcommand{\equals}{\ensuremath{\quad =\quad}}
\newcommand{\defequals}{\ensuremath{\quad :=\quad}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

% latex markup:

\newcommand{\strong}[1]{{\bfseries #1}}
\newcommand{\bfmbox}[1]{\mbox{\bfseries #1}}
\newcommand{\pdfnote}[1]{\,\footnote{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

% latex spacing:

\newcommand{\twoquad}{\ensuremath{\quad\quad}}
\newcommand{\threequad}{\ensuremath{\quad\quad\quad}}
\newcommand{\fourquad}{\ensuremath{\quad\quad\quad\quad}}

\newcommand{\twoqquad}{\ensuremath{\qquad\qquad}}
\newcommand{\threeqquad}{\ensuremath{\qquad\qquad\qquad}}
\newcommand{\fourqquad}{\ensuremath{\qquad\qquad\qquad\qquad}}

% essay symbols:

\newcommand{\bv}[1][v]{\ensuremath{\mathbf #1}}
\newcommand{\powerset}[2][P]{\ensuremath{\mathbb{#1}(#2)}}
\newcommand{\nthps}[2][P]{\ensuremath{\mathbb{#1}^{#2}}}
\newcommand{\nthus}[2][U]{\ensuremath{\mathbb{#1}^{#2}}}
\newcommand{\psunion}[2][P]{\ensuremath{\bigcup\limits_{#2\ge 0}\mathbb{#1}^{#2}}}
\newcommand{\usunion}[2][U]{\ensuremath{\bigcup\limits_{#2\ge 0}\mathbb{#1}^{#2}}}
\newcommand{\stratified}{\ensuremath{\mathbb{U}^\infty}}
\newcommand{\of}[1]{\ensuremath{(\mathcal{#1})}}
\newcommand{\ofbb}[1]{\ensuremath{(\mathbb{#1})}}

\newcommand{\readleft}{\ensuremath{\,_{_\leftarrow}\,}}
\newcommand{\readright}{\ensuremath{\,_{_\rightarrow}\,}}

% essay formatting:

\newcommand{\mss}[1]{\ensuremath{\mbox{\scriptsize #1}}}
\newcommand{\bms}[1]{\ensuremath{_{\mbox{\bfseries\tiny #1}}}}
\newcommand{\bnms}[2]{\ensuremath{\bms{#1,}{_#2}}}

\newcommand{\tab}[1][1.125cm]{\hspace{#1}}

\newcommand{\col}[1][0ex]{& \hspace{#1}}
\newcommand{\scol}{\col[0.15cm]}
\newcommand{\lcol}{\col[0.45cm]}

\newcommand{\msbox}[1]{\ensuremath{_{\mbox{\scriptsize #1}}}}
\newcommand{\cbox}[1]{\mbox{// #1}}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1:}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

% essay functions:

\newcommand{\subcirc}[1][m]{\ensuremath{\underset{#1}{\circ}}}
\newcommand{\lcirc}{\subcirc[\ell]}
\newcommand{\rcirc}{\subcirc[r]}

\newcommand{\subfunc}[1]{\ensuremath{\langle #1\rangle}}

\newcommand{\id}{\mbox{id}}

\newcommand{\compose}{\mbox{compose}}
\newcommand{\precompose}[1]{\ensuremath{\mbox{precompose}_{#1}}}
\newcommand{\postcompose}[1]{\ensuremath{\mbox{postcompose}_{#1}}}

\newcommand{\ndopose}{\mbox{endopose}}
\newcommand{\prendopose}[1]{\ensuremath{\mbox{preendopose}_{#1}}}
\newcommand{\postndopose}[1]{\ensuremath{\mbox{postendopose}_{#1}}}

\newcommand{\lift}{\mbox{lift}}
\newcommand{\stem}{\mbox{stem}}
\newcommand{\costem}{\mbox{costem}}
\newcommand{\distem}{\mbox{distem}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{asydef}
// this comment prevents a compilation bug.

real direction(bool value)
{
	return value ? 1 : -1;
}

pair offset(pair current, string align, real x = 0.03)
{
	pair shifter =	(align == "E") ? (x, 0) :
			(align == "N") ? (0, x) :
			(align == "W") ? (-x, 0) :
			(align == "S") ? (0, -x) : (0,0);

	return shift(shifter)*current;
}

pair segment(pair current = (0,0), string align, real projection, real angle = 0)
{
	align = (align == "E") ? "EU" :
		(align == "N") ? "NL" :
		(align == "W") ? "WD" :
		(align == "S") ? "SR" : align;

	real base          = direction(substr(align, 0, 1) == "E" || substr(align, 0, 1) == "N");
	real adjacent      = direction(substr(align, 1, 1) == "R" || substr(align, 1, 1) == "U") * Tan(angle);
	pair initialVertex = (substr(align, 0, 1) == "E" || substr(align, 0, 1) == "W") ? (base, adjacent) : (adjacent, base);
	pair scaledVertex  = scale(projection) * initialVertex;
	pair shiftedVertex = shift(current)    * scaledVertex;

	return shiftedVertex;
}

//

void drawpath(path p)
{
	draw(p);

	for (int k=0; k < size(p); ++k)
	{
		dot(point(p, k));
	}
}

void safeLabel(picture pic = currentpicture, Label L, pair position, real width, real height,
	align align = NoAlign, pen textpen = currentpen, pen borderpen = currentpen,
	pen fillpen = white, filltype filltype = NoFill, string bordertype = "round")
{
	if (bordertype == "round")
	{
		pair w = position + (-width, 0);
		pair e = position + ( width, 0);
		pair n = position + (0, height);
		pair s = position + (0,-height);

		filldraw(pic, w{up}::n{right}::e{down}::s{left}::cycle, fillpen, borderpen);
	}
	else if (bordertype == "box")
	{
		pair sw = position + (-width,-height);
		pair se = position + (-width, height);
		pair nw = position + ( width,-height);
		pair ne = position + ( width, height);

		filldraw(pic, sw--se--ne--nw--cycle, fillpen, borderpen);
	}

	label(pic, L, position, align, textpen, filltype);
}


\end{asydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{C++17\\Compile Time\\Register Machines}
\author{Daniel Nikpayuk}
\date{December 13, 2021}
\pagestyle{empty}
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{figure}[h]
\centering
\includegraphics[width=1in]{../../../cc-by-nc.png}\\[0.1in]
\tiny This article is licensed under \\
\href{http://creativecommons.org/licenses/by-nc/4.0/}
{Creative Commons Attribution-NonCommercial 4.0 International.}\\[0.3in]
\end{figure}

\section*{Abstract}

The intention of this essay is to introduce a collection of specifications for implementing a system of C++17 compile
time register machines. By writing out the details in an essay style format it is also the intention here to provide
key strategies for narrowing down possible implementations further, as well as offering a narrative story in the hopes
of aiding later on in both proof-verification and debugging. It is assumed the reader has a reasonable understanding
of automata theory and finite state machines.

This essay's specification---which will be elaborated upon in the philosophy section below---can be broadly categorized
by the following design considerations:

\begin{enumerate}
\item \strong{Theory}
	\begin{enumerate}
	\item \strong{Space Design}: We will be designing a space whose objects are register machine programs.
	\item \strong{Algebra Design}: We will design such programs to be atomic, constructive, and callable.
	\end{enumerate}
\item \strong{Practice}
	\begin{enumerate}
	\item \strong{Hardware Constraints}: We will consider the most relevant hardware bottleneck designs.
	\item \strong{Software Constraints}: We will consider the most relevant software bottleneck designs.
	\item \strong{Community Constraints}: We will consider designs which ideally satisfy user-friendliness.
	\end{enumerate}
\end{enumerate}

\section*{Philosophy}

To take a step back from our \emph{specification as object}, we can restate the main goal of this essay here as that
of telling a narrative story which will eventually help us to implement a system of register machines using C++17's
toolset.\pdfnote{Implementing in C++17 is in fact the original application of this design.}

Unfortunately things are not as simple as that, or at least this is the philosophy I'm designing from: For me,
such a system of Turing complete register machines suffers from complexity, which means no single narrative story
is sufficient to describe all the patterns of interest. My compromise (and belief) is that we in fact need two
narratives to tell the design wanting to be told.

The philosophy of this essay can then be said to be a narrative stitched together from two others, which are themselves
informed by three simpler stories.

\subsection*{Story 1: A Humanities Perspective}

The first story in navigating a system of register machines is to conceptualize it using a humanist inspired
triple:\pdfnote{I suspect most readers (of this essay) are already familiar with what \emph{texts} and \emph{readers}
are within natural language contexts, but the idea of \emph{hermeneutics} might be less well known: Mostly it's the idea
that given a text, it can have more than one reading based on how you interpret it, and so there becomes a need to study
the logic of possible interpretations of texts more generally.}
$$ \{\,\mbox{text, reader, hermeneutic}\,\} $$
How does this conceptualization work?

We'll start with the \texttt{reader}: To put it most directly, register machines---as finite state machines---are expected
to be equipped with memory devices known as registers, but from our above humanist perspective we will anthropomorphize
this to in fact be our \texttt{reader}. Why? Reading is a passive act,\pdfnote{This is true even when a reader is associated
with the idea of having \emph{agency}.} which for us suggests that whatever is read is expected to change the internal
nature of the reader. This is no less true for a memory of registers to which we apply their respective state machines
accordingly, and for which we expect those registers to be mutated as they ``read'' their given programs.

With this orientation, we can now associate our idea of a \texttt{text} with register machine programs---though often
we tend to focus more specifically on program \emph{controllers} when no confusion arises.\pdfnote{The reason we don't
associate texts directly with program controllers is that texts also have additional information we tend to take for
granted but for which we otherwise shouldn't actually forget about---such as where we start, or where we currently are
in our reading, etc.} This is reasonable as texts are fixed and unchanging, as are programs, and both are read to create
changes in their respective readers.

All that's left to conceptualize in our triple then is the \texttt{hermeneutic}, which is our given reading of the text.
How do we qualify this? In our register machine framework this would be our state machines. Why? The state machine
(the interpretation) we're applying takes both the \texttt{reader} and the \texttt{text} and creates an interaction
between them. The idea is that the application of the hermeneutic to the reader and the current location of the text
changes the reader (as well as the current location of the text), but this is exactly what state machines and their
transition functions do.

To summarize, a system of register machines is decomposed as follows:

\begin{itemize}
\item \strong{text}: corresponds to programs, often with controllers in mind.
\item \strong{reader}: corresponds to the registers.
\item \strong{hermeneutic}: corresponds to the finite state machines (transition functions?) that act on both
	controller instructions as well as the registers to return the updated register states, as well as the next
	location within the program.
\end{itemize}

Finally, if we were to take this analogy to its logical conclusion, we would say that a reading of a text is a process
where the reader goes from location to location, applies its chosen hermeneutics changing itself along the way, then moving
on to the next location to repeat. Such interactions continue until the reading of the text is considered successful
and complete. As such a process coincides with how register machines work to perform their respective computations,
the intuition is confirmed (or at least not denied).

With this story now told, I would like to add that the main purpose in framing register machine systems from
a humanities lens is in fact to clarify the roles of the actors in this otherwise complex play.

\subsection*{Story 2: A Equivalence Subplot}

Now that we know the main characters, let's reorient and focus on a major subplot, one which itself will ultimately
help us understand the first of the two narratives being presented.

This plot comes from theoretical computing science and automata theory, and is all about ensuring our register machines
are Turing complete. The idea is since our reader---our memory of registers---takes the same shape regardless of texts
or hermeneutics, we can hold it fixed and abstract it out. From here, we can then focus on telling the story of the
relationship between such texts and hermeneutics, or rather between programs and finite state machines.

In particular we are interested in the correspondence between programs and finite state machines that we will call
\texttt{evaluators}. If we defined our programs from a mathematical lens as a \emph{space}, we would ask what the nature
of all such \emph{objects} inhabiting this space could even be? The historical consensus is that a given object belonging
to this space is a program if and only if it is a ``list of instructions'' that we can actually compute. Putting this
another way, we would say the object that is our program has an above mentioned \emph{evaluator}.

Without getting into specifics just yet, that's the basic idea of an evaluator. The thing to note here is that for each
program in our space of programs we can consider the fact that there is a corresponding evaluator in a space of evaluators.
Conceptually this is a nice clean plot point, but it's not a very interesting one, or even a useful story: A theory of
computation isn't all that meaningful if we have to manually (with cleverness and originality) construct an unique
evaluator for each program we want computed.

The plot moves forward by asking if we can do better: Can we find a single ``meta-evaluator'' such that it is finitely
described and can itself simulate all other evaluators in our infinite evaluator space?

Spoilers: The answer to this is yes, but with some tradeoffs. Either way it is known as a universal Turing machine,
and it is our second story.

\subsection*{Story 3: A Memory Arc}

Our second story abstracted out the memory from our system, so we will want a third story to discuss it directly.

Our memory system is a collection of registers for which we have random access to read and write their contents.
Unlike math though, within the theory of computation we can't (and shouldn't) always abstract away from considerations
of performance. This third story then is a story of practical memory access.

\subsection*{The Narrative}

With the plot now in motion, we have enough backstory that we can finally organize our narrative specification:

\begin{enumerate}

\item \strong{Space}: Our system of register machines assumes \texttt{Turing completeness}.

We must identify the C++17 constructs we intend to use to implement our reader, our texts, and our hermeneutics,
and further identify how to build a corresponding meta-hermeneutic (evaluator).

\item \strong{Algebra}: Our system of register machines assumes \texttt{constructivity} and \texttt{callability} of programs.

This means that we should not only be able to build composite programs out of atomic programs (constructivity), but we should
also be able to create composite programs out of other composite programs (callability).

\item \strong{Hardware}: Our system of register machines assumes \texttt{finite memory}.

Whatever our system ends up being, it is intended to be compile time computable, and will thus be simulated on top of
our compiler's own computations. Given this, it is the expectation here that we will inherit any and all of the practical
hardware constraints that the compiler must itself consider---the most notable one for us being \emph{nesting depth} limits.

\item \strong{Software}: Our system of register machines assumes \texttt{reasonable performance}.

As we are simulating on top of the compiler's computations, we are running overhead costs for our simulated register
read/write operations, as well as our simulated program calls---especially recursive program calls.

\item \strong{Community}: Our system of register machines assumes a \texttt{user-friendly interface} for architects to write,
debug, and run their own compile time programs.

\end{enumerate}
The details of this narrative will now be elaborated upon.

\section*{Methodology}

We first need to discuss the idea of a \texttt{vehicle of transmission}.

As mentioned in the narrative specification, we are simulating our register machine system on top of the compiler's own
computations, and as such we need to identify the mechanism or medium---our vehicle---in which our compile time computational
information will be transmitted.

Foregoing the suspense, \emph{function templates} are our vehicle.

\subsection*{Space Methods}

Ultimately the goal here is to simulate a system of register machines using functions and function templates
satisfying the C++17 standard toolset.

\subsubsection*{Turing Completion}

Given access to C++ variadic packs, we can successfully simulate a Turing complete register machine system based off the
theoretical (automata theory) result that says a finite state machine with two stacks as its memory system is sufficient
to be Turing complete.

\subsubsection*{State Machines}

\strong{Atomic Machines}

With this in mind, the following provides a baseline anatomy for how we will implement such finite
state machines, making note that two main variadic packs are used to implement our theoretical stacks:

$$ \def\arraystretch{2}
\tab[0cm] \begin{array}{llll}
\bfmbox{Stack 1:} \col[1em]   \texttt{controls}\mbox{ \{n, c, d, m, i, j\},}             \col\col[1em] \texttt{registers\ldots} \\
\bfmbox{Stack 2:} \col[1em]   \texttt{heap 0, heap 1,}  \col[1em]   \texttt{heap 2, heap 3,} \col[1em] \texttt{arguments\ldots} \\[-2em]
		  \col[1em]   \underbrace{\tab[2.5cm]}  \col[1em]   \underbrace{\tab[2.5cm]}                                    \\
		  \col[2.5em] \bfmbox{Stage 1}          \col[2.5em] \bfmbox{Stage 2}                                            \\[-2ex]
		  \col      \mbox{(register mutations)} \col[3ex] \mbox{(function calls)}
\end{array} $$

\strong{Compound Machines}

It's not so much that there are ``compound'' machines themselves, rather it's that the predefined atomic machines
are monadically composed in predefined ways (as programs) through specified \emph{controllers}. This follows the
traditional design of register machines more generally. Specifically the major design is borrowed from and quite
closely copying ``Structure and Interpretation of Computer Programs'' Chapter 5, which describes a standalone register
machine system implemented in the Scheme programming language, a variant within the larger LISP style of languages.

\subsection*{Algebraic Methods}

\subsubsection*{Continuation Passing}

We know in advance we will be using function templates to implement these compile time machines.
As such, we need a \emph{vehicle} to take the current state and pass it to the next state
(with associated instruction) so as it act on it next.

A most natural design then is to use a continuation passing monad with enough complexity that we achieve Turing
completion as an emergent effect.

\subsubsection*{Program Calls}

As mentioned above, we start with atomic machines but we do not actually build compound machines out of them,
so it is better to reframe the description as \emph{programs}. As such, we start with atomic programs and then
build compound programs from them. Such compound programs correspond to a chaining of atomic machines to start,
but in the long run it is also advantageous to be able to \emph{call} programs as if they were atomic machines.

We do this to satisfy the user-friendly design principle known as \emph{modularity} of design.

\subsection*{Hardware Methods}

although in the context of a compiler and our \strong{vehicle of transmission} this generally means \emph{nesting depth} constraints.

\subsubsection*{Nesting Depths}

To restate the second design constraint here:

\begin{center}
``Assumes finite memory (at any given time, but is potentially extensible).''
\end{center}

Another way to put this---given our reliance on function templates as our vehicle of compile time computation,
another more accurate way to phrase this is as:

\begin{center}
Assumes a finite/fixed nesting depth for function calls (at any given time).
\end{center}

Within the methodology, we have enough restrictions (details) now to choose the method for mitigating finite
nesting depths: \emph{Trampolining}.

Intersectionality of design. Beyond the theoretical, this design constraint is actually most important because
it needs to be considered within every other practical design, which is to say it is at the intersection of all
other designs.

\subsection*{Software Methods}

\strong{Reasonable Performance}

As far as software performance goes, the thing to remember is that we're simulating computations on top of
an existing software computation---the compiler. At the same time, in general our simulation is that of a compiler,
or rather an assembler itself. Keeping this is mind, we can take lessons learned from compiler optimizations themselves.

First is \emph{inlining}, but that requires greater support for manipulating programs as objects, while also maintaining
their types, so that when they are continuation passed the native compiler will do its work without throwing errors.
Unfortunately I myself haven't fully succeeded in this area of research, and have for this reason honestly abandoned
it in favor of other approaches.

Secondly then, is to optimize against each individual hermeneutic machine, but as there are only finitely many such
machines this approach doesn't scale. With that said, we can without loss of generality assume that each individual
hermeneutic machine is of reasonable performance, in which case it then becomes a matter of finding bottlenecks
in terms of how these machines are used---distributional patterns of use.

From this perspective, program calls, especially recursive calls would be the notable constraint. I've read many
social media accounts from practicing compiler theorists, I will claim that this observation also aligns with
traditional wisdom when it comes to compiler optimization theory as well.

With a goal in mind, how do we design for program call optimization then?

Secondly, in terms of compiler bottlenecks, the major one to consider is peformance costs for when our register machines
make recursive (program) calls. As we are simulating on top of the compiler, this cost adds up more quickly than the
rest. Given this, special care must be taken to minimize the cost of simulated recursion, ideally even to make use
of the compiler's own recursive mechanisms without much in the way of our own overhead. As it turns out this is possible,
and is why we privilege \emph{internal function template calls} over \emph{tail function template calls}.

\strong{Tail call vs Internal Call}

The other major performance bottleneck to consider as mentioned earlier is that of accessing registers within
a two stack memory system.

As for program calls themselves: Given the two stack memory design, there is a minimum core of programs and hermeneutic
machines required to achieve near-random memory access. These fall into the \emph{genre} of programs (texts) known
as \emph{block} programs/machines.

\subsection*{Community Methods}

Unfortunatey it is a side effect of our vehicle of transmission that under the current design the architect
is required to add housekeeping instructions to their code that are otherwise tedious an uninformative as
to the intended nature of their program. To abstract this away, we need an additional \emph{community}
mechanism to detour to specific housekeeping machines while simultaneously preserving the current indices
and our ability to return to the existing navigational path, furthermore without interfering with our
ability to trampoline.

\strong{Detour Abstraction}

\section*{Proofs}

\subsection*{Anatomy of a Compile Time Register Machine (C++ Function Template)}

\noindent Each \strong{atomic machine} has the following form:
$$ \def\arraystretch{1.15}
\tab[0cm] \begin{array}{l}
\texttt{template<>}									\\
\texttt{struct machine<\emph{name}>}							\\
\{											\\
\tab[1cm]\texttt{template<\emph{stack}\ldots>}						\\
\tab[1cm]\texttt{static constexpr auto result(\emph{heaps}\ldots)\ \{\ldots\}}		\\
\}
\end{array} $$
The \emph{name} allows for dispatching (template resolution), while the \emph{constexpr function}
has a single \emph{stack} made up of a variadic pack symbolically representing \emph{registers},
along with a fixed number of \emph{heaps} which are also made up of variadic packs, but which
are \emph{cached}, and thus more expensive in general.

We then build \strong{compound machines} by chaining them together with a \strong{controller}.
Such machines and controllers are organized into a \strong{hierarchy} of machine orders using
a \emph{monadic} narrative design: The idea is that the atomics of a higher level are constructed
from the compounds of lower levels which---assuming self-similarity propogates throughout---then
allows this pattern to scale:

\subsection*{Atomic Programs}

\subsection*{Compound Programs}

The idea is that with the chains of machines (at a given level) we can either end the chain with
a \emph{halting instruction} or a \emph{passing instruction}. Halters effectively become standalone functions
(with some interface hiding the chain), returning some standalone value. Passers on the other hand are intended
to continuation pass to other machines at higher orders.

This then implies a few consequences for the design of each individual machine:

\begin{itemize}
\item Each machine is required to carry its own controller, which includes required indices (as well as a nesting
depth counter), along with index iterators. Peformance and modularization design suggests such info should generally
be carried on the stack.
\item Each machine that has a higher order is required to carry the controller, indices, iterators, of the machine
it is eventually returning to. Abstraction-wise it makes the most sense to carry this info in a designated heap.
\end{itemize}

\subsection*{Trampolining}

\subsubsection*{Internal Function Calls}

\ \\[1cm]
\tab[0cm]{\large A model for CTRM Trampolining which uses \emph{internal} rather than tail function calls}
$$ \def\arraystretch{2}
\tab[-2cm] \begin{array}{lll}
\texttt{outer call machine}                                                                     \\
                            \col \texttt{current call machine}                                  \\
			    \col                               \col \texttt{inner call machine}
\end{array} $$

\subsection*{Program Calls}

\subsubsection*{Block Programs}

\subsubsection*{Linear Programs}

\subsubsection*{User Programs}

\newpage

filler.

\end{document}

