% Copyright 2020 Daniel Nikpayuk
\documentclass[twoside]{article}
\usepackage[letterpaper,left=2.5cm,right=2.5cm,top=2cm,bottom=2.5cm]{geometry}
\usepackage{asymptote}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% latex symbols:

\newcommand{\vn}{\ensuremath{\varnothing}}

\newcommand{\RA}{\Rightarrow}
\newcommand{\lra}{\longrightarrow}
\newcommand{\then}{\ensuremath{\quad\Longrightarrow\quad}}
\newcommand{\qmapsto}{\ensuremath{\quad \mapsto \quad}}
\newcommand{\mapsfrom}{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}

\newcommand{\defeq}{\ensuremath{\ :=\ }}
\newcommand{\qeq}{\ensuremath{\quad =\quad}}
\newcommand{\qqeq}{\ensuremath{\qquad =\qquad}}
\newcommand{\qdefeq}{\ensuremath{\quad :=\quad}}
\newcommand{\qqdefeq}{\ensuremath{\qquad :=\qquad}}

% latex markup:

\newcommand{\strong}[1]{{\bfseries #1}}
\newcommand{\bfmbox}[1]{\mbox{\bfseries #1}}

% latex spacing:

\newcommand{\twoquad}{\ensuremath{\quad\quad}}
\newcommand{\threequad}{\ensuremath{\quad\quad\quad}}
\newcommand{\fourquad}{\ensuremath{\quad\quad\quad\quad}}

\newcommand{\twoqquad}{\ensuremath{\qquad\qquad}}
\newcommand{\threeqquad}{\ensuremath{\qquad\qquad\qquad}}
\newcommand{\fourqquad}{\ensuremath{\qquad\qquad\qquad\qquad}}

% essay symbols:

\newcommand{\bv}[1][v]{\mathbf{#1}}

\newcommand{\readleft}{\ensuremath{\,_{_\leftarrow}\,}}
\newcommand{\readright}{\ensuremath{\,_{_\rightarrow}\,}}

\newcommand{\trle}{\ensuremath{\scriptscriptstyle \triangleleft}}
\newcommand{\trri}{\ensuremath{\scriptscriptstyle \triangleright}}

\newcommand{\doublechar}[2]{#2\hspace{#1}#2}
\newcommand{\shiftedchar}[3]{\hspace{#1}\raisebox{#2}{#3}}
\newcommand{\shiftedrule}[5]{\hspace{#1}\raisebox{#2}{\textcolor{#3}{\rule{#4}{#5}}}}
\newcommand{\after}[1]{\hspace{#1}}

\newcommand{\ldp}{\ensuremath{
\doublechar{-0.5ex}{(}
\shiftedchar{-0.35ex}{1.00ex}{$\scriptscriptstyle \bar{ }$}
\shiftedchar{-0ex}{-1.25ex}{$\scriptscriptstyle \bar{ }$}
\after{0.55ex}
}}

\newcommand{\rdp}{\ensuremath{
\doublechar{-0.5ex}{)}
\shiftedchar{-1.00ex}{1.00ex}{$\scriptscriptstyle \bar{ }$}
\shiftedchar{-0ex}{-1.25ex}{$\scriptscriptstyle \bar{ }$}
\after{0.95ex}
}}

\newcommand{\grayt}[1]{\ensuremath{\textcolor{darkgray}{#1}}}
\newcommand{\grply}{\textcolor{darkgray}{,\ \apply(}}
\newcommand{\grbnd}{\textcolor{darkgray}{,\ \bind(}}
\newcommand{\grcns}{\textcolor{darkgray}{,\ \cons(}}

\newcommand{\drop}{\mbox{drop}}
\newcommand{\keep}{\mbox{keep}}

% essay formatting:

\newcommand{\mss}[1]{\ensuremath{\mbox{\scriptsize #1}}}
\newcommand{\bms}[1]{\ensuremath{_{\mbox{\bfseries\tiny #1}}}}
\newcommand{\bnms}[2]{\ensuremath{\bms{#1,}{_#2}}}

\newcommand{\tab}[1][1.125cm]{\hspace{#1}}

\newcommand{\col}[1][0ex]{& \hspace{#1}}
\newcommand{\scol}{\col[0.15cm]}
\newcommand{\lcol}{\col[0.45cm]}

\newcommand{\msbox}[1]{\ensuremath{_{\mbox{\scriptsize #1}}}}
\newcommand{\cbox}[1]{\mbox{// #1}}

% essay functions:

\newcommand{\apply}{\mbox{apply}}
\newcommand{\delay}{\mbox{delay}}
\newcommand{\force}{\mbox{force}}
\newcommand{\transit}{\mbox{transit}}

\newcommand{\id}{\mbox{id}}
\newcommand{\cons}{\mbox{cons}}
\newcommand{\car}{\mbox{car}}
\newcommand{\cdr}{\mbox{cdr}}
\newcommand{\eq}{\mbox{eq?}}
\newcommand{\dec}{\mbox{dec}}
\newcommand{\isNull}{\mbox{isNull?}}
\newcommand{\isZero}{\mbox{isZero?}}
\newcommand{\length}{\mbox{length}}
\newcommand{\push}{\mbox{push}}
\newcommand{\rev}{\mbox{reverse}}
\newcommand{\concat}{\mbox{concat}}

\newcommand{\isProd}{\mbox{isProduction?}}
\newcommand{\isTerm}{\mbox{isTerminal?}}
\newcommand{\isVar}{\mbox{isVariable?}}

\newcommand{\isNotProd}{\mbox{isNotProduction?}}
\newcommand{\isNotNull}{\mbox{isNotNull?}}

% essay operators:

\newcommand{\define}{\mbox{define}}
\newcommand{\induct}[1]{\bfmbox{induct}\mss{\,#1\,}}

\newcommand{\compose}{\mbox{compose}}
\newcommand{\precompose}[1]{\ensuremath{\mbox{precompose}_{#1}}}
\newcommand{\postcompose}[1]{\ensuremath{\mbox{postcompose}_{#1}}}

\newcommand{\ndopose}{\mbox{endopose}}
\newcommand{\prendopose}[1]{\ensuremath{\mbox{preendopose}_{#1}}}
\newcommand{\postndopose}[1]{\ensuremath{\mbox{postendopose}_{#1}}}

\newcommand{\hold}{\mbox{hold}}
\newcommand{\cohold}{\mbox{cohold}}
\newcommand{\pend}{\mbox{pend}}
\newcommand{\dihold}{\mbox{dihold}}
\newcommand{\copend}{\mbox{copend}}
\newcommand{\stem}{\mbox{stem}}
\newcommand{\costem}{\mbox{costem}}
\newcommand{\distem}{\mbox{distem}}

\newcommand{\bind}{\mbox{bind}}
\newcommand{\Hbind}{\ensuremath{>\hspace{-1ex}>\!=}}
\newcommand{\Hrbind}{\ensuremath{=\!<\hspace{-1ex}<}}
\newcommand{\Hfish}{\ensuremath{>\!=\!>}}
\newcommand{\Hrfish}{\ensuremath{<\!=\!<}}

\newcommand{\underpose}[2]{\ensuremath{\underset{\mbox{\tiny #2}}{\{\mbox{#1}\}}}}
\newcommand{\doubleunderpose}[4]{\ensuremath{\underset{\mbox{\tiny #2}}{\{\mbox{#1}}\,||\,\underset{\mbox{\tiny #4}}{\mbox{#3}\}}}}

\newcommand{\varstempose}[4]{\ensuremath{\textcolor{#1}{#2\underpose{#3}{#4}}}}
\newcommand{\spose}[2][blue]{\varstempose{#1}{\star}{closing}{#2}}
\newcommand{\cpose}[2][blue]{\varstempose{#1}{\star}{opening}{#2}}
\newcommand{\dpose}[2][blue]{\varstempose{#1}{\star}{open}{#2}}

\newcommand{\reppose}[2]{\bfmbox{repeat}\ \ #1\ \ #2}

\newcommand{\tpose}[2]{\raisebox{#1}{\ensuremath{\star}}\{\texttt{#2}\}}
\newcommand{\deftpose}[1]{\tpose{0.1ex}{#1}}

\newcommand{\tbind}[3]{\raisebox{#2}{\ensuremath{\vdash}}\hspace{#1}\{\texttt{#3}\}}
\newcommand{\deftbind}[1]{\tbind{-0.43ex}{-0.214ex}{#1}}

\newcommand{\vbind}[4]{
\ensuremath{\textcolor{#1}{\Rightarrow\hspace{#2}\begin{array}[t]{c}
\{\mbox{derive}\} \\[#3]
\mbox{\tiny #4}
\end{array}}}}

\newcommand{\defvbind}[2][blue]{\vbind{#1}{-2.25ex}{-2ex}{#2}}

\newcommand{\mcpose}[6]{\textcolor{#1}{\raisebox{#2}{\ensuremath{\star\doubleunderpose{#3}{#4}{#5}{#6}}}}}
\newcommand{\defmcpose}[5][blue]{\mcpose{#1}{0.214ex}{#2}{#3}{#4}{#5}}

\newcommand{\alias}{\mbox{alias}}
\newcommand{\assign}{\mbox{assign}}

\newcommand{\start}{\mbox{start}}

\newcommand{\angscp}[1]{\langle #1\rangle}
\newcommand{\angscphyp}[2]{\angscp{#1\mbox{-}#2}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{asydef}
// this comment prevents a compilation bug.

real direction(bool value)
{
	return value ? 1 : -1;
}

pair offset(pair current, string align, real x = 0.03)
{
	pair shifter =	(align == "E") ? (x, 0) :
			(align == "N") ? (0, x) :
			(align == "W") ? (-x, 0) :
			(align == "S") ? (0, -x) : (0,0);

	return shift(shifter)*current;
}

pair segment(pair current = (0,0), string align, real projection, real angle = 0)
{
	align = (align == "E") ? "EU" :
		(align == "N") ? "NL" :
		(align == "W") ? "WD" :
		(align == "S") ? "SR" : align;

	real base          = direction(substr(align, 0, 1) == "E" || substr(align, 0, 1) == "N");
	real adjacent      = direction(substr(align, 1, 1) == "R" || substr(align, 1, 1) == "U") * Tan(angle);
	pair initialVertex = (substr(align, 0, 1) == "E" || substr(align, 0, 1) == "W") ? (base, adjacent) : (adjacent, base);
	pair scaledVertex  = scale(projection) * initialVertex;
	pair shiftedVertex = shift(current)    * scaledVertex;

	return shiftedVertex;
}

//

void drawpath(path p)
{
	draw(p);

	for (int k=0; k < size(p); ++k)
	{
		dot(point(p, k));
	}
}

void safeLabel(picture pic = currentpicture, Label L, pair position, real width, real height,
	align align = NoAlign, pen textpen = currentpen, pen borderpen = currentpen,
	pen fillpen = white, filltype filltype = NoFill, string bordertype = "round")
{
	if (bordertype == "round")
	{
		pair w = position + (-width, 0);
		pair e = position + ( width, 0);
		pair n = position + (0, height);
		pair s = position + (0,-height);

		filldraw(pic, w{up}::n{right}::e{down}::s{left}::cycle, fillpen, borderpen);
	}
	else if (bordertype == "box")
	{
		pair sw = position + (-width,-height);
		pair se = position + (-width, height);
		pair nw = position + ( width,-height);
		pair ne = position + ( width, height);

		filldraw(pic, sw--se--ne--nw--cycle, fillpen, borderpen);
	}

	label(pic, L, position, align, textpen, filltype);
}


\end{asydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Grammatical Elements\\of Function Induction}
\author{Daniel Nikpayuk}
\date{May 8, 2020\\[2ex](updated August 26, 2020)\footnote{Updates are to fix the incorrect classification of certain operators.
In particular \texttt{stem}, \texttt{costem}, and \texttt{distem} ``bind'' operators have now been reclassified as
\emph{conditional endopose} operators. The essay itself has been extended to clarify and refine the necessary explanations.
Some minor grammatical fixes have also been made.}}
\thispagestyle{empty}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{figure}[h]
\centering
\includegraphics[width=1in]{../../../../cc-by-nc.png}\\[0.1in]
\tiny This article is licensed under \\
Creative Commons Attribution-NonCommercial 4.0 International.\\[0.3in]
\end{figure}

\section*{Abstract}

There is a correspondence between function construction grammar and finite state automata.

It is my intention in this essay to demonstrate the existence of this correspondence and its variations. Such correspondences
are of theoretical importance as they provide bases toward type theoretic induction operators for function types. They are
also of practical importance as they offer ways to create inventories of variant or performant functions without having
to code each function manually. It is also my intention then to provide a semi-formal walk-through in how to build
functions that build functions, as well as the grammars that allow for such possibilities.

\section*{Philosophy}

The underlying design put forth in this essay---the one which informs all other designs here---is the idea of
a correspondence, in particular between \emph{function kinds} and the finite state \emph{automata} otherwise equivalent
to the Chomsky hierarchy of formal grammars. This design not only informs our ability to classify functions, it can also
be used as a basis of translation for type inductive grammars. As there's a lot to unpack here, let's go over this.

First, a bit of background as to the meaning of the phrase \emph{function induction}: In modern type theories such as
\cite{hott} there's a special function for each \emph{type} called the \strong{induction} function, which is also called
\emph{dependent elimination}. As a rule, it effectively tells us what kind of functions are even possible for its given
\emph{type} definition, but we can also think of it as the function used to build other functions acting on the type.
The philosophy of this essay takes inspiration from this formal version of induction and generalizes it as a concept
to include broader classes of higher order functions, themselves defining classes of functions.

As for the correspondence, we'll get into the \emph{function kinds} and \emph{automata} details below, but the intuition
inspiring this particular design is the idea that an \strong{interpretation} acts as the common thread between evaluating
a function and constructing that same function. Informally, we can say the only major difference between evaluation and
construction here is that we \emph{read} the interpretation when evaluating, while we \emph{write} the interpretation
when constructing.

This may or may not be a novel idea for the reader (it was for me), but at the very least I feel it's not controversial
to accept half of the claim more formally stated as:

\begin{center}
\emph{If we can express how a function is evaluated, we can express how it is constructed}
\end{center}

\noindent On the other hand, its converse:

\begin{center}
\emph{If we can express how a function is constructed, we can express how it is evaluated}
\end{center}

\noindent is not always true. The major claim of this essay restated
another way then is that there are significant contexts in which this converse does hold.

\section*{Methodology}

Based on the above philosophy, the methodology here can be split into two major perspectives: Evaluation, and construction.
With that said, there is one additional perspective we will consider first: Prerequisites.

\subsection*{Prerequisites}

The idea of our prerequisites are that for whatever grammars we do eventually come up with, we will want them to satisify
certain design specifications (constraints) regardless of our other narratives. We begin with notation.

\subsubsection*{Function Notation}

In terms of algorithmic analysis in this essay I will use a combination of notations from the LISP programming language,
and what I call \strong{grammatical path theory} which is independent research I've written up in \cite{nikfs}. Given this,
any grammar we come up with should be amenable to these notations as well. With that said, there is no formal requirement for
the casual reader to know these prerequisites, and for that reason I have largely kept both LISP as well as grammatical path
explanations at an intuitive level---providing basic explanations when needed.

\subsubsection*{Modelling Strategies}

For whatever grammars we do come up with as induction operators, we will need to be able to model their functions
before we can specialize them. An alternative way to view such a task is to consider all such functions together
as a \emph{space}. How then would we go about constructing and representing such a space's elements?

To do this we will want our grammars to possess a \strong{nonlinear combination} design. To explain this,
we first appeal to the idea of a \emph{vector space} which uses the concept of a \emph{linear combination}:
$$ a_1\,\bv_1\ +\ \ldots\ +\ a_n\,\bv_n $$
This represents all vectors in the \emph{span} of a given basis. Notably such a representation is considered to be a
\strong{closed form}, or in other words a \strong{model}.\footnote{One could even say the term ``closed form'' is the
syntactic analog to the semantic ``model''.} We now deconstruct the idea of a linear combination to be applicable to
broader spaces by considering two canonical approaches to \emph{modelling}: Constructive (emergent) combinations which
we'll here call \emph{primary} models, as well as constrictive submodels of those primaries, which we'll call
\emph{secondary} models. In such a paradigm linear combinations are primary models.

As for an example of a secondary model, we could implement one by means of a primary such as $ \mathbb{R}^2 $,
with which we can subset to derive the unit closed disk:
$$ \mathcal{D}\ :=\ \{\ (x,y)\in\mathbb{R}^2\ |\ x^2+y^2 \le 1\ \} $$

With this said, both modelling strategies have their strengths and weaknesses and are each suited for different contexts.
For example, the secondary (submodelling) approach is better suited for function design. It's one thing to build a function,
but we have to know what to build in the first place---which often means we need to design then translate that into
a construction. Submodelling thus allows us to both construct and to conceptualize top-down what's needed to eventually
build things bottom-up.\footnote{I should mention that using submodelling for design is possibly my own bias, but I find
having a single umbrella function as a model in a context really does help clarify the components of design that are used
to translate more refined models.}

As for the primary emergent combination approach: Due to the desire for performance it is usually a better fit when actually
implementing our function construction since then nothing is wasted.\footnote{The other advantage to taking this approach is
that such combinations create a coded combinatorial inventory: We know exactly what is suppose to be in this space, and this
has both theoretical and contextual applications. For example it allows us to make room for its functions when we're organizing
a code library. We can then take a lazy policy where we would only be required to build the functions we need when we need.
Later down the road we would then have a location for them for when it's time to add them in.} Finally, this is where the
\emph{nonlinear combination} phrase comes from---to emphasize its use in function construction as similar to the above
linear combinations.

To reiterate: Functions will be modelled as combinations by means of composition, and we take the name \emph{nonlinear}
because such functions will be shown to be built exactly in that way---nonlinearly.

\subsubsection*{Compression}

This is very much a practical design specification.

In application, nonlinear combinations allow us to define functions by breaking them up into smaller more manageable functions,
but in doing so we actually create a new problem for ourselves: \strong{Compression}, which plays a more active role in
function definitions than we might realize.

In fact we compress for two clear reasons: The first is just to make our human user notations easier to read and work with.
The following function has a sufficiently complicated grammatical path representation such that we would want to refactor
its information and hide any of the details we intended to take for granted:

% y = f(x) = x(x+1)^2 , canonical construction
%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
 \begin{asy}
 unitsize(1cm);

 pair p00 = (0,0);

 pair p10 = segment(p00, "S", 2);
 pair p11 = segment(p10, "E", 6.5);

 pair p20 = segment(p10, "S", 1);
 pair p30 = segment(p20, "S", 1);
 pair p40 = segment(p30, "S", 1);
 pair p50 = segment(p40, "S", 1);

 //

 pair q00 = segment(p10, "EU", 11, 5);

 pair q10 = segment(q00, "S", 1);
 pair lq10 = offset(q10, "W", 0.06);
 pair rq10 = offset(q10, "E", 0.06);

 pair q20 = segment(q10, "SL", 1, 65);
 pair lq20 = offset(q20, "W", 0.06);
 pair rq20 = offset(q20, "E", 0.06);

 pair q21 = segment(q10, "SL", 1, 40);
 pair q22 = segment(q10, "SR", 1, 40);

 pair q31 = segment(q22, "SL", 1, 45);
 pair q32 = segment(q22, "SR", 1, 45);

 pair q41 = segment(q31, "SL", 1, 30);
 pair q42 = segment(q31, "SR", 1, 30);
 pair q43 = segment(q32, "SL", 1, 30);
 pair q44 = segment(q32, "SR", 1, 30);

 //

 pen arrowcolor = mediumred;

 draw(shift(-1.5,0)*p11--shift(1.5,0)*p11, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p20--shift(0.4,0.5)*p20, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p40--shift(0.4,0.5)*p40, arrowcolor, Arrow);
 
 //

 pen stepcolor = gray;
 pen bordercolor = heavygray;
 
 draw(lq10--lq20, bordercolor);
 draw(rq10--rq20, bordercolor);

 draw(q10--q21, bordercolor);
 draw(q10--q22, bordercolor);

 draw(q22--q31, bordercolor);
 draw(q22--q32, bordercolor);

 draw(q31--q41, bordercolor);
 draw(q31--q42, bordercolor);
 draw(q32--q43, bordercolor);
 draw(q32--q44, bordercolor);

 //
 
 label("$y = $", shift(0,-0.06)*p00, E);
 label("$f$", shift(0.775,0)*p00, E, heavyblue);
 label("$(x) = x(x+1)^2$", shift(1.015,0)*p00, E);

 label("$(*, *, +, +, x, x, 1, x, 1)$", p10, E);

 label("\scriptsize applicate", p11, N, arrowcolor);

 label("\scriptsize duplicate", shift(0.5,-0.1)*p20, E, arrowcolor);

 label("$(*, +, x, 1)$", p30, E);

 label("\scriptsize prepare", shift(0.5,-0.1)*p40, E, arrowcolor);

 label("$(x)$", p50, E);

 //

 label("$f$:", q00, W, heavyblue);

 label("\scriptsize $0$", shift(0.6,0)*q20, NE, stepcolor);
 label("\scriptsize $1$", shift(0.2,0)*q21, NE, stepcolor);
 label("\scriptsize $2$", shift(-0.2,0)*q22, NW, stepcolor);

 label("\scriptsize $1$", shift(0.4,0.17)*q31, NE, stepcolor);
 label("\scriptsize $2$", shift(-0.4,0.17)*q32, NW, stepcolor);

 label("\scriptsize $1$", shift(0.2,0.17)*q41, NE, stepcolor);
 label("\scriptsize $2$", shift(-0.2,0.17)*q42, NW, stepcolor);

 label("\scriptsize $1$", shift(0.2,0.17)*q43, NE, stepcolor);
 label("\scriptsize $2$", shift(-0.2,0.17)*q44, NW, stepcolor);

 safeLabel("$*$", q10, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$y$", q20, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$x$", q21, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$*$", q22, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$+$", q31, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$+$", q32, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$x$", q41, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$1$", q42, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$x$", q43, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$1$", q44, 0.25, 0.25, borderpen = bordercolor);

 \end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
I describe this diagram as \emph{canonical}, doing so with the sense that the tree component of this grammatical
path notation is of maximum size. Regardless of this canonicity, if we go in the reverse direction of the mapping
arrows we're able to compress the function signature---first reducing redundancy, then hiding those values that
are otherwise held constant---allowing us to simplify all the way down to $ (x) $.

The second reason we compress is for computational performance. If you'll notice in the above diagram there is some
redundancy with the $ (x+1) $ expressions, which if we calculated separately would be wasteful. In this case, if
we were to again refactor the tree into a signature, we could take advantage of this repetition and simplify:

% y = f(x) = x(x+1)^2 , optimized construction
%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
 \begin{asy}
 unitsize(1cm);

 pair p00 = (0,0);

 pair p10 = segment(p00, "S", 2);
 pair p11 = segment(p10, "E", 6.5);

 pair p20 = segment(p10, "S", 1);
 pair p30 = segment(p20, "S", 1);
 pair p40 = segment(p30, "S", 1);
 pair p50 = segment(p40, "S", 1);
 pair p60 = segment(p50, "S", 1);
 pair p70 = segment(p60, "S", 1);
  
 //

 pair r00 = segment(p10, "EU", 11, 5);

 pair r10 = segment(r00, "S", 1);
 pair lr10 = offset(r10, "W", 0.06);
 pair rr10 = offset(r10, "E", 0.06);

 pair r20 = segment(r10, "SL", 1, 65);
 pair lr20 = offset(r20, "W", 0.06);
 pair rr20 = offset(r20, "E", 0.06);

 pair r21 = segment(r10, "SL", 1, 40);
 pair r22 = segment(r10, "SR", 1, 40);

 pair r31 = segment(r22, "SL", 1, 45);
 pair r32 = segment(r22, "SR", 1, 45);
  
 //

 pen arrowcolor = mediumred;

 draw(shift(-1.5,0)*p11--shift(1.5,0)*p11, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p20--shift(0.4,0.5)*p20, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p40--shift(0.4,0.5)*p40, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p60--shift(0.4,0.5)*p60, arrowcolor, Arrow);
  
 //

 pen stepcolor = gray;
 pen bordercolor = heavygray;
 
 draw(lr10--lr20, bordercolor);
 draw(rr10--rr20, bordercolor);

 draw(r10--r21, bordercolor);
 draw(r10--r22, bordercolor);

 draw(r22--r31, bordercolor);
 draw(r22--r32, bordercolor);

 //
 
 label("$y = $", shift(0,-0.06)*p00, E);
 label("$f$", shift(0.775,0)*p00, E, heavyblue);
 label("$(x) = x(x+1)^2$", shift(1.015,0)*p00, E);
 
 label("$(*, *, x,\ x+1,\ x+1)$", p10, E);

 label("\scriptsize applicate", p11, N, arrowcolor);

 label("\scriptsize duplicate", shift(0.5,-0.1)*p20, E, arrowcolor);

 label("$(*, x,\ x+1)$", p30, E);

 label("\scriptsize decompress", shift(0.5,-0.1)*p40, E, arrowcolor);

 label("$(*, x)$", p50, E);

 label("\scriptsize prepare", shift(0.5,-0.1)*p60, E, arrowcolor);

 label("$(x)$", p70, E);

 //

 label("$f$:", r00, W, heavyblue);

 label("\scriptsize $0$", shift(0.6,0)*r20, NE, stepcolor);
 label("\scriptsize $1$", shift(0.2,0)*r21, NE, stepcolor);
 label("\scriptsize $2$", shift(-0.2,0)*r22, NW, stepcolor);

 label("\scriptsize $1$", shift(0.4,0.17)*r31, NE, stepcolor);
 label("\scriptsize $2$", shift(-0.4,0.17)*r32, NW, stepcolor);

 safeLabel("$*$", r10, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$y$", r20, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$x$", r21, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$*$", r22, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$x+1$", r31, 0.65, 0.25, borderpen = bordercolor);
 safeLabel("$x+1$", r32, 0.65, 0.25, borderpen = bordercolor);

 \end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}

So far so good, but how then does compression become an issue here?

Compression works best with \strong{complete information}. Generally, we would comprehend the bigger picture (all at once)
to find those repetitive patterns we'd want to compress. This becomes an issue when we \emph{divide and express} our above
mentioned functions through nonlinear combinations: By modularizing out simpler functions we create information silos---we
isolate our ability to recognize redundant patterns across subfunctions. We can still compress each component function,
but our ability to compress overall along with our compression ratios will on average lessen. Can we mitigate this issue?

Yes, fortunately. We mitigate this issue by realizing that the \strong{applicate} function above---which bijectively maps
the \emph{raw} (uncompressed) signature content to the tree nodes---can be reinterpreted to handle \strong{just in time}
compression: We equip our applicate function with the idea of ``lazy evaluation'' creating a kind of ``delayed applicate''.
It has the same mapping pattern as before, but now it only maps as needed.

The implication of this change in perspective is we can now map parts of the signature to the whole tree piece by
piece---parts corresponding to the component functions used to define the function whole. In this sense we have what
I would call a \strong{scope signature}, a component of local memory that spans across each module function. In practice
what this means is we can \emph{after the fact} push our compressions---otherwise redundant computations---onto this scope
memory to be used across the range of function components.

Any grammar we then come up with for our induction operators should be able to simulate such scope signatures.

\subsubsection*{Signatures}

One of the important ways to create variations of functions is to create variations within their signatures.
The act of creating these variations is known as \strong{the signature problem}.

Once the best practice solutions are known this problem is rather straightforward, but up until then it can be considered
complicated and as we'll come to know is in fact what prevents us from initially creating more powerful versions of our
intended construction grammar. It is for this reason that here I've decided to present a quick review of \strong{tuples}
and how they're built and used to implement function signatures. Later in the essay once a baseline of grammar is introduced
we will then deal with the signature problem itself.

As for tuples, one can consider such objects to each have a given \emph{length},
and are otherwise defined by the independent construct known as a \strong{pair}:
$$ (a, b, c, \ldots, z) = (a, (b, (c, (\ldots, (z,\ \mbox{null}\,) \ldots )))) $$
Tuples then are recursive nestings of pairs with their final value always being the $ 0\, $-tuple,
also known as the \emph{null} tuple.

Beyond this basic definition there are tuple operators, for which we now take an inventory of their primitives.
In fact, there are four that will be shown as sufficient to model the rest:
$$ \{\,\cons,\ \car,\ \cdr,\ \eq\,\} $$
The first three are defined as follows:
$$ \begin{array}{lcl}
\cons(x, y)		& := & (x, y)			\\
\car((x, y))		& := & x			\\
\cdr((x, y))		& := & y
\end{array} $$
The definition of the fourth (\texttt{eq?}) is slightly trickier as it requires recursion, so we'll take its definition
for granted. The \strong{cons}, \strong{car}, \strong{cdr} (and \strong{eq?}) terminology I borrow from the LISP programming
language. In particular, \texttt{cons} is the pair constructor, \texttt{car} is the first element projection function,
and \texttt{cdr} the second element projection function. In coding literature the second element of a tuple is frequently
referred to as the \emph{rest} (of the tuple following the first element), which is a convention we will use in the following.

There are actually three \emph{auxiliary} functions we'll want as well:
$$ \{\,\push,\ \rev,\ \concat\,\} $$
The \texttt{push} function \emph{appends} a value to the back of a tuple, analogous to how \texttt{cons} \emph{prepends}
one to the beginning. Versions of this function which are considered performant require more complicated implementations
of tuples, as such we can't always assume users have this access with respect to their given contexts. In that case we
will at least need \texttt{reverse} which takes a tuple as input and returns the tuple with the same content but in
(componentwise) reversed order. Finally, \texttt{concat} takes two tuples and concatenates them together as one.

Before continuing with further methodology, I here implement a basic ``length'' function in LISP style grammar with C style
comments in order to demonstrate how the initial primitives mentioned above can be used:\footnote{In fact, \texttt{eq?},
\texttt{reverse}, \texttt{push}, \texttt{concat} can all be implemented in similar ways.}
$$ \def\arraystretch{1.15}
\tab[0cm] \begin{array}{ll}
(\define\ (\length'\ count\ tuple)				\col[2cm] \cbox{declare and define length}'		\\
\tab[1.15cm] (\mbox{if}\ (\eq\ tuple\ \mbox{null})		\col[2cm] \cbox{if $tuple == $ null}			\\
\tab[1.75cm]  count						\col[2cm] \cbox{return } count				\\
\tab[1.60cm]  (\length'\ (+\ count\ 1)\ (\cdr\ tuple))		\col[2cm] \cbox{else (recursively) call}		\\
\tab[1.15cm] )							\col[2cm] //\ (\length'\ (count + 1)\ rest)		\\
)
\end{array} $$
By the way, if the above version of the tuple \texttt{length} function is unfamiliar,
we can achieve the more intuitive interpretation as a specialization:
$$ \mbox{length}(tuple)\ :=\ \mbox{length}'(0,\ tuple) $$

\subsection*{Evaluation}

The major claim in this essay is of the correspondence between function construction grammar and finite state automata.
If that's the case, how do automata even come into the picture in the first place?

Finite state automata in particular come into play here because the theory of automata espouses the idea that
function \strong{evaluation} corresponds to \emph{membership tests} for strings within languages. If there really
is a correspondence between function construction and evaluation, then by transitivity there should also be one
between construction and automata.

The advantage of us exploring this connection by using automata is that automata theory already has well developed ideas
of function evaluation, as well as logical expressivity (membership testing) which we can use to prove theorems about the
induction grammar we eventually decide upon. As such, it would be best if the reader already has some familiarity with
automata theory, but for a smoother transition I will provide a quick review here, as well as more specific details
later on regarding each variety when we come across them.

\subsubsection*{Finite State Automata}

The review I offer here is largely a combination of elements from \cite{iatlc} and \cite{sicp}.

To begin, automata is known to be equivalent to the \emph{Chomsky hierarchy of formal grammars} which I find provides
a simpler narrative for this introduction, and is the basis for the correspondence between evaluation and membership:

\begin{enumerate}
\item \strong{Regular Languages} - correspond to regular automata as well as regular expressions.
\item \strong{Context-Free Languages} - correspond to pushdown automata.
\item \strong{Context-Sensitive Languages} - correspond to linear bounded automata.
\item \strong{Semidecidable Languages} - correspond to register machines.\footnote{In traditional teachings of automata
      theory, instead of register machines the theoretical and alternative automata taught is that of the well known Turing
      machine: Such machines are conceptually simpler and easier to use when proving mathematical theorems about computability.}
\end{enumerate}

You can think of each level in the hierarchy as being built on top of the previous levels, where regular languages are
simplest and semideciable languages are most complex.

As for the definition of a \strong{language}, we start with an \emph{alphabet} $ \Sigma $, and create \emph{strings}
of a given length $ \Sigma^n $ (for arbitrary lengths $ n $) and then we take the set of all strings to be:
$$ \Sigma^* \ :=\ \bigcup_{n\ge 0}\Sigma^n $$
where $ \Sigma^0 := \{\epsilon\} $, the set containing the empty string.
A language is defined to be a secondary model derived from this primary:
$$ L \subseteq \Sigma^* $$

As for the correspondence between languages and automata, it comes from the fact that \emph{finite state automata} are
able to recognize whether a given string belongs to a given language. The differences between levels of automata come
from a specific implementation detail: The \emph{memory system} required in order for them to be able to recognize
their preferred patterns within strings.

To elaborate, each layer of automata can actually only recognize a certain subclass of languages. When a language is beyond
the capacity of the automata layer, we then move on to a higher level automata. This ends with register machines which have
the complexity potential of (ability to represent) arbitrary computable functions, but which also come with their own
theoretical limitations---namely decidability.\footnote{Decidability is equivalent to the halting problem for which there
is no single general solution.  The idea is if we are going to compute a function we'd like to know if it will halt based
on the input we give it.  A language is recursively enumerable (semidecidable) if we know its recognizer function will
halt with proper input.  It may or may not halt given \emph{incorrect} input, and we may even be able to prove as much
case by case for each given function, but the halting problem says there's no unified proof for all such functions.}

As stated in the philosophy section above, the novel idea of this essay is to present induction grammar that parallels
function evaluation. I can now say specifically the intention here is to demonstrate grammar that corresponds to the regular
and context-free levels of automata, and to show the potential for grammar at the higher levels along with the theoretical
implications of these possibilities.

\subsection*{Construction}

We've reached the other half of the evaluation correspondence: \strong{construction}.

Since we're modelling function evaluation with automata, grammars, and languages, the correspondence with construction
now comes from \emph{function composition} which uses the insight that \strong{chain composition} aligns quite well with
the idea of a \emph{string}:
$$ f_0f_1f_2\ldots f_{n-1}f_n \qdefeq f_n \circ f_{n-1} \circ \ldots \circ f_2 \circ f_1\circ f_0 $$
Here we choose to perceive the individual functions as ``characters'' and use composition to sequence them into the string.
Keep in mind we will need some way to check the \strong{compositional coherence} of such strings, which is to say we will
need to be able to check when such strings are even valid compositions in the first place. As for how these strings are
read, we adhere to the cultural convention of reading from \emph{left-to-right}, but it's certainly possible to read
the other way as well:
$$ \begin{array}{rcl}
\readright f_0f_1f_2\ldots f_{n-1}f_n		& = & f_n \circ f_{n-1} \circ \ldots \circ f_2 \circ f_1\circ f_0	\\
															\\
						& = & f_nf_{n-1}\ldots f_2f_1f_0 \readleft 
\end{array} $$
In practice when the context is clear we will default to the left-to-right reading, but if it's \emph{right-to-left}
it will be made more explicit in the given discourse either in words or through this \emph{arrow reading} notation.

In anycase, since our theory of construction is very much dependent upon the idea of composition, we will borrow concepts
from category theory as it already has many well developed notions in this area. As I can't assume everyone is familiar
with category theory, I will now give a quick summary of some basics.

\subsubsection*{Category Theory}

Category theory is a foundational branch of mathematics which can act as an alternative to set theory. My explanation of
it is informed by \cite{ctic}, although our notations differ a little. Category theory provides many valuable models for
type theory and is increasingly becoming an active area of research in computing science because of this. We will focus
on the computationally oriented aspects of the theory here.

Casually, a category $ \mathcal{C} $ is made up of a class of objects and their morphisms, such that the morphisms have
an operator $ \circ $ which otherwise acts like our well behaved function composition operator. I will use this notation
for both, but to signify the difference I will at times write function composition more specifically as:
$$ \compose(f, g) \defeq f \circ g $$
Otherwise, the morphism composition operator is well behaved with the following properties:

\begin{itemize}
\item Composition is associative: $ (f \circ g) \circ h = f \circ (g \circ h) $.
\item Composition has an identity: $ f \circ \id = f = \id \circ f $.
\end{itemize}
Keep in mind I'm oversimplifying things here. To be technically accurate we have to consider the \emph{domains}
and \emph{codomains} of morphisms to maintain their compositional coherence. Also, the above identity property
makes it appear like there is only one identity morphism \texttt{id} per composition operator, but in fact there
is one identity morphism $ \texttt{id}_c $ per category object $ c $, we only hide the index by convention.

Given such morphism composition we introduce two specializations which will be useful in the following:
$$ \def\arraystretch{1.5}
\begin{array}{rclcl}
\precompose{f}(g)	& = & f^\circ(g)	& := & g \circ f		\\
\postcompose{f}(g)	& = & f_\circ(g)	& := & f \circ g
\end{array} $$
These specializations are achieved by fixing either of the arguments (respectively) of our composition operator.

As with any other theory of math we will want to be able to compare our subjects of interest (here categories) using
some kind of specialized \emph{property preserving map}. In this context the equivalent idea is that of a \emph{functor}
which is traditionally signified by uppercase roman letters: $ F:\mathcal{C}\to\mathcal{D} $. Beyond that, this theory
diverges from most math\footnote{The exception being homotopy and related theories which are in fact the origins for
category theory.} because it also finds a way not only to compare categories, but to compare functors. To do this,
it uses the idea of a mapping between functors which it calls a \emph{natural transformation}, where such transformations
are themselves traditionally denotated by lowercase greek letters such as $ \alpha:F\to G $.

Functors and natural transformations are very important in category theory in general, but are more specifically relevant
to us because each behaves enough like the common intuitive idea we have of a function that we can talk about them having
their own composition operators. In the case of functors, we reuse the function composition notation as the context tends
to be clear enough that no ambiguity ensues:
$$ F:\mathcal{C}\to\mathcal{D} \quad \mbox{and} \quad G:\mathcal{D}\to\mathcal{E}
\quad \mbox{suggest} \quad (G\circ F):\mathcal{C}\to\mathcal{E} $$
In theory we could apply this same logic to natural transformations except that based on how they're defined they
actually allow for two distinct equally valuable composition operators to which we give them their own notations:
$$ \begin{array}{rrclcl}
\mbox{vertical composition:} & \alpha:F\to G & \mbox{and} & \beta:G\to H & \mbox{suggest} & (\beta\cdot\alpha):F\to H		\\
																\\
\mbox{horizontal composition:} & \alpha:F\to G & \mbox{and} &
\beta:H\to K & \mbox{suggest} & (\beta\star\alpha):(H\circ F)\to (K\circ G)
\end{array} $$
Vertical composition is analogous to our common idea of composition, but horizontal composition is pretty out there
if you're not familiar with it. I won't go into the details as to how it actually works, just know it exists and is
made possible because natural transformation domains and codomains (being functors) are themselves composable.

Okay, so we now kind of know some category theory, great! But how do these specific ideas of composition help us in particular?

The last major idea to introduce from category theory is that of a \emph{monad}, which is an endofunctor
$ T:\mathcal{C}\to\mathcal{C} $ (a functor with equal domain and codomain) along with two of its own
specialized natural transformations:
$$ \begin{array}{rlcl}
\mbox{unit:}		& \eta:\mbox{Id}\to T		\\
\mbox{multiply:}	& \mu:T^2\to T
\end{array} $$
where Id$ : \mathcal{C}\to\mathcal{C} $ is the identity endofunctor and $ T^2 $ is just shortform for $ T\circ T $.

The relevance of a monad is it allows us to define a secondary composition
operator for the morphisms of a given category. This is done by way of defining what's called a \strong{Kleisli} category which
is built using some given monad $ (T, \eta, \mu) $. From this we then specify the secondary composition operator for morphisms as:
$$ \mbox{endopose:} \quad f:A\to T(B) \quad \mbox{and} \quad  g:B\to T(C) \quad \mbox{suggest} \quad (g\star f):A\to T(C) $$
we name this operator \strong{endopose}, the word being derived from the \emph{endofunctor} with which it is
built.\footnote{This is not to downplay the importance of the underying monad (as there exist endofunctors which are not monads),
rather it is more about designating a proper name for such secondary compositions. In particular the word \emph{endopose}
semantically aligns with the intended associations given that \emph{endo-} means ``within'' and \emph{poser} means ``to place''.}

The big idea here for us is that wherever we encounter composition (or some baseline interpretation of it), and we also
observe objects which don't quite compose but nearly do $ A\to T(B),\ B\to T(C) $, we can often generate a secondary
composition operator within the same context. I won't get into the math here, but this secondary monadic composition
is just as well behaved as the original in terms of its own compositional coherence properties.\footnote{These can be
proven using the horozontal composition properties of the monad's natural transformations $ \eta, \mu $. For this reason,
secondary composition inherits the horizontal composition notation $ \star $ as this further alludes to its origins.}

To reiterate, we now have two composition operators within the same context:
$$ \def\arraystretch{1.5}
\begin{array}{rlclcl}
\mbox{compose:} & f:A\to B	& \mbox{and} & g:B\to C		& \mbox{suggest} & (g\circ f):A\to C		\\
\mbox{endopose:} & f:A\to T(B)	& \mbox{and} & g:B\to T(C)	& \mbox{suggest} & (g\star f):A\to T(C)
\end{array} $$
though in practice we can have as many \emph{derivative} composition operators as there are relevant monads.

Secondary composition parallels its primary, so it becomes quite reasonable to include its own specializations:
$$ \def\arraystretch{1.5}
\begin{array}{rclcl}
\prendopose{f}(g)	& = & f^\star(g)	& := & g \star f		\\
\postndopose{f}(g)	& = & f_\star(g)	& := & f \star g
\end{array} $$

Next, there is one other operator which is so deeply connected to composition it's also worth introducing here:
$$ \def\arraystretch{1.5}
\begin{array}{rll}
\apply(f, x)	& :  & \hspace{-1em}(A\to B)\times A\ \to\ B		\\
		& := & f(x)						\\
		& \;= & f\ \mapsfrom\ x
\end{array} $$

This \strong{apply} operator takes a function $ f $, and some value $ x $ and \emph{applies} them to get their output
value. The main connection for us is that composition can be defined out of application:
$$ \def\arraystretch{1.5}
\begin{array}{rll}
\compose(g, f)	& :  & \hspace{-1em}(B\to C)\times (A\to B)\ \to\ (A\to C)		\\
		& := & \apply(g,\ \apply(f, -)\,)
\end{array} $$
where $ (-) $ is the variable placeholder for the composite function's input.

Actually, application can also be defined out of composition so we can consider them alternatives, but for our purposes we
will frequently use \texttt{apply} over \texttt{compose} because the \texttt{apply} operator ends up being more flexible for
the grammars we want to build.\footnote{By using the \texttt{apply} operator we can not only evaluate a function immediately
by providing a given argument $ x $, we can alternatively extend the function through composition allowing for the future
possibility of further compositions. In the context of function construction we consider this a higher entropy design.}
With that said, we will also often use them interchangeably when no confusion would otherwise arise.

Finally, given the duality between primary and secondary compositions (when they exist),
there is a parallel to the \texttt{apply} operator called \strong{bind}:\footnote{The Haskell programming language has no
name I'm aware of for \texttt{endopose}, but denotes it as $ \Hrfish $. It also makes strong use of the \texttt{bind}
operator denoting it as $ \Hrbind $. Such monadic theory is valuable in this line of programming languages as it allows
their compilers to automate the definitions of many type-safe and user-friendly grammatical constructs such as \strong{do}
based off these and other monadic constructs such as \texttt{join}, \texttt{return}, and \texttt{fmap}.}
$$ \def\arraystretch{1.5}
\begin{array}{rll}
\bind(f, x)	& :  & \hspace{-1em}(A\to T(B)\,)\times T(A) \to T(B)		\\
		& := & \mu(T(f)(x))						\\
		& \ = & f\langle x\rangle					\\
		& \ = & f\ \dashv\ x
\end{array} $$
In this case we can then properly define \texttt{endopose} as:
$$ \def\arraystretch{1.5}
\begin{array}{rll}
\ndopose(g, f)	& :  & \hspace{-1em}(B\to T(C)\,)\times (A\to T(B)\,)\ \to\ (A\to T(C)\,)		\\
		& := & \bind(g,\ \bind(f,\,\eta(-)\,)\,)
\end{array} $$

\subsubsection*{Lazy Construction}

There's one last set of operators worth introducing in this methodology section which are key points of grammar when
implementing what's known as the \strong{lazy programming paradigm}---which itself is something we'll need in the following.

In functional programming languages such as LISP, \emph{lazy evaluation} (also known as \emph{normal order evaluation})
is the idea that function application is achieved by only evaluating the parts of the function that are actually
used at the time that the function is called. In practice there are two operators needed to implement this paradigm:
$$ \def\arraystretch{1.5}
\begin{array}{rrl}
\delay(f, x)		& := & (f, x)				\\
\force((f, x))		& := & f(x)
\end{array} $$
their terminology comes from \cite{sicp}. Lazy evaluation is achieved by using \strong{delay} and \strong{force}
to decompose \texttt{apply}:
$$ \apply \ =\ \force \circ \delay $$

The operators \texttt{delay} and \texttt{force} are key, but for our purposes we will include a third:
$$ \transit(g) \qdefeq g \circ \force \quad =\ \force^\circ(g) $$
I have named this function \strong{transit} as for me it resembles logical \emph{transitivity} in its use.
Its value for us here is that it allows us to extend \texttt{force} so that our lazy evaluation can be
applied to a wider range of grammar.

\section*{Conditional Composition}

We are finally past the methodology section, but before we can get to proper forms of automata induction we need
to build up the endoposition operators for which they are defined. In particular these operators can be viewed
as forms of \strong{conditional composition}, and so this now becomes our focus.

\subsection*{The Operator Lattice}

There are several operators that will be introduced in this section which can be summarized with the following lattice:

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00 = (0,0);

pair p10 = segment(p00, "SL", 1, 60);
pair p11 = segment(p00, "SR", 1, 60);

pair p20 = segment(p10, "SL", 1, 60);
pair p21 = segment(p11, "SL", 1, 60);
pair p22 = segment(p11, "SR", 1, 60);

pair p30 = segment(p21, "SL", 1, 60);
pair p31 = segment(p21, "SR", 1, 60);

pair p40 = segment(p31, "SL", 1, 60);

//

pen linecolor = gray;

draw(p00--p10, linecolor, MidArrow(size = 4));
draw(p00--p11, linecolor, MidArrow(size = 4));

draw(p10--p20, linecolor, MidArrow(size = 4));
draw(p10--p21, linecolor, MidArrow(size = 4));
draw(p11--p21, linecolor, MidArrow(size = 4));
draw(p11--p22, linecolor, MidArrow(size = 4));

draw(p20--p30, linecolor, MidArrow(size = 4));
draw(p21--p30, linecolor, MidArrow(size = 4));
draw(p21--p31, linecolor, MidArrow(size = 4));
draw(p22--p31, linecolor, MidArrow(size = 4));

draw(p30--p40, linecolor, MidArrow(size = 4));
draw(p31--p40, linecolor, MidArrow(size = 4));

//

pen bordercolor = white;

safeLabel("if$_{\scriptscriptstyle\,(1,1)}$", p00, 0.60, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("hold$_{\scriptscriptstyle\,(1,2)}$", shift(0.25,0)*p10, 0.85, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("cohold$_{\scriptscriptstyle\,(2,1)}$", shift(0.2,0)*p11, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("pend$_{\scriptscriptstyle\,(1,3)}$", shift(0.4,0)*p20, 0.85, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("dihold$_{\scriptscriptstyle\,(2,2)}$", shift(0.3,0)*p21, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("copend$_{\scriptscriptstyle\,(3,1)}$", p22, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("stem$_{\scriptscriptstyle\,(2,3)}$", shift(0.25,0)*p30, 0.85, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("costem$_{\scriptscriptstyle\,(3,2)}$", shift(0.2,0)*p31, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("distem$_{\scriptscriptstyle\,(3,3)}$", shift(0.3,0)*p40, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}

This lattice represents a dependency narrative between these operators,
for which we start with the well known \strong{if} operator:
$$ \mbox{if}\,(\,boolean,\ antecedent,\ consequent)\, $$
This construct returns $ antecedent $ when the $ boolean $ value is \emph{true}, otherwise it returns $ consequent $
when it's \emph{false}. This is the ``conditional'' in these composition operators. In terms of the lattice, the operator
\texttt{if} is subscripted with $ (1,1) $ which signifies it accepts $ 1 $ argument on its \strong{antecedent side} and $ 1 $
on its \strong{consequent side}. In particular the arguments it accepts are the \emph{antecendent} and \emph{consequent}
themselves. The meaning of this will become clearer as we continue.

The next step up is to equip this conditional operator with what can be called aspects of composition. The following four
functions \strong{hold}, \strong{cohold}, \strong{pend}, \strong{copend} are the first extensions of \texttt{if}:

$$ \def\arraystretch{1.15}
\tab[-1.5cm] \begin{array}{ll}

(\define\ (\hold\ true?\ d\ e\ f)		\col[3cm] (\define\ (\pend\ true?\ d\ e\ f\ g)			\\
\tab[1.15cm] (\mbox{if}\ (true?)\ d		\col[3cm] \tab[1.15cm] (\mbox{if}\ (true?)\ d			\\
\tab[1.60cm]  (\delay\ e\ f)))			\col[3cm] \tab[1.60cm]  (\delay					\\
						\col[3cm] \tab[2.05cm]  (\transit\ e)				\\
						\col[3cm] \tab[2.05cm]  (\delay\ f\ g))				\\
						\col[3cm] \tab[1.15cm] ))					\\[0.75cm]

(\define\ (\cohold\ true?\ d\ e\ f)		\col[3cm] (\define\ (\copend\ true?\ d\ e\ f\ g)		\\
\tab[1.15cm] (\mbox{if}\ (true?)		\col[3cm] \tab[1.15cm] (\mbox{if}\ (true?)			\\
\tab[1.60cm]  (\delay\ d\ e)			\col[3cm] \tab[1.60cm]  (\delay					\\
\tab[1.80cm]  f					\col[3cm] \tab[2.05cm]  (\transit\ d)				\\
\tab[1.15cm] ))					\col[3cm] \tab[2.05cm]  (\delay\ e\ f))				\\
						\col[3cm] \tab[1.60cm]  g					\\
						\col[3cm] \tab[1.15cm] ))					\\[0.25cm]
\end{array} $$

Truth be told these operators serve more as stepping stones and placeholders as I have not yet found other uses
for them. I introduce them here because they create a smoother transition in building the remaining operators,
and later when we perform certain optimizations they act as convenient boundary references.

As for their nomenclature, the word ``hold'' is intended to suggest connotations along the lines of
``the \emph{holding} of a transaction'' (until certain conditions are met). The word ``pend'' is similar,
but instead of a single transaction its word associations suggest ``more than one operation is \emph{pending}.''
As for the prefix ``co-'' it's actually analogous to traditional word use of the \emph{sine} and \emph{co-sine}
trigonometric functions, or \emph{product} and \emph{co-product} in category theory: Strictly one could be
defined by the other, but we have both because it's practical to have both.

\subsection*{Dihold}

The first of our serious conditional composition operators is called \strong{dihold}, and is defined as follows:
\ \\

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00 = (0,0);
pair p10 = segment(p00, "S", 1.5);
pair p20 = segment(p10, "S", 1);

pair t00 = segment(p20, "SL", 1, 70);

pair b00 = segment(p20, "S", 1);
pair b10 = segment(b00, "SL", 1, 40);
pair b11 = segment(b00, "SR", 1, 40);

pair c00 = segment(p20, "SR", 1, 70);
pair c10 = segment(c00, "SL", 1, 40);
pair c11 = segment(c00, "SR", 1, 40);

//

pen stepcolor = gray;
pen fillcolor = lightgray;
pen bordercolor = heavygray;

draw(p10--p20, bordercolor);
draw(p20--t00, bordercolor);
draw(p20--b00, bordercolor);
draw(p20--c00, bordercolor);

draw(b00--b10, bordercolor);
draw(b00--b11, bordercolor);

draw(c00--c10, bordercolor);
draw(c00--c11, bordercolor);

//

label("dihold$(true?, d, e, f, g)$:", p00);

label("\scriptsize $1$", shift(0.12, 0.4)*p20, stepcolor);

label("\scriptsize $1$", shift(1.50, 0.40)*t00, stepcolor);

label("\scriptsize $2$", shift(0.12, 0.42)*b00, stepcolor);
label("\scriptsize $1$", shift(0.35, 0.20)*b10, stepcolor);
label("\scriptsize $2$", shift(-0.36, 0.20)*b11, stepcolor);

label("\scriptsize $3$", shift(-1.50, 0.40)*c00, stepcolor);
label("\scriptsize $1$", shift(0.35, 0.20)*c10, stepcolor);
label("\scriptsize $2$", shift(-0.35, 0.20)*c11, stepcolor);

safeLabel("force", p10, 0.60, 0.25, borderpen = bordercolor);
safeLabel("if", p20, 0.25, 0.25, borderpen = bordercolor);

safeLabel("$true?$", t00, 0.65, 0.26, borderpen = bordercolor);

safeLabel("delay", b00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$d$", b10, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$e$", b11, 0.25, 0.25, borderpen = bordercolor);

safeLabel("delay", c00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$f$", c10, 0.25, 0.27, borderpen = bordercolor);
safeLabel("$g$", c11, 0.25, 0.25, borderpen = bordercolor);

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}

\noindent To contrast this operator with \texttt{hold} and \texttt{cohold} it instead ``holds'' on both sides of its
conditional. This provides us with a unique opportunity to extend the \emph{lazy paradigm} since either output will
be a delay, which for us means there's no issue in forcing their respective compositions or evaluations. Also,
notice in the above lattice that \texttt{dihold} is scripted with $ (2,2) $ and that here it takes $ 2 $ arguments
on its \emph{antecedent side} and $ 2 $ on its \emph{consequent side}. This is the meaning behind this pattern,
which will be used later when we optimize.

If we were to code \texttt{dihold} in the LISP programming style while
also building on its predecessors it would be along these lines:
$$ \def\arraystretch{1.15}
\tab[-8cm] \begin{array}{l}
(\define\ (\dihold\ true?\ d\ e\ f\ g)				\\
\tab[1.15cm] (\force						\\
\tab[1.60cm]  (\hold\ true?\ (\delay\ d\ e)\ f\ g)		\\
\tab[1.15cm]  ))
\end{array} $$

\subsection*{The Stem Operator}

The next conditional composition operator is an extension of \texttt{dihold} designed to be more suitable for
\emph{continuation passing}, which is a valuable paradigm to us as it happens to be a known monad.

Intuitively, continuation passing works when you have a function $ f(x, c) $ which not only takes its expected input $ x $,
it takes a \emph{continuation} function for which the output is passed. Think of it like this: We take the function $ f $
and input value $ x $ and partially apply them. The internal definition of $ f $ is such that we get some partial output
$ y $, but because our function $ f $ is of continuation passing style we then pass this partial output to the
continuation function:
$$ f(x, c) \qdefeq c(\hat{f}(x)) $$
where $ \hat{f}(x) $ represents the partial application output.

Next, as continuation passing style is known to be a monad, I present here an informal definition of what would be the
\emph{endoposition} of two continuation passing functions:
$$ f(x, c_1(y))\ \star\ g(y, c_2(z)) \qdefeq f(\,x,\ \lambda y.g(y)(c_2(z))\,) $$
Here the use of the lambda symbol ($ \lambda $) is as in the Lambda Calculus---a way to represent \emph{anonymous} functions.
As well, the $ g(y) $ component is the partial application known as \emph{currying}.

As for the definition of the \texttt{stem} operator, it is as follows:

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00 = (0,0);
pair p10 = segment(p00, "S", 1.5);
pair p20 = segment(p10, "S", 1);

pair t00 = segment(p20, "SL", 1, 70);

pair b00 = segment(p20, "S", 1);
pair b10 = segment(b00, "SL", 1, 40);
pair b11 = segment(b00, "SR", 1, 45);

pair c00 = segment(p20, "SR", 1, 78);
pair c10 = segment(c00, "SL", 1, 60);
pair c11 = segment(c00, "SR", 1, 60);
pair c20 = segment(c10, "S", 1);
pair c21 = segment(c11, "SL", 1, 45);
pair c22 = segment(c11, "SR", 1, 45);

//

pen stepcolor = gray;
pen fillcolor = lightgray;
pen bordercolor = heavygray;

draw(p10--p20, bordercolor);
draw(p20--t00, bordercolor);
draw(p20--b00, bordercolor);
draw(p20--c00, bordercolor);

draw(b00--b10, bordercolor);
draw(b00--b11, bordercolor);

draw(c00--c10, bordercolor);
draw(c00--c11, bordercolor);
draw(c10--c20, bordercolor);
draw(c11--c21, bordercolor);
draw(c11--c22, bordercolor);

//

label("stem$(true?, d, e, f, g, h)$:", p00);

label("\scriptsize $1$", shift(0.12, 0.4)*p20, stepcolor);

label("\scriptsize $1$", shift(1.45, 0.38)*t00, stepcolor);

label("\scriptsize $2$", shift(0.12, 0.41)*b00, stepcolor);
label("\scriptsize $1$", shift(0.36, 0.20)*b10, stepcolor);
label("\scriptsize $2$", shift(-0.43, 0.20)*b11, stepcolor);

label("\scriptsize $3$", shift(-2.52, 0.37)*c00, stepcolor);
label("\scriptsize $1$", shift(0.87, 0.20)*c10, stepcolor);
label("\scriptsize $2$", shift(-0.77, 0.20)*c11, stepcolor);

label("\scriptsize $1$", shift(0.12, 0.4)*c20, stepcolor);
label("\scriptsize $1$", shift(0.40, 0.15)*c21, stepcolor);
label("\scriptsize $2$", shift(-0.40, 0.15)*c22, stepcolor);

safeLabel("force", p10, 0.60, 0.25, borderpen = bordercolor);
safeLabel("if", p20, 0.25, 0.25, borderpen = bordercolor);

safeLabel("$true?$", t00, 0.65, 0.26, borderpen = bordercolor);

safeLabel("delay", b00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$d$", b10, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$e$", b11, 0.25, 0.25, borderpen = bordercolor);

safeLabel("delay", c00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("transit", c10, 0.80, 0.25, borderpen = bordercolor);
safeLabel("delay", c11, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$f$", c20, 0.25, 0.27, borderpen = bordercolor);
safeLabel("$g$", c21, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$h$", c22, 0.25, 0.25, borderpen = bordercolor);

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
In this case we have the same \emph{branching} pattern as with \texttt{dihold}, but now if the boolean value is
\emph{false} and we return the \emph{consequent} instead of the \emph{antecedent} of the conditional \texttt{if},
we not only force the function and value pair $ g, h $, we also pass them as input to the function $ f $.

In LISP style, and based off our above \texttt{dihold} definition, \texttt{stem} would be coded as:

$$ \def\arraystretch{1.15}
\tab[-6cm] \begin{array}{l}
(\define\ (\stem\ true?\ d\ e\ f\ g\ h)						\\
\tab[1.15cm] (\dihold\ true?\ d\ e\ (\transit\ f)\ (\delay\ g\ h))		\\
)
\end{array} $$
This operator is interesting in many ways in its own right, in fact I call it the \texttt{stem} operator in analogy
to \emph{stem cells} in biology: As a function model it can specialize to several important compositional patterns---such
as recursion---known in existing computational theories. Yet, for our purposes here its greatest value is in its ability
to chain compose with itself.

To simplify notation a little, I now introduce \texttt{stem}'s conditional endopose operators.

\subsubsection*{Stem's CPose Operators}

We start by taking the \texttt{stem} operator:
$$ \stem(\,policy?,\ break,\ arg_1,\ cont,\ next,\ arg_2\,) $$
Now, we refactor
$$ arg_1,\,arg_2 $$
and partially apply
$$ policy?,\ break,\ next $$
to derive a function in continuation passing style:
$$ \langle\,policy?,\ break,\ next\,\rangle(\,arg,\ cont\,) \quad := \quad \stem(\,policy?,\ break,\ arg,\ cont,\ next,\ arg\,)	$$
Note that $ \langle\,policy?,\ break,\ next\,\rangle $ represents the function name. This function is in what I would call
\emph{conditional continuation passing style}. In particular, if we adhere to the rule that we don't apply or simplify the
\texttt{stem} operator directly, then all the monadic rules of continuation passing hold. I mentioned \emph{conditional
endoposition} above, which I can now explain as being in the sense that once we allow ourselves to evaluate the internal
\texttt{stem} operators, we then have to accept that any chain of conditionally endoposed functions may \emph{break} before
all of such functions are called.

From such monadic extensions we can now introduce \texttt{stem}'s \strong{cpose} operators:

$$ \def\arraystretch{1.75}
\small
\begin{array}{lllcl}
\langle\,true?,\ break,\ next\,\rangle		& \spose{call call}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ -_{arg},\ cont,\ next,\ -_{arg}\,)		\\

\langle\,true?,\ break,\ next\,\rangle		& \spose{call call}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ -_{arg},\ cont,\ next,\ -_{arg}\,)		\\

\langle\,true?,\ break,\ x\,\rangle		& \spose{call pass}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ -_{arg},\ cont,\ -_{arg},\ x\,)		\\

\langle\,true?,\ break,\ next,\ x\,\rangle	& \spose{call pose}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ -_w,\ cont,\ next,\ x\,)			\\

\langle\,true?,\ w,\ next\,\rangle		& \spose{pass call}							&
cont & \qdefeq					& \stem(\,true?,\ -_{arg},\ w,\ cont,\ next,\ -_{arg}\,)		\\

\langle\,true?,\ w,\ x\,\rangle			& \spose{pass pass}							&
cont & \qdefeq					& \stem(\,true?,\ -_{arg},\ w,\ cont,\ -_{arg},\ x\,)			\\

\langle\,true?,\ w,\ next,\ x\,\rangle		& \spose{pass pose}							&
cont & \qdefeq					& \stem(\,true?,\ -_{break},\ w,\ cont,\ next,\ x\,)			\\

\langle\,true?,\ break,\ w,\ next\,\rangle	& \spose{pose call}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ w,\ cont,\ next,\ -_x\,)			\\

\langle\,true?,\ break,\ w,\ x\,\rangle		& \spose{pose pass}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ w,\ cont,\ -_{next},\ x\,)			\\

\langle\,true?,\ break,\ w,\ next,\ x\,\rangle	& \spose{pose pose}							&
cont & \qdefeq					& \stem(\,true?,\ break,\ w,\ cont,\ next,\ x\,)
\end{array} $$

\ \\
This is a lot to take in, so let's go over the details.

For starters, given the informational complexity presented in these endoposition definitions, I have chosen to display
them here while hiding the details of the right side argument (which is its own continuation passing function) replacing
them with the condensed format of $ cont $. Although I won't prove it here, these conditional endopose operators
$ \textcolor{blue}{\star} $ really do satisfy the continuation passing composition properties described
previously.\footnote{I have worked out a pencil and paper proof myself, it's fairly tedious, but true.}

Next, the generic or umbrella term for all the \texttt{cpose} operators in the above list is titled \strong{closing}.
The nomenclature here comes from the following naming convention regarding set \emph{intervals} in math:
$$ \def\arraystretch{1.25}
\begin{array}{ll}
(a, b)		\col[1cm] \mbox{names an \strong{open} interval.}		\\{} % This empty scope is here to prevent a bug.
[\,a, b)	\col[1cm] \mbox{names a \strong{closing} interval.}		\\{} % This empty scope is here to prevent a bug.
[\,a, b\,]	\col[1cm] \mbox{names a \strong{closed} interval}		\\
(a, b\,]	\col[1cm] \mbox{names an \strong{opening} interval}
\end{array} $$
The idea is, starting with an \emph{open} interval, if we were to begin \emph{closing} it we would do so from
left-to-right which coincides with its conventional read order. Similarly when we start with a \emph{closed} interval
and we begin \emph{opening} it we do so from left-to-right as well. Ideas of ``openness'' and ``closedness'' are relevant
here in the sense that these \ \texttt{cpose closing}\ \ operators are \emph{closed to continuing} when the boolean value
is \emph{true}, and are otherwise \emph{open to continuing} when it's \emph{false}.

Beyond this, each \texttt{cpose} operator is bottom-scripted with two words corresponding to the branching behaviours
of the endoposition: They indicate how we compose the operator's input. For example \strong{call call} indicates that
when we break (based on \texttt{stem}'s definition) we do so by \emph{calling} the $ break $ function to the as of yet
named value $ -_{arg} $. Of the two bottom-script words, this behaviour is specified by the left \texttt{call}.
If instead this \texttt{stem} operator continues, it does so by \emph{calling} the function $ next $ to our
unnamed input $ -_{arg} $. This is specified by the bottom-script right \texttt{call}.

To contrast \texttt{call call}, the \strong{pass call} operator acts similarly, but upon breaking it instead \emph{passes}
the $ break $ function to the $ -_{arg} $ value, which in this context must be a function of its own. In fact this brings up
another point, that the standard interpretation when evaluating these \texttt{cpose}s is that of composing functions, but in
practice we often also use it to express the application of function and value. The terminology for \emph{call} and \emph{pass}
come from the composition
specializations previously introduced:
$$ \def\arraystretch{1.5}
\begin{array}{rclcll}
\precompose{f}(g)	& = & f^\circ(g) & := & g \circ f \col[2cm] \mbox{(``$ f $ is \emph{passed} to $ g $'')}		\\
\postcompose{f}(g)	& = & f_\circ(g) & := & f \circ g \col[2cm] \mbox{(``$ f $ is \emph{called} for $ g $'')}
\end{array} $$

Finally, if you'll notice, some of the above \texttt{cpose}s contain the \strong{pose} modifier instead of \texttt{call} or
\texttt{pass}. In practice the situation occasionally arises where we don't actually want to call or pass the $ -_{arg} $
input value, but simply want to return a fixed composition or application. In such cases we make use of \texttt{pose}.
Its name being short for the baseline com\emph{pose} operator.

This whole naming system might seem a bit elaborate at the moment, but it really does make things simpler in the long
run: Not only will it come up again with the remaining conditional endoposes, but it will also be used to standardize
the nomenclature for a class of automata induction notation introduced later on. In anycase, the thing to note
is that the above cpose operators are really just variations of each other, all of which afford us the ability
to chain compose the \texttt{stem} operator with itself, and even more so, alternative instances of itself.

So, we've explored the naming system, but just how can these \texttt{cpose} operators lead us to better forms of
\emph{conditional composition}? At this point the easiest explanation might actually be just to show an example,
so here goes:
$$ \def\arraystretch{1.25}
\begin{array}{lr}
\langle\,true?_0,\ break_0,\ f_0\,\rangle		& \quad\spose{call call}	\\
\langle\,true?_1,\ break_1,\ f_1\,\rangle		& \quad\spose{call call}	\\
\langle\,true?_2,\ break_2,\ f_2\,\rangle		& \quad\spose{call call}	\\

\hspace{1.5cm}\vdots						    			\\

\langle\,true?_n,\ break_n,\ f_n\,\rangle		& \quad\spose{call call}	\\
\hspace{1ex}\,\id\ \ x
\end{array} $$
Here \texttt{id} doesn't add anything meaningful to the composition except that by convention we are using it to represent
the halting of what is otherwise a continuation passing process. The $ x $ value following \texttt{id} is applied at the very
end once the final \emph{composed} function is built. Beyond this, the major thing to note about such a pattern is that
in the special case when all its $ true? $ values fail, we end up with a chain composition:
$$ f_n \circ f_{n-1} \circ \ldots \circ f_1 \circ f_0\,(x) $$
To understand this let's translate the entire construct back to its definition:
$$ \def\arraystretch{1.2}
\begin{array}{lcl}
		&   & \stem(\,true?_0,\ break_0,\ x,\ cont_0,\ f_0,\ x\,)					\\
cont_0		& = & \stem(\,true?_1,\ break_1,\ -_{arg},\ cont_1,\ f_1,\ -_{arg}\,)				\\
cont_1		& = & \stem(\,true?_2,\ break_2,\ -_{arg},\ cont_2,\ f_2,\ -_{arg}\,)				\\

		&\vdots&											\\

cont_{n-1}	& = & \stem(\,true?_n,\ break_n,\ -_{arg},\ cont_n,\ f_n,\ -_{arg}\,)				\\
cont_n		& = & \id
\end{array} $$
On the one hand, we can start at the end with the identity function \texttt{id} and chain these \texttt{cposes}
from that direction. We can do this on account that these cposes are (unconditionally) \emph{right associative}.
From this theoretical underpinning, we then properly define each
$$ cont_n,\ cont_{n-1},\ \ldots,\ cont_1,\ cont_0 $$
in sequence. Based off the definition of \texttt{stem} this should leave us with a single (if not large)
function to which we can finally apply the value $ x $. How then does this turn into a chain composition?

So on the other hand, as an alternative strategy we could have started out from the top of the sequence and tried composing
that way. So long as we compose symbolically without evaluating the internal \texttt{stem} operators this works, but otherwise
these \texttt{cpose} operators start to behave a lot stranger than we'd like. In general the forward approach is less revealing.

The exception here is when we restrict ourselves to these expressions where we know we will never break---where the $ true? $
values are always \emph{false}: In such cases we would start by checking the boolean value of the first \texttt{stem} in the
chain, and we'd discover the $ true? $ value to fail. As such, and given the nature of \texttt{stem}'s definition as a
conditional, we wouldn't actually need to compose the whole operator with the next \texttt{stem} in line. With such
foreknowledge we could instead dump the excess baggage (so to speak), and continue applying what remains. That's
how we get the above chain composition.

In fact, given the regularity of the interpretation here we can simplify this whole sequence into the following
formal grammatical pattern:
$$ \def\arraystretch{1.2}
\begin{array}{llll}
\bfmbox{closing}	\col[5ex] \bfmbox{call}		\col[5ex] \bfmbox{call}	\col[5ex] x	\\
true?_0			\col[5ex] break_0		\col[5ex] f_0				\\
true?_1			\col[5ex] break_1		\col[5ex] f_1				\\
true?_2			\col[5ex] break_2		\col[5ex] f_2				\\

\tab[2.5ex] \vdots	\col[7.5ex] \vdots		\col[5.5ex] \vdots			\\

true?_n			\col[5ex] break_n		\col[5ex] f_n				\\
\bfmbox{closed}
\end{array} $$
where \strong{closed} (at the bottom) is a special \texttt{cpose} that halts regardless of its \emph{boolean} input value.
As well, if its options aren't specified it defaults to the identity \texttt{id}.

\subsection*{The Costem Operator}

As with \texttt{hold} and \texttt{pend}, \texttt{stem} also has a dual operator:
\ \\

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00 = (0,0);
pair p10 = segment(p00, "S", 1.5);
pair p20 = segment(p10, "S", 1);

pair t00 = segment(p20, "SL", 1, 70);

pair b00 = segment(p20, "S", 1);
pair b10 = segment(b00, "SL", 1, 40);
pair b11 = segment(b00, "SR", 1, 60);
pair b20 = segment(b10, "S", 1);
pair b21 = segment(b11, "SL", 1, 40);
pair b22 = segment(b11, "SR", 1, 40);

pair c00 = segment(p20, "SR", 1, 78);
pair c10 = segment(c00, "SL", 1, 40);
pair c11 = segment(c00, "SR", 1, 40);

//

pen stepcolor = gray;
pen fillcolor = lightgray;
pen bordercolor = heavygray;

draw(p10--p20, bordercolor);
draw(p20--t00, bordercolor);
draw(p20--b00, bordercolor);
draw(p20--c00, bordercolor);

draw(b00--b10, bordercolor);
draw(b00--b11, bordercolor);
draw(b10--b20, bordercolor);
draw(b11--b21, bordercolor);
draw(b11--b22, bordercolor);

draw(c00--c10, bordercolor);
draw(c00--c11, bordercolor);

//

label("costem$(true?, d, e, f, g, h)$:", p00);

label("\scriptsize $1$", shift(0.12, 0.4)*p20, stepcolor);

label("\scriptsize $1$", shift(1.47, 0.38)*t00, stepcolor);

label("\scriptsize $2$", shift(0.12, 0.42)*b00, stepcolor);
label("\scriptsize $1$", shift(0.50, 0.38)*b10, stepcolor);
label("\scriptsize $2$", shift(-0.93, 0.38)*b11, stepcolor);

label("\scriptsize $1$", shift(0.12, 0.4)*b20, stepcolor);
label("\scriptsize $1$", shift(0.35, 0.20)*b21, stepcolor);
label("\scriptsize $2$", shift(-0.35, 0.20)*b22, stepcolor);

label("\scriptsize $3$", shift(-2.45, 0.37)*c00, stepcolor);
label("\scriptsize $1$", shift(0.35, 0.20)*c10, stepcolor);
label("\scriptsize $2$", shift(-0.35, 0.20)*c11, stepcolor);

safeLabel("force", p10, 0.60, 0.25, borderpen = bordercolor);
safeLabel("if", p20, 0.25, 0.25, borderpen = bordercolor);

safeLabel("$true?$", t00, 0.65, 0.26, borderpen = bordercolor);

safeLabel("delay", b00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("transit", b10, 0.80, 0.25, borderpen = bordercolor);
safeLabel("delay", b11, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$d$", b20, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$e$", b21, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$f$", b22, 0.25, 0.27, borderpen = bordercolor);

safeLabel("delay", c00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$g$", c10, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$h$", c11, 0.25, 0.25, borderpen = bordercolor);

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
Like its predecessors \strong{costem} is provided for convenience and performance
(potentially; in some situations). In LISP it would be encoded as:

$$ \def\arraystretch{1.15}
\tab[-6cm] \begin{array}{l}
(\define\ (\costem\ true?\ d\ e\ f\ g\ h)					\\
\tab[1.15cm] (\dihold\ true?\ (\transit\ d)\ (\delay\ e\ f)\ g\ h)		\\
)
\end{array} $$
If one wishes we could alternatively implement \texttt{costem} using its dual by negating the $ true? $ value.
Either way, and given this duality, \texttt{costem} has its own \texttt{cpose} operators in parallel to \texttt{stem}:

\subsubsection*{Costem's CPose Operators}

$$ \def\arraystretch{1.75}
\small
\begin{array}{lllcl}
\langle\,true?,\ next,\ break\,\rangle		& \cpose{call call}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ next,\ -_{arg},\ break,\ -_{arg}\,)		\\

\langle\,true?,\ next,\ w\,\rangle		& \cpose{call pass}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ next,\ -_{arg},\ -_{arg},\ w\,)		\\

\langle\,true?,\ next,\ break,\ w\,\rangle	& \cpose{call pose}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ next,\ -_x,\ break,\ w\,)			\\

\langle\,true?,\ x,\ break\,\rangle		& \cpose{pass call}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ -_{arg},\ x,\ break,\ -_{arg}\,)		\\

\langle\,true?,\ x,\ w\,\rangle			& \cpose{pass pass}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ -_{arg},\ x,\ -_{arg},\ w\,)			\\

\langle\,true?,\ x,\ break,\ w\,\rangle		& \cpose{pass pose}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ -_{next},\ x,\ break,\ w\,)			\\

\langle\,true?,\ next,\ x,\ break\,\rangle	& \cpose{pose call}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ next,\ x,\ break,\ -_w\,)			\\

\langle\,true?,\ next,\ x,\ w\,\rangle		& \cpose{pose pass}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ next,\ x,\ -_{break},\ w\,)			\\

\langle\,true?,\ next,\ x,\ break,\ w\,\rangle	& \cpose{pose pose}							&
cont & \qdefeq					& \costem(\,true?,\ cont,\ next,\ x,\ break,\ w\,)
\end{array} $$

\subsection*{The Distem Operator}

Finally, we end this section on conditional composition with the \strong{distem} operator:

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00 = (0,0);
pair p10 = segment(p00, "S", 1.5);
pair p20 = segment(p10, "S", 1);

pair t00 = segment(p20, "SL", 1, 70);

pair b00 = segment(p20, "S", 1);
pair b10 = segment(b00, "SL", 1, 40);
pair b11 = segment(b00, "SR", 1, 60);
pair b20 = segment(b10, "S", 1);
pair b21 = segment(b11, "SL", 1, 40);
pair b22 = segment(b11, "SR", 1, 40);

pair c00 = segment(p20, "SR", 1, 78);
pair c10 = segment(c00, "SL", 1, 40);
pair c11 = segment(c00, "SR", 1, 60);
pair c20 = segment(c10, "S", 1);
pair c21 = segment(c11, "SL", 1, 40);
pair c22 = segment(c11, "SR", 1, 40);

//

pen stepcolor = gray;
pen fillcolor = lightgray;
pen bordercolor = heavygray;

draw(p10--p20, bordercolor);
draw(p20--t00, bordercolor);
draw(p20--b00, bordercolor);
draw(p20--c00, bordercolor);

draw(b00--b10, bordercolor);
draw(b00--b11, bordercolor);
draw(b10--b20, bordercolor);
draw(b11--b21, bordercolor);
draw(b11--b22, bordercolor);

draw(c00--c10, bordercolor);
draw(c00--c11, bordercolor);
draw(c10--c20, bordercolor);
draw(c11--c21, bordercolor);
draw(c11--c22, bordercolor);

//

label("distem$(true?, d, e, f, g, h, i)$:", p00);

label("\scriptsize $1$", shift(0.12, 0.4)*p20, stepcolor);

label("\scriptsize $1$", shift(1.47, 0.38)*t00, stepcolor);

label("\scriptsize $2$", shift(0.12, 0.42)*b00, stepcolor);
label("\scriptsize $1$", shift(0.50, 0.38)*b10, stepcolor);
label("\scriptsize $2$", shift(-0.93, 0.38)*b11, stepcolor);

label("\scriptsize $1$", shift(0.12, 0.4)*b20, stepcolor);
label("\scriptsize $1$", shift(0.35, 0.20)*b21, stepcolor);
label("\scriptsize $2$", shift(-0.35, 0.20)*b22, stepcolor);

label("\scriptsize $3$", shift(-2.45, 0.37)*c00, stepcolor);
label("\scriptsize $1$", shift(0.50, 0.38)*c10, stepcolor);
label("\scriptsize $2$", shift(-0.93, 0.38)*c11, stepcolor);

label("\scriptsize $1$", shift(0.12, 0.4)*c20, stepcolor);
label("\scriptsize $1$", shift(0.35, 0.20)*c21, stepcolor);
label("\scriptsize $2$", shift(-0.35, 0.20)*c22, stepcolor);

safeLabel("force", p10, 0.60, 0.25, borderpen = bordercolor);
safeLabel("if", p20, 0.25, 0.25, borderpen = bordercolor);

safeLabel("$true?$", t00, 0.65, 0.26, borderpen = bordercolor);

safeLabel("delay", b00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("transit", b10, 0.80, 0.25, borderpen = bordercolor);
safeLabel("delay", b11, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$d$", b20, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$e$", b21, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$f$", b22, 0.25, 0.27, borderpen = bordercolor);

safeLabel("delay", c00, 0.65, 0.27, borderpen = bordercolor);
safeLabel("transit", c10, 0.80, 0.25, borderpen = bordercolor);
safeLabel("delay", c11, 0.65, 0.27, borderpen = bordercolor);
safeLabel("$g$", c20, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$h$", c21, 0.25, 0.25, borderpen = bordercolor);
safeLabel("$i$", c22, 0.25, 0.25, borderpen = bordercolor);

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
which in LISP becomes:

$$ \def\arraystretch{1.15}
\tab[-6cm] \begin{array}{l}
(\define\ (\distem\ true?\ d\ e\ f\ g\ h\ i)					\\
\tab[1.15cm] (\stem\ true?\ (\transit\ d)\ (\delay\ e\ f)\ g\ h\ i))		\\
)
\end{array} $$

The importance of the \texttt{distem} operator is that it extends \emph{continuation passing} to both sides of the conditional:
Instead of the possibility of $ break $ing as with \texttt{stem} or \texttt{costem} it continues no matter what. In the special
case where both continuations are the same, this allows for \emph{chain alternation} which greatly increases our expressivity
to build functions in the long run. As for the situation where the two continuations are in fact different (known as
\emph{chain branching}), it's certainly possible, but we can also achieve the same effect with variables and a memory
system for which to split up our grammatical source code into smaller parts. This is likely a better design.

In anycase, when restricted to a single continuation, \texttt{distem}
like its predecessor \texttt{stem}s also has its own \texttt{cpose} operators.

\subsubsection*{Distem's CPose Operators}

$$ \def\arraystretch{1.75}
\small
\begin{array}{lllcl}
\langle\,true?,\ next_1,\ next_2\,\rangle		& \dpose{call call}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ -_{arg},\ cont,\ next_2,\ -_{arg}\,)	\\

\langle\,true?,\ next_1,\ x_2\,\rangle			& \dpose{call pass}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ -_{arg},\ cont,\ -_{arg},\ x_2\,)		\\

\langle\,true?,\ next_1,\ next_2,\ x_2\,\rangle		& \dpose{call pose}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ -_{x_1},\ cont,\ next_2,\ x_2\,)		\\

\langle\,true?,\ x_1,\ next_2\,\rangle			& \dpose{pass call}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ -_{arg},\ x_1,\ cont,\ next_2,\ -_{arg}\,)		\\

\langle\,true?,\ x_1,\ x_2\,\rangle			& \dpose{pass pass}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ -_{arg},\ x_1,\ cont,\ -_{arg},\ x_2\,)		\\

\langle\,true?,\ x_1,\ next_2,\ x_2\,\rangle		& \dpose{pass pose}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ -_{next_1},\ x_1,\ cont,\ next_2,\ x_2\,)		\\

\langle\,true?,\ next_1,\ x_1,\ next_2\,\rangle		& \dpose{pose call}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ next_2,\ -_{x_2}\,)		\\

\langle\,true?,\ next_1,\ x_1,\ x_2\,\rangle		& \dpose{pose pass}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ -_{next_2},\ x_2\,)		\\

\langle\,true?,\ next_1,\ x_1,\ next_2,\ x_2\,\rangle	& \dpose{pose pose}								&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ next_2,\ x_2\,)		\\
\end{array} $$

\section*{Induction Automata Correspondences}

We now have enough of a basis to understand our first class of induction operators.

\subsection*{Regular Induction}

The first layer of functions that we can create an inductive grammar for are what I would call \strong{regular functions}.
The local correspondence is with \emph{regular automata}, which informs the naming convention here. As for regular
automata themselves, we will in fact be making use of an equivalent known as \emph{regular expressions}.

As a quick review, regular expressions are recursively defined objects. Starting with their initial specification they are
first and foremost strings (made up of characters), but are otherwise built from such strings using three core operators:

\begin{enumerate}
\item {\bfseries catenation} - If $ a, b $ are regular expressions, then $ ab $ is a regular expression.
\item {\bfseries alternation} - If $ a, b $ are regular expressions, then $ (a\,|\,b) $ is a regular expression.
\item {\bfseries repetition} - If $ a $ is a regular expression, then $ a^* $ is a regular expression.
\end{enumerate}

If you've used regular expressions before you might protest that there are many more expression operators provided within
actual programming languages. The majority of these are either syntactic sugar (convenience operators) derived from the
above three, or they are custom extensions which may be useful in practice but otherwise deviate from the underlying
mathematical theory. Either way, we will not delve into such variations here.

Finite state automata at their most basic are \emph{recognizers}, also known as \emph{acceptors}. Without going into
the underlying details of how they work, regular automata are machines with a finite and fixed memory system which
they use to recognize strings given to them. In particular they are only able to recognize those strings which
were constructed by the above operators (given some initial alphabet).

The point for us though is not to be able to read strings of chain composed functions, but to write them.

\subsubsection*{Catenation}

The writer form of catenation is straightforward function composition,
which is immediately achieved using the \texttt{apply} operator:
$$ \readright f_0f_1 \grayt{\ldots} f_{n-1}f_n \qdefeq
   \grayt{\apply(} f_n \grply f_{n-1} \grply \grayt{\ldots}\,\grply f_1 \grply f_0 \grayt{,}\ \grayt{-)) \ldots )))} $$
We could of course also make use of \texttt{bind} operators when their respective monads exist for them:
$$ \readright f_0f_1 \grayt{\ldots} f_{n-1}f_n \qdefeq
   \grayt{\bind(} f_n \grbnd f_{n-1} \grbnd \grayt{\ldots}\,\grbnd f_1 \grbnd f_0 \grayt{,}\ \grayt{\eta(-))) \ldots )))} $$
In this sense catenation parallels tuple construction by means of recursively nested pairs:
$$ \begin{array}{rrl}
\grayt{(} f_n \grayt{,}\ f_{n-1} \grayt{, \ldots,}\ f_1 \grayt{,}\ f_0 \grayt{)}
 & := & \grayt{\cons(} f_n \grcns f_{n-1} \grcns \grayt{\ldots}\,\grcns f_1 \grcns f_0 \grayt{,}\ \grayt{\mbox{null})) \ldots )))}	\\
																	\\
 & = & \grayt{(} f_n \grayt{,\ (} f_{n-1} \grayt{,\ (\ldots, (} f_1 \grayt{,\ (} f_0 \grayt{,\ \mbox{null}\,)) \ldots )))}
\end{array} $$
which might be something worth noting.

So far we've only demonstrated the \emph{horizontal} notation for chain composition,
but we will also want a \emph{vertical} notation in the long run:
$$ \def\arraystretch{1.2}
\begin{array}{l}
\bfmbox{chain call}\ \ x			\\
function_0					\\
function_1					\\
function_2					\\

\hspace{2em}\vdots				\\

function_n					\\
\bfmbox{done}
\end{array} $$
Unlike horizontal chain composition there's no need for \emph{read} directions---the only direction here being
\emph{top-down}.\footnote{An exception comes when we take this notation to its logical extreme and fill up an
entire page. If we continue such a composition by making further columns on the same page, we will then need
to specify a horizontal read direction once again.} There is however a variant which composes in reverse:
$$ \def\arraystretch{1.2}
\begin{array}{l}
\bfmbox{chain pass}\ \ \id			\\
function_n					\\
function_{n-1}					\\
function_{n-2}					\\

\hspace{2em}\vdots				\\

function_0					\\
\bfmbox{done}
\end{array} $$
Either way, this style can be considered more user-friendly to read when the composite functions
themselves take up a lot more horizontal spacing (which in practice happens often enough).

\subsubsection*{Alternation}

The meaning of alternation is that of \emph{alternatives}: It parallels the logical idea of the \strong{or} operator
($ \vee $), as well as the type theoretic \emph{coproduct}. In the theory of automata it means that any regular
expression possessing the term $ (a\,|\,b) $ has an \emph{alternation site} which will be recognized. For example
if we're searching a text for the string ``Fourier'' and we want to include versions with both lower and uppercase
lettering for the initial character we could specify:
$$ (\,\mbox{F}\,|\,\mbox{f}\,)\mbox{ourier} $$
which when translated into a regular automata would now recognize both possibilities:
$$ \{\ \mbox{Fourier}\ ,\ \mbox{fourier}\ \} $$

In the theory being offered here, when interpreting such strings as function compositions this regular expression
$ (\,\mbox{F}\,|\,\mbox{f}\,)\mbox{ourier} $ would then be read as two independent chain constructions. As such,
using this style of grammar alone we can already specify two alternative functions within a chain composition.

In practice though, this style of expression is not directly as useful as one would hope, especially as the set of possible
strings doubles in size with every additional alternation site. In fact it's more than likely in our applications we would
want to resolve such a specification down to a single chain within the given alternatives.

Now in terms of retooling alternation for function construction, we rely on the previously introduced \texttt{distem}
operator. If we go back to \texttt{distem}'s \texttt{cpose} operators and their definitions we are only really missing
a single component, namely the boolean value $ true? $. As such, we equip our alternation grammar with this variable:
$$ \underset{true?}{(\,\mbox{F}\,|\,\mbox{f}\,)}\mbox{ourier}\readleft $$
In this context though I feel it's more appropriate to relabel this boolean value as $ policy?\, $:
$$ \underset{policy?}{(\,\mbox{F}\,|\,\mbox{f}\,)}\mbox{ourier}\readleft $$
which gives a stronger connotation of \emph{choice making} when it comes to our construction.

This notation is incomplete though. It currently corresponds to \texttt{distem}'s \ \texttt{open cpose}, but we're lacking
the appropriate modifiers. As example let's take \texttt{call call}, which we can then specify as:
$$ \underset{\scriptsize \begin{array}{l}\mbox{call call}\\policy?\end{array}}{(\,\mbox{F}\,|\,\mbox{f}\,)}
 \tab[-0.825ex] \mbox{ourier}\readleft \qquad \mbox{or} \qquad \underset{policy?}
 {\stackrel{\scriptsize \mbox{call call}}{(\,\mbox{F}\,|\,\mbox{f}\,)}}\tab[-0.565ex]\mbox{ourier}\readleft $$
At this point I find the notation lacking in general elegance. In particular I favor use of the specialized composition operators:
$$ \underset{policy?}{(\,\mbox{F}_\circ\,|\,\mbox{f}_\circ\,)}\mbox{ourier}\readleft $$
which works if you recall such specializations as being:

$$ \def\arraystretch{1.5}
\begin{array}{rclcll}
\precompose{f}(g)	& = & f^\circ(g) & := & g \circ f \col[2cm] \mbox{(``$ f $ is \emph{passed} to $ g $'')}		\\
\postcompose{f}(g)	& = & f_\circ(g) & := & f \circ g \col[2cm] \mbox{(``$ f $ is \emph{called} for $ g $'')}
\end{array} $$
\\[0.1cm]

In this case the alternatives are as follows:
$$ \def\arraystretch{1.5}
\begin{array}{llll}
\bfmbox{open call call:} & \underset{policy?}{(\,\mbox{F}_\circ\,|\,\mbox{f}_\circ\,)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{open pass call:} & \underset{policy?}{(\,\mbox{F}^\circ\,|\,\mbox{f}_\circ\,)}\mbox{ourier}\readleft		\\

\bfmbox{open call pass:} & \underset{policy?}{(\,\mbox{F}_\circ\,|\,\mbox{f}^\circ\,)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{open pass pass:} & \underset{policy?}{(\,\mbox{F}^\circ\,|\,\mbox{f}^\circ\,)}\mbox{ourier}\readleft		\\

\bfmbox{open call pose:} & \underset{policy?}{(\,\mbox{F}_\circ\,|\,\mbox{f}\ x)}\mbox{ourier}\readleft			\col[2cm]
\bfmbox{open pass pose:} & \underset{policy?}{(\,\mbox{F}^\circ\,|\,\mbox{f}\ x)}\mbox{ourier}\readleft			\\[0.7cm]

&\col[2cm] \bfmbox{open pose call:} & \underset{policy?}{(\,\mbox{F}\,x\,|\,\mbox{f}_\circ\,)}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{open pose pass:} & \underset{policy?}{(\,\mbox{F}\,x\,|\,\mbox{f}^\circ\,)}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{open pose pose:} & \underset{policy?}{(\,\mbox{F}\,x_1\,|\,\mbox{f}\ x_2)}\mbox{ourier}\readleft	\\
\end{array} $$
\\[0.1cm]
On the other hand, if we want to reduce the number of \emph{potential} parentheses when using this notation within
our applications we can attach the composition circles $ \circ $ to the alternation bar symbols $ | $ directly:

$$ \def\arraystretch{1.5}
\begin{array}{llll}
\bfmbox{open call call:} & \underset{policy?}{(\,\mbox{F}\,_\circ|_\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{open pass call:} & \underset{policy?}{(\,\mbox{F}\,^\circ|_\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\\

\bfmbox{open call pass:} & \underset{policy?}{(\,\mbox{F}\,_\circ|^\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{open pass pass:} & \underset{policy?}{(\,\mbox{F}\,^\circ|^\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\\

\bfmbox{open call pose:} & \underset{policy?}{(\,\mbox{F}\,_\circ|\,\mbox{f}\ x)}\mbox{ourier}\readleft			\col[2cm]
\bfmbox{open pass pose:} & \underset{policy?}{(\,\mbox{F}\,^\circ|\,\mbox{f}\ x)}\mbox{ourier}\readleft			\\[0.7cm]

&\col[2cm] \bfmbox{open pose call:} & \underset{policy?}{(\,\mbox{F}\,x\,|_\circ\,\mbox{f}\,)}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{open pose pass:} & \underset{policy?}{(\,\mbox{F}\,x\,|^\circ\,\mbox{f}\,)}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{open pose pose:} & \underset{policy?}{(\,\mbox{F}\,x_1\,|\,\mbox{f}\ x_2)}\mbox{ourier}\readleft	\\
\end{array} $$
\\[0.1cm]
This actually ties back to the nomenclature introduced with the \texttt{stem} and \texttt{costem cpose}s, and although
technically it moves us slightly outside of standard automata theory, \texttt{stem}'s correspondences are as follows:

$$ \def\arraystretch{1.5}
\begin{array}{llll}
\bfmbox{closing call call:} & \underset{policy?}{[\,\mbox{F}\,_\circ|_\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{closing pass call:} & \underset{policy?}{[\,\mbox{F}\,^\circ|_\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\\
                                                                        
\bfmbox{closing call pass:} & \underset{policy?}{[\,\mbox{F}\,_\circ|^\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{closing pass pass:} & \underset{policy?}{[\,\mbox{F}\,^\circ|^\circ\,\mbox{f}\,)}\mbox{ourier}\readleft		\\
                                                           
\bfmbox{closing call pose:} & \underset{policy?}{[\,\mbox{F}\,_\circ|\,\mbox{f}\ x)}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{closing pass pose:} & \underset{policy?}{[\,\mbox{F}\,^\circ|\,\mbox{f}\ x)}\mbox{ourier}\readleft		\\[0.7cm]

&\col[2cm] \bfmbox{closing pose call:} & \underset{policy?}{[\,\mbox{F}\,x\,|_\circ\,\mbox{f}\,)}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{closing pose pass:} & \underset{policy?}{[\,\mbox{F}\,x\,|^\circ\,\mbox{f}\,)}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{closing pose pose:} & \underset{policy?}{[\,\mbox{F}\,x_1\,|\,\mbox{f}\ x_2)}\mbox{ourier}\readleft	\\
\end{array} $$
with \texttt{costem}'s correspondences being symmetric:

$$ \def\arraystretch{1.5}
\begin{array}{llll}
\bfmbox{opening call call:} & \underset{policy?}{(\,\mbox{F}\,_\circ|_\circ\,\mbox{f}\,]}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{opening pass call:} & \underset{policy?}{(\,\mbox{F}\,^\circ|_\circ\,\mbox{f}\,]}\mbox{ourier}\readleft		\\
                                                                        
\bfmbox{opening call pass:} & \underset{policy?}{(\,\mbox{F}\,_\circ|^\circ\,\mbox{f}\,]}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{opening pass pass:} & \underset{policy?}{(\,\mbox{F}\,^\circ|^\circ\,\mbox{f}\,]}\mbox{ourier}\readleft		\\
                                                           
\bfmbox{opening call pose:} & \underset{policy?}{(\,\mbox{F}\,_\circ|\,\mbox{f}\ x]}\mbox{ourier}\readleft		\col[2cm]
\bfmbox{opening pass pose:} & \underset{policy?}{(\,\mbox{F}\,^\circ|\,\mbox{f}\ x]}\mbox{ourier}\readleft		\\
															\\
&\col[2cm] \bfmbox{opening pose call:} & \underset{policy?}{(\,\mbox{F}\,x\,|_\circ\,\mbox{f}\,]}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{opening pose pass:} & \underset{policy?}{(\,\mbox{F}\,x\,|^\circ\,\mbox{f}\,]}\mbox{ourier}\readleft	\\
&\col[2cm] \bfmbox{opening pose pose:} & \underset{policy?}{(\,\mbox{F}\,x_1\,|\,\mbox{f}\ x_2]}\mbox{ourier}\readleft	\\
\end{array} $$

These are the horizontal notations for our alternation operators. There are also vertical notations analogous to the ones
for the above catenation operators. In particular we reuse the grammatical forms introduced in the conditional composition
section, for example the following horizontal expression:
$$ x \readright
  \underset{\tab[-6ex] policy?_0}{[\,break_0\ _\circ|_\circ\,f_0)}
\ \underset{\tab[-6ex] policy?_1}{[\,break_1\ _\circ|_\circ\,f_1)}
\ \underset{\tab[-6ex] policy?_2}{[\,break_2\ _\circ|_\circ\,f_2)}
\ \ldots
\ \underset{\tab[-6ex] policy?_n}{[\,break_n\,_\circ|_\circ\,f_n)}
$$
translates into the following vertical form:

$$ \def\arraystretch{1.2}
\begin{array}{llll}
\bfmbox{closing}	\col[5ex] \bfmbox{call}		\col[5ex] \bfmbox{call}	\col[5ex] x	\\
policy?_0		\col[5ex] break_0		\col[5ex] f_0				\\
policy?_1		\col[5ex] break_1		\col[5ex] f_1				\\
policy?_2		\col[5ex] break_2		\col[5ex] f_2				\\

\tab[3.5ex] \vdots	\col[7.5ex] \vdots		\col[5.5ex] \vdots			\\

policy?_n		\col[5ex] break_n		\col[5ex] f_n				\\
\bfmbox{closed}
\end{array} $$

\subsubsection*{Repetition}

In regular expression theory the repetition operator is known as the \emph{Kleene star} operator, which as a recognizer
allows us to match against repetitive strings:
$$ a^* \qdefeq \{\,\epsilon,\ a,\ aa,\ aaa,\ aaaa,\ \ldots \,\} $$
where $ \epsilon $ is the empty string. Use of this \emph{repeat} operator can be surprisingly effective,
for example we need only it and alternation to generate all finite length binary strings:
$$ (\,0\,|\,1\,)^* \qdefeq \{\,\epsilon,\ 0,\ 1,\ 00,\ 01,\ 10,\ 11,\ 000,\ 001,\ 010,\ \ldots \,\} $$

As powerful an operation as this is, how can we convert it to be applicable to function construction?
For example how do we interpret the repetition of a single function $ f $ ?

This operator works by means of composition just like the other regular expressions:
$$ \begin{array}{rcl}
f^*	& \sim		& \id				\\
	& \mbox{or}	& f				\\
	& \mbox{or}	& f\circ f			\\
	& \mbox{or}	& f\circ f\circ f		\\
	& \mbox{or}	& f\circ f\circ f\circ f	\\
	& \vdots	\col[5.5ex] \vdots
\end{array} $$
but in this case the empty string $ \epsilon $ is interpeted as the identity function \texttt{id}. Otherwise, as with
alternation this notation has limited expressivity until we can narrow things down to a specific instance (rather than
the whole of the language it represents). To do this, we distinguish individual strings by their lengths $ \ell $:
$$ (F)^\ell\tab[0ex]\mbox{ourier}\readleft $$
which upon first glance might appear anti-climactic, but once we combine repetition
with alternation we can actually achieve a reasonable level of expressivity, for example:
$$ \underset{\hspace{-1ex} p}{(f\,_\circ|_\circ\,g)^5} $$
which would expand to
$$ \underset{p_0}{(f\,_\circ|_\circ\,g)}
   \underset{p_1}{(f\,_\circ|_\circ\,g)}
   \underset{p_2}{(f\,_\circ|_\circ\,g)}
   \underset{p_3}{(f\,_\circ|_\circ\,g)}
   \underset{p_4}{(f\,_\circ|_\circ\,g)} $$
In fact we can even take things further, but for the more interesting versions of
repetition we will need to solve the signature problem (which we'll get to soon enough).

The vertical notation for this repetition operator is an extension of the previous operator's vertical notations.
In this case we \emph{prepend} the keyword \strong{repeat} followed by a number $ m $ which tells us how many times
we want to repetitively compose the initial operator, for example:
$$ \def\arraystretch{1.2}
\begin{array}{lll}
\reppose{m}{x}											\\
\bfmbox{closing}	\col[5ex] \bfmbox{call}		\col[5ex] \bfmbox{call}			\\
policy?_0		\col[5ex] break_0		\col[5ex] f_0				\\
policy?_1		\col[5ex] break_1		\col[5ex] f_1				\\
policy?_2		\col[5ex] break_2		\col[5ex] f_2				\\

\tab[3.5ex] \vdots	\col[7.5ex] \vdots		\col[5.5ex] \vdots			\\

policy?_n		\col[5ex] break_n		\col[5ex] f_n				\\
\bfmbox{closed}
\end{array} $$
The proper interpretation here is to use this induction grammar to first construct the function for which
we're passing the value $ x $. More specifically, this means we start with $ n+1 $ distinct $ policy? $ values:
$$ policy?_0,\ policy?_1,\ policy?_2,\ \ldots\ policy?_n $$
which expand to $ m(n+1) $ such $ policy? $ values, and then we evaluate. This doesn't make a difference when
such values are constant, but becomes especially meaningful when they are replaced with functions of their own.
Also, in addition to a finite valued $ m $, we can even specify it to be infinity $ \infty $, but in this case
we would expand it under an \emph{only-as-needed} lazy style, for which we would also require guarantees in
advance that the algorithm would eventually halt.

\subsubsection*{The Regular Pumping Lemma}

So far we've been discussing the practical considerations of finite automata and their corresponding grammars.
We haven't yet discussed their theoretical limitations. To that end, by going back to the original automata theory
we are provided with \strong{the pumping lemma for regular languages}, which states:

\begin{center}
\begin{minipage}{13cm}
Let $ \mathcal{L} $ be a regular language with strings $ w,x,y,z \in \mathcal{L} $ such that $ wxy = z $
and  $ x \neq \epsilon $. There exists a natural number $ n \in \mathbb{N} $ (dependent only on $ \mathcal{L} $)
such that if $ \length(z) \ge n $, and $ \length(wx) \le n $, then for all $ k\in\mathbb{N} $ we have:
$$ wx^ky \in \mathcal{L} $$
\end{minipage}
\end{center}

Without going heavily into the details of regular languages, the pumping lemma is a consequence of the finite memory
system of their corresponding automata. In particular, if such a finite state machine has $ n $ states (size $ n $ memory)
and the string we're testing against has length greater than $ n $, then by the pigeonhole counting principle
two or more characters in the string must be recognized by the same state.

The big realization here is that in the case such an automata needs to be able to distinguish the locations of two
characters as part of its recognition algorithm, such information actually gets lost through these pigeonholes: It's
like adding $ 3+4 $, if our memory system only kept the end result $ 7 $ it wouldn't be able to recover the addends
after the fact if they were later needed, for example we could guess $ 2+5 $ instead with no way of knowing. Hence,
there are many patterns of strings that regular automata simply don't have the capacity to recognize.

This lemma not only gives us a criterion to test if a particular chain composition of functions is a regular function,
it also demonstrates the need to move beyond \strong{regular induction}. With that said, we're still not quite ready
to introduce the context-free variety: Before we do, we will need a few additional prerequisites which unfortunately
could not have been introduced in the previous sections. As such, it's now time to revisit and finally solve
the signature problem.

\subsubsection*{The Signature Problem}

I had previously stated that the signature problem arises from the desire to \emph{create variations
within the signatures of functions}. More specifically it arises from wanting to create variations within
\strong{subsignatures} of functions. For example we had previously defined the \texttt{stem} operator
which I now re-present in its LISP form:

$$ \def\arraystretch{1.15}
\tab[-6cm] \begin{array}{l}
(\define\ (\stem\ policy?\ d\ e\ f\ g\ h)					\\
\tab[1.15cm] (\dihold\ policy?\ d\ e\ (\transit\ f)\ (\delay\ g\ h))		\\
)
\end{array} $$

Notice here the $ policy? $ variable is a boolean value which is either \emph{true} or \emph{false} ?
What if instead we wanted to turn it into a boolean function accepting the input $ h $ from the main signature:

$$ \def\arraystretch{1.15}
\tab[-6cm] \begin{array}{l}
(\define\ (\stem\ policy?\ d\ e\ f\ g\ h)						\\
\tab[1.15cm] (\dihold\ (policy?\ h)\ d\ e\ (\transit\ f)\ (\delay\ g\ h))		\\
)
\end{array} $$
Can you spot the difference in these two definitions?

We now have a small variation which semantically creates an entirely different operator than our original \texttt{stem}.
In such cases do we then have to recode from scratch every time we make a small change to its subsignatures? For example
we might alternatively have a $ policy? $ function that accepts the following signature as its input:
$$ (policy?\ e\ g) $$
All in all there are $ 2^5 = 32 $ possible variations based on whether a given variable belongs to our desired signature
or not. Frankly that's a lot of \emph{nearly identical} functions to have to hand code!

As for solving this problem there are a few common approaches: First, we could encapsulate the main function (or operator)
within another superficial function and set the variable values accordingly:
$$ \def\arraystretch{1.15}
\tab[-8.5cm] \begin{array}{l}
(\define\ (\mbox{superficial}\ policy?\ d\ e\ f\ g\ h)			\\
\tab[1.15cm] (\stem\ (policy?\ h)\ d\ e\ f\ g\ h)			\\
)
\end{array} $$
but this still doesn't help us when it comes to the other $ 31 $ variations. The second option is we could encapsulate
$ policy? $ within its own superficial function when implementing the main function, and then disregard argument variables
as needed:
$$ \def\arraystretch{1.15}
\tab[-3cm] \begin{array}{l}
(\define\ (\stem\ policy?\ d\ e\ f\ g\ h)										\\
\tab[1.15cm] (\dihold\ (\mbox{superficial}'\ policy?\ d\ e\ f\ g\ h)\ d\ e\ (\transit\ f)\ (\delay\ g\ h))		\\
)
\end{array} $$
where for example:
$$ \def\arraystretch{1.15}
\tab[-8.5cm] \begin{array}{l}
(\define\ (\mbox{superficial}'\ policy?\ d\ e\ f\ g\ h)			\\
\tab[1.15cm] (policy?\ h)						\\
)
\end{array} $$
but this runs into the same problem in terms of the remaining variations.\footnote{Neither approach is the worst compromise,
but if we survey how memory systems tend to be implemented we find extra work would be done in storing the unused variable
value \emph{bindings}. It's possible that in a given practical context we might have a nice compiler that will optimize
such details out, but a user also shouldn't be forced to rely on that assumption---for example there are
\emph{mission critical} contexts where every computation counts and which might necessitate more refined control.}

As such, how do we go about solving our signature problem? Sticking with the example signature:
$$ (d,\, e,\, f,\, g,\, h) $$
which we now view as a tuple, we want in this case a grammatical operator that accepts policies determining the variables
to \emph{keep} and then builds the tuple accordingly. With that said, I would like to take this opportunity to practice
the primary and secondary modelling approaches to operator design discussed in the methodology section. In that spirit,
our first attempt at implementing this grammatical operator is to take the secondary modelling approach:

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00  = (0,0);

pair p10  = segment(p00, "S", 1.125);

pair d00 = segment(p10, "S", 0.75);
pair d10 = segment(d00, "SL", 0.375, 70);
pair d11 = segment(d00, "SR", 0.375, 70);

pair e00 = segment(d11, "S", 0.75);
pair e10 = segment(e00, "SL", 0.375, 70);
pair e11 = segment(e00, "SR", 0.375, 70);

pair f00 = segment(e11, "S", 0.75);
pair f10 = segment(f00, "SL", 0.375, 70);
pair f11 = segment(f00, "SR", 0.375, 70);

pair g00 = segment(f11, "S", 0.75);
pair g10 = segment(g00, "SL", 0.375, 70);
pair g11 = segment(g00, "SR", 0.375, 70);

pair h00 = segment(g11, "S", 0.75);
pair h10 = segment(h00, "SL", 0.375, 70);
pair h11 = segment(h00, "SR", 0.375, 70);

//

pen stepcolor = gray + fontsize(8pt);
pen altcolor = gray + fontsize(10pt);
pen fillcolor = lightgray;
pen bordercolor = heavygray;
pen textcolor = fontsize(10pt);

draw(p10--d00, bordercolor);
draw(d00--d10, bordercolor);
draw(d00--d11, bordercolor);

draw(d11--e00, bordercolor);
draw(e00--e10, bordercolor);
draw(e00--e11, bordercolor);

draw(e11--f00, bordercolor);
draw(f00--f10, bordercolor);
draw(f00--f11, bordercolor);

draw(f11--g00, bordercolor);
draw(g00--g10, bordercolor);
draw(g00--g11, bordercolor);

draw(g11--h00, bordercolor);
draw(h00--h10, bordercolor);
draw(h00--h11, bordercolor);

//

label("signature$\,(\,keep_d?,\ keep_e?,\ keep_f?,\ keep_g?,\ keep_h?\,)$:", shift(1,0)*p00, fontsize(10pt));

label("$ = \{\,keep_d?\,\}$", shift(1.35, 0)*p10, altcolor);
label("$ = \{\,keep_e?\,\}$", shift(1.35, 0)*d11, altcolor);
label("$ = \{\,keep_f?\,\}$", shift(1.35, 0)*e11, altcolor);
label("$ = \{\,keep_g?\,\}$", shift(1.35, 0)*f11, altcolor);
label("$ = \{\,keep_h?\,\}$", shift(1.35, 0)*g11, altcolor);

real s = 0.33;

label("$1$", shift(0.12, 0.41)*d00, stepcolor);
label("$1$", shift(s, -0.05)*d10, stepcolor);
label("$2$", shift(-s, -0.05)*d11, stepcolor);

label("$1$", shift(0.12, 0.41)*e00, stepcolor);
label("$1$", shift(s, -0.05)*e10, stepcolor);
label("$2$", shift(-s, -0.05)*e11, stepcolor);

label("$1$", shift(0.12, 0.41)*f00, stepcolor);
label("$1$", shift(s, -0.05)*f10, stepcolor);
label("$2$", shift(-s, -0.05)*f11, stepcolor);

label("$1$", shift(0.12, 0.41)*g00, stepcolor);
label("$1$", shift(s, -0.05)*g10, stepcolor);
label("$2$", shift(-s, -0.05)*g11, stepcolor);

label("$1$", shift(0.12, 0.41)*h00, stepcolor);
label("$1$", shift(s, -0.05)*h10, stepcolor);
label("$2$", shift(-0.55, -0.05)*h11, stepcolor);

real r0 = 0.20;
real r1 = 0.22;

safeLabel(" ", p10, r0, r0, textpen = textcolor, borderpen = bordercolor, fillpen = fillcolor);

safeLabel("cons", d00, 0.42, 0.22, textpen = textcolor, borderpen = bordercolor);
safeLabel("$d$", d10, r1, r1, textpen = textcolor, borderpen = bordercolor);
safeLabel(" ", d11, r0, r0, textpen = textcolor, borderpen = bordercolor, fillpen = fillcolor);

safeLabel("cons", e00, 0.42, 0.22, textpen = textcolor, borderpen = bordercolor);
safeLabel("$e$", e10, r1, r1, textpen = textcolor, borderpen = bordercolor);
safeLabel(" ", e11, r0, r0, textpen = textcolor, borderpen = bordercolor, fillpen = fillcolor);

safeLabel("cons", f00, 0.42, 0.22, textpen = textcolor, borderpen = bordercolor);
safeLabel("$f$", f10, r1, r1+0.01, textpen = textcolor, borderpen = bordercolor);
safeLabel(" ", f11, r0, r0, textpen = textcolor, borderpen = bordercolor, fillpen = fillcolor);

safeLabel("cons", g00, 0.42, 0.22, textpen = textcolor, borderpen = bordercolor);
safeLabel("$g$", g10, r1, r1, textpen = textcolor, borderpen = bordercolor);
safeLabel(" ", g11, r0, r0, textpen = textcolor, borderpen = bordercolor, fillpen = fillcolor);

safeLabel("cons", h00, 0.42, 0.22, textpen = textcolor, borderpen = bordercolor);
safeLabel("$h$", h10, r1, r1, textpen = textcolor, borderpen = bordercolor);
safeLabel("null", h11, 0.42, 0.22, textpen = textcolor, borderpen = bordercolor);

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
Here each $ keep? $ variable is meant to be set to either \texttt{id} or \texttt{cdr}, where \texttt{id}
corresponds to \emph{keeping} the variable and \texttt{cdr} corresponds to \emph{dropping} it.

This approach is considered a secondary model because there are plenty of functions we could have substituted other than
\texttt{id} or \texttt{cdr} having nothing to do with building tuples, but they'd be valid nonetheless. In this case such
a pattern represents an entire class of functions. Regardless, you'll notice the inefficiencies in this style of modelling
as there are a lot of unnecessary calculations within any actual tuple building function we create.

As I had mentioned in the methodology section this approach is a good place to start as it still tells us the bare minimum
of components that a more refined primary model would likely require. To that end let's now turn to such a primary:

$$ \def\arraystretch{1.2}
\tab[-3cm] \begin{array}{l}
\induct\ \langle induct\_name\rangle \tab[1ex] func\_name
\tab[1ex] keep_d? \tab[1ex] keep_e? \tab[1ex] keep_f? \tab[1ex] keep_g? \tab[1ex] keep_h?:		\\[1ex]

\tab \begin{array}{lll}
\bfmbox{define}	\tab[1ex] func\_name									\\[1ex]

\id													\\

\bfmbox{open}	\col[-8ex] \bfmbox{pass}	\col[5ex] \bfmbox{id}					\\
keep_d?		\col[-8ex] \cons\ d									\\
keep_e?		\col[-8ex] \cons\ e									\\
keep_f?		\col[-8ex] \cons\ f									\\
keep_g?		\col[-8ex] \cons\ g									\\
keep_h?		\col[-8ex] \cons\ h									\\[1ex]

\bfmbox{chain pass}											\\
\mbox{null}												\\[1ex]
\bfmbox{halt}												\\
\end{array}												\\
\end{array} $$
There's a few new things to note here, as it's our first real source code using this style of grammar.
For starters, we have a two new keywords: \strong{induct} and \strong{define}.

The \texttt{induct} operator declares this source code to be a function defining operator, yet as this is a generic
keyword we still need to specify $ \langle induct\_name\rangle $ so we can reference the operator itself later on.
A simple example of such a name might be ``signature\_\,maker''. Following this, the \texttt{define} keyword reaffirms
that we're defining a function, which itself needs to be followed by its $ func\_name $ and any argument variables
it accepts (in this case it accepts none).

Otherwise, the interpretation of the induction operator's body is that we set each $ keep? $ policy to its respective
boolean value, and then run the \deftpose{open pass id} \texttt{cpose} operator with \texttt{id} as the initial input.
As for the column title \strong{id}, I hadn't previously declared its meaning when I introduced the \texttt{distem cpose}s.
Basically it just means each function in the column defaults to \texttt{id} so we don't need to specify it each time,
and given that it's \texttt{id} each time it doesn't make a difference whether it's \texttt{call} or \texttt{pass}.

This model actually points out an interesting change in perspective: We seek to build tuples, but here we're not trying
to build these tuples directly, we're trying to build the functions that build these tuples. Generally speaking,
this is the underlying paradigm used to solve the signature and other function building problems. As a consequence
of this strategy, we can now define an induction operator for our $ 32 $ flavors of \texttt{stem}:

$$ \def\arraystretch{1.2}
\tab[-3cm]\begin{array}{l}
\induct\ \mbox{stem\_\,maker} \tab[1ex] stem\_\,name
\tab[1ex] keep_d? \tab[1ex] keep_e? \tab[1ex] keep_f? \tab[1ex] keep_g? \tab[1ex] keep_h?:		\\[1ex]

\tab \begin{array}{lll}
\bfmbox{define}	\tab[1ex] stem\_name\ policy?\ d\ e\ f\ g\ h						\\[1ex]

\bfmbox{chain pass} \tab[1ex] \dihold									\\[1ex]

policy?													\\

\bfmbox{open}	\col[-3.75cm] \bfmbox{pass}	\col[5ex] \bfmbox{id}					\\
keep_d?		\col[-3.75cm] \cons\ d									\\
keep_e?		\col[-3.75cm] \cons\ e									\\
keep_f?		\col[-3.75cm] \cons\ f									\\
keep_g?		\col[-3.75cm] \cons\ g									\\
keep_h?		\col[-3.75cm] \cons\ h									\\[1ex]

\bfmbox{chain pass}											\\
\mbox{null}												\\
d													\\
e													\\
\transit\ f												\\
\delay\ g\ h												\\[1ex]

\bfmbox{halt}												\\
\end{array}
\end{array} $$
Notice in this version when defining the $ policy? $ function's signature we didn't start the \texttt{open}
chain with \texttt{id} as its input like we had in the standalone version? The reason for this is we had
more specific content available this time. I've mentioned this before, but by convention an \texttt{id}
in these locations is just a placeholder signifying the end of a continuation passing line.

Also, note that we've now used the original \texttt{distem} operator and its \texttt{cpose}s to reimplement
\texttt{stem}? What happens if we instead use \texttt{distem} it to reimplement itself? Such an implementation
is said to be \strong{meta-circular}, this terminology being borrowed from \cite{sicp}. Such meta-circular
operators tell us the bare minimum tools needed to rebuild themselves, which is an important theoretical
boundary of our narrative design.\footnote{Not only that, but if we were to take an inventory of all the grammar
used within a meta-circular construction, we would know the bare minimum tools needed from another language
if we wanted to implement these tools from scratch. This has practical implications in library design,
not to mention theoretical implications when it comes to consistency semantics.}

\subsubsection*{Lattice Optimizations}

There's actually one loose end within the previous example source code: The use of the \deftpose{open pass id} cpose.

I had made a big fuss that primary modelling is better than secondary for actual implementation, pointing out that
our particular secondary model had inefficiencies because it made unnecessary computations such as identity compositions.
We then went on to accept the example primary model as better even though it does the same thing: The \texttt{id} modifier
within \deftpose{open pass id} defaults all functions passed to its column to be composed with the \texttt{id} function.

The reason this primary approach is still an improvement over the secondary is because we can systematically
optimize these identity functions out of the final construction. This is due to the \emph{operator lattice}
that was previously used to introduce the core conditional composition operators:

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
\begin{asy}
unitsize(1cm);

//

pair p00 = (0,0);

pair p10 = segment(p00, "SL", 1, 60);
pair p11 = segment(p00, "SR", 1, 60);

pair p20 = segment(p10, "SL", 1, 60);
pair p21 = segment(p11, "SL", 1, 60);
pair p22 = segment(p11, "SR", 1, 60);

pair p30 = segment(p21, "SL", 1, 60);
pair p31 = segment(p21, "SR", 1, 60);

pair p40 = segment(p31, "SL", 1, 60);

//

pen linecolor = gray;

draw(p00--p10, linecolor, MidArrow(size = 4));
draw(p00--p11, linecolor, MidArrow(size = 4));

draw(p10--p20, linecolor, MidArrow(size = 4));
draw(p10--p21, linecolor, MidArrow(size = 4));
draw(p11--p21, linecolor, MidArrow(size = 4));
draw(p11--p22, linecolor, MidArrow(size = 4));

draw(p20--p30, linecolor, MidArrow(size = 4));
draw(p21--p30, linecolor, MidArrow(size = 4));
draw(p21--p31, linecolor, MidArrow(size = 4));
draw(p22--p31, linecolor, MidArrow(size = 4));

draw(p30--p40, linecolor, MidArrow(size = 4));
draw(p31--p40, linecolor, MidArrow(size = 4));

//

pen bordercolor = white;

safeLabel("distem$_{\scriptscriptstyle\,(3,3)}$", shift(0.3,0)*p00, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("stem$_{\scriptscriptstyle\,(2,3)}$", shift(0.25,0)*p10, 0.85, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("costem$_{\scriptscriptstyle\,(3,2)}$", shift(0.2,0)*p11, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("pend$_{\scriptscriptstyle\,(1,3)}$", shift(0.4,0)*p20, 0.85, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("dihold$_{\scriptscriptstyle\,(2,2)}$", shift(0.3,0)*p21, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("copend$_{\scriptscriptstyle\,(3,1)}$", p22, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("hold$_{\scriptscriptstyle\,(1,2)}$", shift(0.25,0)*p30, 0.85, 0.30, borderpen = bordercolor, bordertype = "box");
safeLabel("cohold$_{\scriptscriptstyle\,(2,1)}$", shift(0.2,0)*p31, 1.00, 0.30, borderpen = bordercolor, bordertype = "box");

safeLabel("if$_{\scriptscriptstyle\,(1,1)}$", p40, 0.60, 0.30, borderpen = bordercolor, bordertype = "box");

\end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
Here I have presented it in its dual form to which we will now use in the following derivations.

The general approach to these optimizations is to first take our given \texttt{cpose} \deftpose{open pass id},
and translate it into its \texttt{distem} form:\footnote{We chose the second modifier to also be a \texttt{pass},
but it could just as easily have been \texttt{call} without any loss of generality.}
$$ \distem(\,true?,\ cont,\ -_{arg},\ next,\ cont,\ -_{arg},\ \id\,) $$
The second step is to ignore the $ true? $ value at the beginning, and then strike out the \texttt{id}
coordinate from the signature as it is not actually needed in \texttt{distem}'s computations:
$$ \def\arraystretch{1}
\begin{array}{lrl}
(\,true?,\ cont,\ -_{arg},\ next,\ cont,\ -_{arg},\ \id\,)
\col[2ex] \lra \col[2ex] (\,cont,\ -_{arg},\ next,\ cont,\ -_{arg},\ \id\,)		\\
\col[2ex] \lra \col[2ex] (\,cont,\ -_{arg},\ next,\ cont,\ -_{arg}\,)
\end{array} $$
We make note of the fact that \texttt{id} was located at the \emph{sixth} coordinate of the tuple (when we ignore the boolean
value $ true? $ ; or if we're indexing by zero). We now reference the above lattice to retrieve \texttt{distem}'s subscript:
$$ (3,3) $$
The interpretation here is that this tuple represents the six arguments of \texttt{distem}'s signature (again ignoring the
initial boolean value), and that this noted sixth argument belongs to the $ 3 $ arguments on the \emph{right side} of the
pair. Now, since we have removed the \texttt{id} argument from the tuple we should also update this information in our pair:
$$ (3,2) $$
We use this derived index to reference the lattice again and finally arrive at the operator we will be using
in our optimization: \texttt{costem}. We finish by combining this derived operator back with our derived
signature (including the initial boolean value) to obtain \deftpose{open pass id}'s optimized implementation:
$$ \costem(\,true?,\ cont,\ -_{arg},\ next,\ cont,\ -_{arg}\,) $$

This general approach applies not only to \texttt{distem} and its cposes, it applies to any of the operators in the above
lattice for which there is a meaningful downward arrow. Keep in mind though this only applies to the respective known
\texttt{cpose}s when the derived operator also has its own cposes, otherwise the continuation passing process breaks down.

In anycase, this approach is so routine it could be standardized as its own induction operator---that is if we
decided to use it in a \emph{meta-circular} way to reimplement the \texttt{cpose}s of the various \texttt{stem}s.
Either way, I have supplied a subset of these optimizations for reference in the appendix.

\subsubsection*{Refined Regular Induction}

We left off the signature problem subsubsection nearly rederiving \texttt{stem} in a metacircular way. Given these optimizations,
if we had at the time derived a ``\texttt{costem\,\_\,maker}'' instead, it actually would have been meta-circular.

Yet, these induction operators can do even more for us as they allow for alternative \texttt{stem} style operators
where the $ policy? $ boolean value (as part of the signature) instead becomes a \emph{boolean valued function}.
With such induction operators we could even extend the various \texttt{cpose}s, where their extensions also
allow for the $ policy? $ variables to be boolean valued functions.

If we take this to its logical extreme, we can even extend \emph{all} of our \strong{regular induction} operators this
way---not just our alternations. Keep in mind though, at this point such operators are potentially now outside the range
of the theory of \emph{regular languages}, and might require their own proofs as to things like the pumping lemma.

As for giving this \emph{multiplicity} of new operators their own nomenclatures? I find overall that it is unnecessary.
In practice the only extension I have (as of yet) ever used is the one where $ policy? $ accepts the $ -_{arg} $ variable,
which is to say the one passed from the previous step in the chain. As such, we can reuse the same names and notations
and simply equip $ policy? $ with appropriate uses of the variable placeholder $ - $ as needed, with no ambiguity ensuing.

To be safe though, in the cases where we want to be more overt about which signature variables the $ policy? $ function
accepts, we might for example equip our notation as follows:
$$ \bfmbox{open}{\scriptstyle \{1,4\}} \quad \bfmbox{call} \quad \bfmbox{pass} \quad $$
meaning $ policy? $ accepts the first and fourth variables of its respective \texttt{stem} operator.

\subsubsection*{The Factorial Function}

Let's now put all this theory to good use, and demonstrate an implementation of \emph{the factorial function}.

I'm not going to assume the reader is fully comfortable with these newer notations just yet,
so let's learn how to translate from the classical mathematical definition and its notation:
$$ n! \qdefeq \left\{\begin{array}{ll}
1		& \qquad \mbox{if } n = 0,						\\
n \cdot (n-1)!	& \qquad \mbox{otherwise.}
\end{array}\right. $$
The first step for us is to actually reimplement the factorial function with something slightly more general:
$$ \def\arraystretch{1.1}
(p, n)! \qdefeq \left\{\begin{array}{ll}
\ \ p			& \qquad \mbox{if } n = 0,					\\
(\,p\cdot n,\ n-1\,)!	& \qquad \mbox{otherwise.}
\end{array}\right. $$
In this case we can redefine our first factorial function as:
$$ n! \qdefeq (1,n)! $$
Next (and this step is a bit of a transition), we translate this
\emph{pair factorial} function into the \texttt{stem} operator style:
$$ !\ p\ n \qdefeq \stem(\,\isZero\,n,\ \id,\ p,\ !\ (p \cdot n),\ \dec,\ n\,) $$
Here \texttt{isZero?} tests if $ n $ is zero, and \texttt{dec} just decrements it to become $ n-1 $.
Also, notice that the right side recursive call of this pair factorial is a \emph{curried} version:
$$ !\ (p\cdot n)\ - $$

We now translate this further into its \texttt{cpose} form:
$$ \def\arraystretch{1.2}
\begin{array}{l}
\begin{array}{llll}
\bfmbox{closing}	\col[8.5ex] \bfmbox{call}	\col[4ex] \bfmbox{call}	\col (p, n)				\\
\end{array}														\\
\left.\begin{array}{llll}
\isZero\ \cdr		\col[5ex] \car			\col[5ex] (\cons\ (\,\cdot\ -)\ (\dec\ \cdr\ -))		\\
\isZero\ \cdr		\col[5ex] \car			\col[5ex] (\cons\ (\,\cdot\ -)\ (\dec\ \cdr\ -))		\\
\isZero\ \cdr		\col[5ex] \car			\col[5ex] (\cons\ (\,\cdot\ -)\ (\dec\ \cdr\ -))		\\

\tab[6ex] \vdots	\col[6ex] \vdots		\col[16.5ex] \vdots						\\

\isZero\ \cdr		\col[5ex] \car			\col[5ex] (\cons\ (\,\cdot\ -)\ (\dec\ \cdr\ -))		\\
\end{array} \quad \right\} n+1 \mbox{ times}
\end{array} $$
Notice the traditional $ policy? $ value in the \texttt{closing} column is now a function? Hence, this is an
extended version of our \texttt{closing} cpose where the input from the previous row is passed to the testing
function as well. The ``$ n+1 $ times'' comment on the side is to say that we only need to repeat this chain
so many times before we know it will halt, which is readily deducible from the factorial function itself.
Also, I'm reusing the $ (\cdot) $ function notation to multiply the contents of a pair:
$$ \cdot\,(p,n) = p\cdot n $$
We're not quite done as this notation can be compressed using the \texttt{repeat} operator:
$$ \def\arraystretch{1.2}
\begin{array}{lll}
\reppose{n+1}{\:(p, n)}											\\
\bfmbox{closing}	\col[5ex] \bfmbox{call}	\col[5ex] \bfmbox{call}					\\
\isZero\ \cdr		\col[5ex] \car		\col[5ex] \ldp\,\cdot\ ,\ \dec\ \cdr\,\rdp		\\
\bfmbox{closed}
\end{array} $$
In fact we've simplified further by using \emph{bifunctions}. I haven't introduced this idea yet, and I don't want
to go deeply into it because it takes us outside the scope of this essay, but we can extend our 1-dimensional functions
to 2-dimensional (or higher), possessing identities such as:
$$ \def\arraystretch{1}
\begin{array}{lcl}
f\,\ldp g_1,\ g_2\rdp				& := & \ldp f \circ g_1,\ f \circ g_2\rdp		\\
\ldp g_1,\ g_2\rdp\,f				& := & \ldp g_1 \circ f,\ g_2 \circ f\rdp		\\
\ldp f_1,\ f_2\rdp\ \bullet\ \ldp g_1,\ g_2\rdp	& := & \ldp f_1 \circ g_1,\ f_2 \circ g_2\rdp
\end{array} $$
where we've used the $ \bullet $ operator and the double parentheses $ \ldp,\rdp $ to discern from 1-dimensional functions.

If instead we wanted an especially terse presentation, we can retranslate this into its horizontal format:
$$ \def\arraystretch{1.2}
\begin{array}{l}
(p,n)\ [\ \car\ _\circ|_\circ\ \ldp\,\cdot\ ,\,\dec\ \cdr\rdp\,)^{n+1}					\\
\hspace{7.5ex}\scriptsize \isZero\ \cdr
\end{array} $$
Notice the absence of the \emph{read} direction? Here it is implied given the input pair $ (p,n) $ on the left-hand side.

Finally, we can then use this to define our original factorial:
$$ \def\arraystretch{1.2}
\begin{array}{rcl}
n! & \qdefeq	& (1,n)\ [\ \car\ _\circ|_\circ\ \ldp\,\cdot\ ,\,\dec\ \cdr\rdp\,)^{n+1}		\\
   &		& \hspace{7.5ex}\scriptsize \isZero\ \cdr
\end{array} $$

There's an important change in perspective worth noting in all of this: Up until now the construction and evaluation
of regular functions we're independent of each other, but here this factorial function intermixes its construction with
its evaluation, and the two can't be separated out. This is theoretically relevant as it might indicate the deeper nature
of \emph{recursion}, but it's also something we shouldn't take for granted when we're considering performance.

For example, in the cases where we \emph{can} separate out construction from evaluation, we should. If we can construct
the function first, independent of any input value, we can assign it a name with which to reference later on. Then any
time we evaluate we only need refer to the already constructed function. On the other hand, if we intertwined construction
with evaluation, then every time the function was called we'd be doing a lot of extra work for nothing as we'd be rebuilding
it each time.

As for the factorial function here, if we wanted to minimize memory use in
its construction we could instead reimplement it as the infinity repetition version:
$$ \def\arraystretch{1.2}
\begin{array}{rcl}
n! & \qdefeq	& (1,n)\ [\ \car\ _\circ|_\circ\ \ldp\,\cdot\ ,\,\dec\ \cdr\rdp\,)^\infty		\\
   &		& \hspace{7.5ex}\scriptsize \isZero\ \cdr
\end{array} $$
in which case we'd have to take a lazy policy when constructing it---we'd build only the next part of the function,
evaluate what we could, then continue. In this case though, each individual \texttt{cpose} step is independent of the input,
so we could at least seperate out the single \texttt{stem} operator representing those, so as to build it only once.

\subsection*{Context-Free Induction}

The pumping lemma for regular languages was our turning point for regular induction as it demonstrated the memory
limitations of this style of construction. Now that we have the methods of the signature problem at hand we're
finally ready to change our focus to the next level of induction operators.

To help us ease into things let's actually return to discussing the limits of regular induction, but this time we'll
keep things at an informal level: Such limits can be demonstrated by more closely inspecting any composition where a
component function possesses an \strong{arity} greater than 1. For example, let's consider what happens if
we define some function $ h $ using \texttt{eq?} which itself takes two input. In particular,
if we use the \emph{string paradigm} of regular induction we can implement $ h $ as:
$$ h \qdefeq \eq\ f\ g $$
The problem here is that this is ambiguous. We can interpret such a definition as either of two possibilities:
$$ h\ - \qdefeq \eq\ (f\ -)\ (g\ -) \twoqquad \mbox{or} \twoqquad h\ - \qdefeq \eq\ (f\ g\ -)\ - $$
which depending on $ f, g $ are entirely different functions. These two alternatives demonstrate that we can easily resolve
such ambiguity ourselves by manually specifying parentheses, and this works fine in local contexts, but in the long run
it is \strong{context-free induction} that allows us to systematize this process, and mitigate this memory limitation overall.

\subsubsection*{Context-Free Grammars}

We begin this narrative with an introduction to context-free languages and their grammars, for which I borrow heavily
from \cite{iatlc}. Let's go straight to an example of a simple but well designed context-free grammar:

$$ \def\arraystretch{1.2}
\begin{array}{lcccl}
\mbox{identifier}	& \mbox{-} & I & \to & a\ |\ b\ |\ Ia\ |\ Ib\ |\ I0\ |\ I1		\\
\mbox{factor}		& \mbox{-} & F & \to & I\ |\ (E)					\\
\mbox{term}		& \mbox{-} & T & \to & F\ |\ T*F					\\
\mbox{expression}	& \mbox{-} & E & \to & T\ |\ E+T
\end{array} $$
The idea of such a grammar is that it's made up of a collection of \strong{productions}:
$$ \{\ \mathcal{H}_\alpha \to \mathcal{B}_\beta\ \}_{\alpha, \beta} $$
which themselves are made up of \strong{heads} $ \mathcal{H}_\alpha $ pointing to their respective \strong{bodies}
$ \mathcal{B}_\beta $. In practice, there are often many productions that share the same head but have different bodies,
and so it is common convention to condense them into the appearance of a single production:
$$ \mathcal{H} \to \mathcal{B}_0\ |\ \ldots\ |\ \mathcal{B}_n $$
which in fact is what was done in the above example. Also, notice how such condensation aligns nicely with
the \emph{alternation} notation of regular induction? It's good to keep notations consistent when we can.

As for the components of the production bodies, we consider them to be \emph{characters} which make up the strings of our
context-free language. In particular, the characters that correspond to production heads are called \strong{variables},
while those which don't are called \strong{terminals}, and can otherwise be considered constants.

In our example grammar the variables are:
$$ \{\ E, T, F, I\ \} $$
These variables have also been given natural language names, but this is only for semantic clarity for our sake---it
is otherwise external to the grammar's actual definition. In anycase the terminals in this grammar are as follows:
$$ \{\ a, b, 0, 1, (, ), *, +\ \} $$

As for how these productions, variables, terminals are used within a context-free grammar: They are used to generate
strings by means of what are called \strong{derivations}. One specific variable is declared the \strong{start} (for the whole
grammar, not just the derivation), and from there we choose any of the \texttt{start}'s productions to take us to the next
step by substituting its body. From there, if the given body contains variables of its own we further the derivation by
substituting those with their own appropriately chosen productions. The derivation finally halts once we arrive
at a string containing only terminals. That terminal string is our generated string.

As for an example derivation using the above grammar:
$$ \def\arraystretch{1.2}
\begin{array}{rclllllll}
E & \RA & E+T	& \RA & T+T	& \RA & F+T	& \RA & I+T		\\
  & \RA & a+T	& \RA & a+T*F	& \RA & a+F*F	& \RA & a+I*F		\\
  & \RA & a+a*F	& \RA & a+a*I	& \RA & a+a*a
\end{array} $$
Notice the use of the double bar arrows $ \RA $ instead of the single bar ones $ \to $ we used to define the productions?
I doubt any confusion would arise if we instead used the latter $ \to $, but to distinguish intent the former $ \RA $
is standard. Also, I didn't specify it at the time, but our example grammar's start symbol is the variable $ E $.
As well, the intermediate strings that make up the steps of these derivations don't actually correspond to any
predefined part of the context-free grammar, but for clarity they also have their own name: They are called
\emph{sentential forms}.

Next, given such derivations there is need to introduce the idea of their corresponding \strong{parse trees}.
For our example derivation we have its given parse tree as:

% a+a*a , context-free construction
%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
 \begin{asy}
 unitsize(1cm);

 real step = 0.90;

 pair p00 = (0,0);

 //

 pair p10 = segment(p00, "S", 1);

 pair p20 = segment(p10, "SL", step, 60);
 pair p21 = segment(p10, "S", step);
 pair p22 = segment(p10, "SR", step, 60);

 pair p30 = segment(p20, "S", step);
 pair p31 = segment(p22, "SL", step, 60);
 pair p32 = segment(p22, "S", step);
 pair p33 = segment(p22, "SR", step, 60);

 pair p40 = segment(p30, "S", step);
 pair p41 = segment(p31, "S", step);
 pair p42 = segment(p33, "S", step);

 pair p50 = segment(p40, "S", step);
 pair p51 = segment(p41, "S", step);
 pair p52 = segment(p42, "S", step);

 pair p60 = segment(p50, "S", step);
 pair p61 = segment(p51, "S", step);

 //

 pen stepcolor = gray;
 pen linecolor = heavygray;
 pen bordercolor = white;
 
 draw(p10--p20, linecolor);
 draw(p10--p21, linecolor);
 draw(p10--p22, linecolor);

 draw(p20--p30, linecolor);
 draw(p22--p31, linecolor);
 draw(p22--p32, linecolor);
 draw(p22--p33, linecolor);

 draw(p30--p40, linecolor);
 draw(p31--p41, linecolor);
 draw(p33--p42, linecolor);

 draw(p40--p50, linecolor);
 draw(p41--p51, linecolor);
 draw(p42--p52, linecolor);

 draw(p50--p60, linecolor);
 draw(p51--p61, linecolor);

 //
 
 label("$a+a*a$", p00);

 safeLabel("$E$", p10, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$E$", p20, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$+$", p21, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$T$", p22, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$T$", p30, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$T$", p31, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$*$", p32, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$F$", p33, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$F$", p40, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$F$", p41, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$I$", p42, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$I$", p50, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$I$", p51, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$a$", p52, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$a$", p60, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$a$", p61, 0.30, 0.30, borderpen = bordercolor);

 \end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
The top level of this tree is the start of the derivation, while each level of nodes below it represents the steps
that follow.\footnote{This representation points out that our chain derivation although in appearance is sequential,
is in fact only partially so: It defaults to being concurrent when the respective subderivations are independent.}
Notice how each branch terminates in a leaf node which otherwise contains a terminal for its label? If we now parse
through only these leaf node terminals (regardless of level) and collect them into a string, it represents
the final result of the derivation.

Parse trees bring up one other consideration: \emph{order of derivation}. So far we've only seen one example derivation
which in fact was patterned as a \emph{leftmost derivation}. Such derivations make their substitutions one at a time
from the leftmost variable in each step. There are also \emph{rightmost derivations} which always substitute from the
rightmost variable instead. Then there's everything in between, which starts to get complicated. For our purposes
we will stick to a leftmost policy.

This concludes the basic introduction to context-free grammars. It is relatively trivial to interpret their derivations
as the induction operators we seek: We can use the production patterns to create the strings that become our chain
compositions. With that said, it is less straightforward when we try and consider exactly how to translate such
\emph{manual} derivations into automated constructors. For that we will need to review the next level of automata.

\subsubsection*{Pushdown Automata}

We review pushdown automata here because they are equipotent to context-free languages. This is to say that for every
context-free grammar used to generate a string, it has a corresponding pushdown automata able to recognize it.

Pushdown automata are \emph{acceptors} with a similar nature to that of their \emph{regular automata} predecessors.
In fact they can be considered extensions of such finite state machines, but with an upgrade: They have a memory
\strong{stack}. A theoretical stack is a memory device with unlimited capacity to store information, but it's
limited in how we can access it. Its interface can be summarized as \emph{last in, first out}. This is to say
the last thing we asked it to remember is the first thing we can access from it. If we remove that from its
memory only then we can access the next last rememberance, and so on and so on.

Going back to our example context-free derivation,
$$ \def\arraystretch{1.2}
\begin{array}{rclllllll}
E	& \RA & E+T	& \RA & T+T	& \RA & F+T	& \RA & I+T		\\
	& \RA & a+T	& \RA & a+T*F	& \RA & a+F*F	& \RA & a+I*F		\\
	& \RA & a+a*F	& \RA & a+a*I	& \RA & a+a*a
\end{array} $$
how would a corresponding pushdown automata go about recognizing its derived string ``$ a+a*a $'' ?

As it turns out, pushdown automata use a tried-and-true algorithm known as: Guessing and checking. To ease this
process---at least conceptually---the version of automata we will use here is called \emph{nondeterministic}
pushdown automata with $ \epsilon $-\emph{transitions}. This is just fancy speak for guessing and checking
\emph{in parallel}, where $ \epsilon $ is the empty string and an $ \epsilon $-transition is a technicality
that lets us divide and express such automata during their respective implementations. I offer these formal
names as a reference for anyone who wants to learn more about the subject.

In anycase, let's now change our perspective to that of the pushdown automata itself: The way this parallel guessing
and checking works is we're first given the string $ a+a*a $ in its final form, but we're not told how it was derived.
We then parse this string using a left-to-right policy, which also means we start from the leftmost character $ a $.
We seek to guess and check the first production of this derivation.

We now ask our given stack what its first symbol is, which by convention defaults to the \emph{start symbol} $ E $,
for which we then scan all of the $ E $ productions and try each one. It's unlikely a single production will derive
our full string, but at this point we only seek the productions which don't produce an immediate contradiction when
tested against the current string. At this stage there are only two known productions for our start symbol:
$$ \def\arraystretch{1.2}
\begin{array}{rcl}
E & \to & T				\\
E & \to & E+T
\end{array} $$
It's not immediately clear if either path will lead to a contradiction, but we don't go actively searching for
possible contradictions: If they're going to find us, they're going to find us. As such, this is where we begin
to apply the parallel guessing and checking algorithm: We test both paths as if we were running both processes
independently and concurrently.

For the sake of convenience let's pretend we have a separate stack for each new thread we run.
In this case for each path we replace our starting head $ E $ with its respective body:
$$ \def\arraystretch{1.5}
\begin{array}{lcll}
\bfmbox{stack}		\col		\col[5ex] \bfmbox{stack}_1		\col[5ex] \bfmbox{stack}_2		\\
E			\col \lra	\col[5ex] T				\col[5ex] E+T
\end{array} $$
Keep in mind this is just a visual representation of our stacks---for convenience we can peer into them and read
their whole content, but in actuality we only have access to the top of each stack, which we interpret here as
their leftmost symbols.

Since we have not reached an indication of any contradictions, our second step is to now inspect the top symbol
in each of the updated stacks, which in our example are $ T $ and $ E $, respectively. We apply the same guess
and check strategy and again replace these heads with the bodies of their respective productions:
$$ \def\arraystretch{1.5}
\begin{array}{lcll}
\bfmbox{stack}_1	\col		\col[5ex] \bfmbox{stack}_{1,1}		\col[5ex] \bfmbox{stack}_{1,2}			\\
T			\col \lra	\col[5ex] F				\col[5ex] T*F					\\[1ex]

\bfmbox{stack}_2	\col		\col[5ex] \bfmbox{stack}_{2,1}		\col[5ex] \bfmbox{stack}_{2,2}			\\
E+T			\col \lra	\col[5ex] T+T				\col[5ex] E+T+T
\end{array} $$
In this case, we now have enough information to know that some of these threads will eventually lead to contradictions.
For example \texttt{stack}$_{2,2}$ has two plus signs $ + $ in its string, but the string we're testing against
$ a+a*a $ does not, so no matter how we continue this thread the final derivation will inevitably not match.

\noindent Also, \texttt{stack}$_{1,2}$ contradicts. It's not as immediately obvious because our
test string has a multiplication symbol $ * $, but if you go back to the rules of the grammar:
$$ \def\arraystretch{1.2}
\begin{array}{lcccl}
\mbox{identifier}	& \mbox{-} & I & \to & a\ |\ b\ |\ Ia\ |\ Ib\ |\ I0\ |\ I1		\\
\mbox{factor}		& \mbox{-} & F & \to & I\ |\ (E)					\\
\mbox{term}		& \mbox{-} & T & \to & F\ |\ T*F					\\
\mbox{expression}	& \mbox{-} & E & \to & T\ |\ E+T
\end{array} $$
there is no longer any way to include a plus sign $ + $ in its further derivations. In that case we can
now rule out this possible thread as well, leaving us to continue with a reduced number of stacks:
$$ \def\arraystretch{1.5}
\begin{array}{lcll}
\bfmbox{stack}_1	\col		\col[5ex] \bfmbox{stack}_{1,1}		\\
T			\col \lra	\col[5ex] F				\\[1ex]

\bfmbox{stack}_2	\col		\col[5ex] \bfmbox{stack}_{2,1}		\\
E+T			\col \lra	\col[5ex] T+T
\end{array} $$
and if we repeat our guess and check game yet again we have:
$$ \def\arraystretch{1.5}
\begin{array}{lcll}
\bfmbox{stack}_{1,1}	\col		\col[5ex] \bfmbox{stack}_{1,1,1}	\col[5ex] \bfmbox{stack}_{1,1,2}		\\
F			\col \lra	\col[5ex] I				\col[5ex] (E)					\\[1ex]

\bfmbox{stack}_{2,1}	\col		\col[5ex] \bfmbox{stack}_{2,1,1}	\col[5ex] \bfmbox{stack}_{2,1,2}		\\
T+T			\col \lra	\col[5ex] F+T				\col[5ex] T*F+T
\end{array} $$
Here we also arrive at two new contradictions to rule out, where one of them is because our test string
doesn't contain the left parenthesis symbol `$ ( $' and the other is because our test string in fact does
contain the symbols $ *,+ $, but it does so in the opposite order than what's on the stack.

Applying this process once more we arrive at something different:
$$ \def\arraystretch{1.5}
\begin{array}{lclll}
\bfmbox{stack}_{1,1,1}	\col		\col[5ex] \bfmbox{stack}_{1,1,1,1}	\col[5ex] \bfmbox{stack}_{1,1,1,2}\col[5ex] \ldots \\
I			\col \lra	\col[5ex] a				\col[5ex] b			  \col[5ex] \ldots \\[1ex]

\bfmbox{stack}_{2,1,1}	\col		\col[5ex] \bfmbox{stack}_{2,1,1,1}	\col[5ex] \bfmbox{stack}_{2,1,2,2}		   \\
F+T			\col \lra	\col[5ex] I+T				\col[5ex] (E)+T
\end{array} $$
So far the front of our stacks have only had variables, but now we have some terminal symbols---in particular $ a, b $.
The \texttt{stack}$_{1,1,1,1}$ matches the first character of our string, but that's as far as it can go as there
are no more variables on this particular stack to continue the process. As a consequence of this we have arrived
at another contradictory thread, which is followed by two others when we explore the logic.

The only thread in the above which isn't contradictory is \texttt{stack}$_{2,1,1,1}$
which is what leads us to the first meaningful continuation in this whole process:
$$ \def\arraystretch{1.5}
\begin{array}{lclll}
\bfmbox{stack}_{2,1,1,1}\col		\col[5ex] \bfmbox{stack}_{2,1,1,1,1}	\col[5ex] \bfmbox{stack}_{2,1,1,1,2}	\col[5ex] \ldots \\
I+T			\col \lra	\col[5ex] a+T				\col[5ex] b+T				\col[5ex] \ldots
\end{array} $$
Here \texttt{stack}$_{2,1,1,1,1}$ matches the first two characters of our test string $ a+a*a $, and even has a variable
to continue the thread. As such, we've now reached the end of round one of this recursive loop. We pop the front terminals
from both the stack and our string:
$$ a+T \quad \mbox{becomes} \quad T \tab[1cm] \mbox{and} \tab[1cm] a+a*a \quad \mbox{becomes} \quad a*a $$
Then we start over again with the reduced string $ a*a $ and the reduced stack:
$$ \def\arraystretch{1.5}
\begin{array}{l}
\bfmbox{stack}\msbox{round 2}		\\
T
\end{array} $$
and that's the general algorithm! If we end up arriving at contradictions for all the parallel threads we had started,
it means the test string does not in fact belong to our language of interest. Otherwise, if we successfully find
a derivation then we have confirmed that the string in question does in fact belong.

\subsubsection*{Context-Free Bind Operators}

Pushdown automata are great, but they weren't our final goal either. How can we use
the pushdown algorithm just learned to derive strings rather than to recognize them?

For our example string ``$ a+a*a $'' we would like to be able to model our
induction grammar based on the expressive form of its context-free derivation:
$$ \def\arraystretch{1.2}
\begin{array}{rclllllll}
E	& \RA & E+T	& \RA & T+T	& \RA & F+T	& \RA & I+T		\\
	& \RA & a+T	& \RA & a+T*F	& \RA & a+F*F	& \RA & a+I*F		\\
	& \RA & a+a*F	& \RA & a+a*I	& \RA & a+a*a
\end{array} $$
Truth be told this exact form isn't the most user-friendly for our purposes. If we're reading over this derivation
in person it's a bit awkward to follow the logic without having to first memorize the productions for this particular
grammar. Such memory constraints don't generally scale given that there are infinitely many context-free grammars
in existence, and we certainly could not memorize the productions for them all.

For me the most straightforward and scalable way to express context-free derivations is to use their productions directly:
$$ \def\arraystretch{1.1}
\tab[0cm] \begin{array}{rlll}
	& \bfmbox{production}	\col[15ex] //\tab[3ex] \bfmbox{assumes}	\col[5ex] \bfmbox{derives}		\\[1ex]
 1)	& E \to E+T		\col[15ex] //\tab[3ex] E		\col[5ex] E+T				\\
 2)	& E \to T		\col[15ex] //\tab[3ex] E+T		\col[5ex] T+T				\\
 3)	& T \to F		\col[15ex] //\tab[3ex] T+T		\col[5ex] F+T				\\
 4)	& F \to I		\col[15ex] //\tab[3ex] F+T		\col[5ex] I+T				\\
 5)	& I \to a		\col[15ex] //\tab[3ex] I+T		\col[5ex] a+T				\\
 6)	& T \to T*F		\col[15ex] //\tab[3ex] a+T		\col[5ex] a+T*F				\\
 7)	& T \to F		\col[15ex] //\tab[3ex] a+T*F		\col[5ex] a+F*F				\\
 8)	& F \to I		\col[15ex] //\tab[3ex] a+F*F		\col[5ex] a+I*F				\\
 9)	& I \to a		\col[15ex] //\tab[3ex] a+I*F		\col[5ex] a+a*F				\\
10)	& F \to I		\col[15ex] //\tab[3ex] a+a*F		\col[5ex] a+a*I				\\
11)	& I \to a		\col[15ex] //\tab[3ex] a+a*I		\col[5ex] a+a*a				\\[1ex]
	& \bfmbox{terminal}
\end{array} $$
For each step in this new interface (they are numbered for reference), we show the production used to create
it. This is the \strong{production} column. As for the two side columns, they are commented out because technically
they don't contribute anything to the derivation itself. They're only there for our benefit---so we can follow through
the derivation as if it were a human readable \emph{proof}.\footnote{Such grammatical constructs could be extended
in practice to formally include these side columns. As stated, they wouldn't contribute anything to the derivation
itself, but the implementation of such grammars could then test these additional considerations against various
logics so as to validate, confirm, and verify.}

Let's go through how such a derivation would work, but this time focusing on what's going on behind the scenes.
To do that, we change this \emph{production oriented} interface just a little bit further still:
$$ \def\arraystretch{1.1}
\tab[0cm] \begin{array}{rlll}
	& \bfmbox{production}	\col[5ex] \bfmbox{stack}	\col[6ex] \bfmbox{composite call}	\\[1ex]
 0)	& 			\col[5ex] E			\col[6ex] \id				\\
 1)	& E \to E+T		\col[5ex] E+T			\col[6ex] \id				\\
 2)	& E \to T		\col[5ex] T+T			\col[6ex] \id				\\
 3)	& T \to F		\col[5ex] F+T			\col[6ex] \id				\\
 4)	& F \to I		\col[5ex] I+T			\col[6ex] \id				\\
 5)	& I \to a		\col[5ex] T  			\col[6ex] a+   				\\
 6)	& T \to T*F		\col[5ex] T*F			\col[6ex] a+				\\
 7)	& T \to F		\col[5ex] F*F			\col[6ex] a+				\\
 8)	& F \to I		\col[5ex] I*F			\col[6ex] a+				\\
 9)	& I \to a		\col[5ex] F			\col[6ex] a+a*				\\
10)	& F \to I		\col[5ex] I			\col[6ex] a+a*				\\
11)	& I \to a		\col[5ex] \varnothing		\col[6ex] a+a*a				\\[1ex]
	& \bfmbox{terminal}
\end{array} $$
As with our previous induction grammars, we're effectively building a function by passing it along the \strong{composite},
but this time around we also equip our grammar with a \strong{stack} construct similar to that of pushdown automata.
The \texttt{composite} here is initialized with its default placeholder \texttt{id}, while the \texttt{stack} is
initialized to contain only the \emph{start symbol} $ E $.

We then update both the \texttt{stack} and the \texttt{composite} in line $ 0 $ by using the \texttt{production}
of line $ 1 $. The \texttt{stack} in this line is now the result of substituting the existing top element with the
given production body. We effectively follow the same pattern as the pushdown automata algorithm, continuing
this process as we move down the rows, until we get to line $ 5 $.

In this step, when we make the necessary substitution to update the \texttt{stack} we end up with the sentential form
$ a+T $, which differs from previous steps because we now have a terminal at its top. We continue shadowing the pushdown
algorithm, except here instead of simply popping this terminal from the \texttt{stack} we also move it
over to the \texttt{composite} column.

As a point of fact we've appended the terminal to the back of the current function. As such, I have labelled the column
with a \strong{call} modifier. Notice how the plus sign $ + $ terminal has also been moved from the \texttt{stack} over
to the \texttt{composite}? Once we start the process of moving terminals to the composition it's necessary to move
them all to continue. In summary of line $ 5 $, we apply the \texttt{production} and then update the \texttt{stack}
to $ T $, while the \texttt{composite} then becomes $ a+ $. It is at this point that the algorithm repeats---that is
until the \texttt{stack} becomes empty, at which point it halts.

This is the overall idea of our context-free induction grammar, except such an interpretation still doesn't lend itself
directly to the implementation we seek. In that case, we turn to the idea of a \texttt{bind} operator, the conventional
wisdom being that wherever there's a \emph{function} to be evaluated, there's a \texttt{bind} operator to evaluate it.
This also suggests that wherever there's a function to be constructed, there's a bind operator for that as well.

This inspires the following \texttt{bind}, which we call \strong{derive}:
$$ \def\arraystretch{1.2}
\tab[0cm] \begin{array}{lll}
\bfmbox{define}\ \ (stack,\ composite)\tab[2ex] \deftbind{derive}\tab[2ex] (head,\ body)					\\[2ex]

\reppose{\infty}{(stack,\ composite)}												\\
\bfmbox{closing}	\col[-2.5cm] \bfmbox{pose}			\col[1cm] \bfmbox{chain call}				\\[0.5ex]

\alias\ \ (local\_stack,\ local\_composite)											\\[-0.5ex]
\assign\ \ (front,\ rest)\ \ local\_stack											\\[1.5ex]

\eq\ front\ \ head	\col[-2.5cm] \bfmbox{chain pass}		\col[1cm] \cdr						\\
			\col[-2.5cm] \cons				\col[1cm] \push\ front					\\
			\col[-2.5cm] \concat\ body			\col[1cm] \cons\ rest					\\
			\col[-2.5cm] rest\ local\_composite									\\
\bfmbox{end}
\end{array} $$
The intention of this algorithm is to take a $ (stack, composite) $ pair, and apply the $ production $ which here is
represented as $ (head, body) $. The expected return is the next $ stack $ and $ composite $ pair in the derivation.

There's actually a lot going on in this source code, so we should go over the details. For starters, if you're not
use to \emph{abstract} bind operators you might take issue with the $ (head, body) $ pair representing a function.
Referring back to bind's notational definitions we have:

\ \\
$$ \def\arraystretch{1.5}
\begin{array}{rrl}
x\vdash f	&  = & f\dashv x 		\\
		& := & f\langle x\rangle
\end{array} $$
\ \\

\noindent It seems implicit that $ f $ should be a function. Remember, our theory of binds comes from category theory,
but this isn't the way category theory actually works: As a matter of fact anything can be a \emph{morphism} with respect
to how a given category is interpreted---not just the intuitive functions that are a natural inspiration. If this
isn't fully satisfactory keep in mind that the $ (head, body) $ pair is meant to represent a \emph{production},
which if it's more to your liking we could reinterpret to be a function if we really needed to.

Next, notice the use of the \texttt{alias} and \texttt{assign} operators? These grammatical constructs haven't been
introduced yet. The \texttt{alias} keyword is more of a convenience than anything, it allows us to \emph{pattern match}
the current input and give temporary names to its internal structure. This is just to help clarify the situation for
the human code writer. As for \texttt{assign}, it goes back to the methodology of \emph{compression}, and the idea
of \emph{scope signatures}: In effect we are \emph{assigning} a variable to a value so that we only need to compute
it once, and can otherwise refer to it from then on. For our convenience it can also pattern match.

Lastly, I should state to be fair that this particular implementation doesn't following best practice coding, though
you might not realize it as this grammatical form is still new to us. In particular notice the internal column functions:

\ \\
$$ \def\arraystretch{1.2}
\tab[0cm] \begin{array}{ll}
\bfmbox{chain pass}		\col[3cm] \bfmbox{chain call}				\\
\cons				\col[3cm] \cdr						\\
\concat\ body			\col[3cm] \push\ front					\\
rest\ local\_composite		\col[3cm] \cons\ rest
\end{array} $$
\ \\

\noindent It would have been better to have defined these as their own functions in advance given that they tend to read
as a bit awkward here and otherwise obscure the intended meaning. In fact we could have done as much using \texttt{assign},
but it didn't exist for us just yet.

Beyond that, note that this above \texttt{bind} operator implementation is rather streamlined: There's a lot of things
that could in fact go wrong which would break the consistency semantics. The idea is that instead, if everything went
right---meaning all the necessary assumptions were met---this algorithm would work as advertised.
Let's now examine what could go wrong:

\begin{itemize}
\item We assumed the stack was not empty, but it might be. This assumption is evident by the fact
      that we pattern matched against its first and second elements with the \texttt{assign} operator.
\item We assumed the production was valid for the underlying context-free grammar, and it might not be.
\item We assumed if the $ front $ of the stack did not equal the $ head $, it meant the $ front $ was a terminal.
      It's possible the production is valid, but the derivation as a whole isn't. In which case both the $ front $
      and $ head $ might be variables which are simply not equal.
\end{itemize}

With these considerations in mind, we can now reinforce the stability of this code with several tests to make sure
the assumptions are actually valid before we apply the bind:

\vfill

\newpage

$$ \def\arraystretch{1.2}
\begin{array}{l}
\induct\ \mbox{derivation\_\tab[-0.5ex] maker}\ \ context\_free\_\,grammar:							\\

\tab \begin{array}{llll}
\assign\ \ cfg\ \ context\_free\_\,grammar											\\[1ex]
\bfmbox{define}\ \ (stack,\ composite)\tab[2ex] \defvbind{$cfg$}\tab[2ex] (head,\ body)						\\[2ex]

(head,\ body)															\\[1ex]
\bfmbox{closing}			\col[-3.5cm] \bfmbox{pose}	\col[-1.5cm] \bfmbox{id}				\\[2ex]
\isNotProd\ \ cfg\ 			\col[-3.5cm] error\ \ 0
					 &				\col[1ex] \cbox{if it's not a production,}		\\
					&&				\col[1ex] \cbox{break with $ error\ 0 $,}		\\
					&&				\col[1ex] \cbox{otherwise continue.}			\\[1ex]

\isNull\ \ stack			\col[-3.5cm] error\ \ 1
					 &				\col[1ex] \cbox{if the stack is empty,}			\\
					&&				\col[1ex] \cbox{break with $ error\ 1 $,}		\\
					&&				\col[1ex] \cbox{otherwise continue.}			\\

\assign\ \ front\ \ \car\ stack													\\[2ex]

\isVar\ \ cfg\ front			&&				\col[1ex] \cbox{if the front of the stack is}		\\
\mbox{and not}\ \eq\ front\ head	\col[-3.5cm] error\ \ 2
					 &				\col[1ex] \cbox{a variable, but not equal}		\\
					&&				\col[1ex] \cbox{to the head, then break}		\\
					&&				\col[1ex] \cbox{with $ error\ 2 $, else\ldots}		\\

\bfmbox{chain call}														\\[1ex]
(stack,\ composite)\tab[2ex]
\deftbind{derive}			&&				\col[1ex] \cbox{apply \deftbind{derive}}		\\[1ex]
\bfmbox{halt}
\end{array}
\end{array} $$

With this algorithm at hand we are almost ready to translate the previous vertical \emph{derivation} grammatical
form into its proper implementation, but for one additional function definition, the \strong{start} of the bind:
$$ \start(E) \qdefeq (E,\ \id) $$

Finally, our example derivation with its $ a+a*a $ construction can be expressed as follows:
$$ \def\arraystretch{1.2}
\tab[0cm] \begin{array}{rllrll}
	& \bfmbox{production}												\\[1ex]
 0)	& \start(E)	\col[0ex] \defvbind{cfg}	\col[1cm] 6)	& T \to T*F	\col[0ex] \defvbind{cfg}	\\
 1)	& E \to E+T	\col[0ex] \defvbind{cfg}	\col[1cm] 7)	& T \to F	\col[0ex] \defvbind{cfg}	\\
 2)	& E \to T	\col[0ex] \defvbind{cfg}	\col[1cm] 8)	& F \to I	\col[0ex] \defvbind{cfg}	\\
 3)	& T \to F	\col[0ex] \defvbind{cfg}	\col[1cm] 9)	& I \to a	\col[0ex] \defvbind{cfg}	\\
 4)	& F \to I	\col[0ex] \defvbind{cfg}	\col[1cm]10)	& F \to I	\col[0ex] \defvbind{cfg}	\\
 5)	& I \to a	\col[0ex] \defvbind{cfg}	\col[1cm]11)	& I \to a					\\[1ex]
	& \bfmbox{terminal}
\end{array} $$
we then retrieve our final composition from the second component of the returned pair.

Admittedly in the case of context-free induction, for this to actually work we would have to specially interpret
terminals such as the left `$ ( $' and right `$ ) $' parentheses, not to mention the use of \emph{infix} functions
such as $ +, * $. Actually, regarding infix operators, so far we've taken them for granted so just to be thorough
here I will specify their rule of application:
$$ -_x\ \{ infix\}\ -_y \qeq \{ prefix\}\ -_x\ -_y $$

\subsubsection*{Ambiguity}

One aspect of context-free grammars we have not yet discussed is the idea of \strong{ambiguity}.

In particular, ambiguity is an issue that initially arises with pushdown automata: In the process of trying
to recognize a string the automata will determine if the string belongs or not, but in the case that it does
nothing is said about whether the parse tree associated with its derivation is unique.

This wasn't an issue with (what is at this point) our canonical string example $ a+a*a $, as the productions of its
context-free grammar were designed to prevent such ambiguity, but if for example we condense the underlying grammar
to the following:
$$ \def\arraystretch{1.2}
\begin{array}{lclcl}
\mbox{identifier}	& \mbox{-} & I & \to & a\ |\ b\ |\ Ia\ |\ Ib\ |\ I0\ |\ I1		\\
\mbox{expression}	& \mbox{-} & E & \to & I\ |\ E+E\ |\ E*E\ |\ (E)
\end{array} $$
we are still able to derive the string $ a+a*a $, but now there are alternative paths to do so:
$$ \def\arraystretch{1.2}
\begin{array}{rlclllllll}
\mbox{left derivation:}								\\[2ex]

& E	& \RA & E+E	& \RA & I+E	& \RA & a+E	& \RA & a+E*E		\\
&	& \RA & a+I*E	& \RA & a+a*E	& \RA & a+a*I	& \RA & a+a*a		\\[2ex]

\mbox{right derivation:}							\\[2ex]

& E	& \RA & E*E	& \RA & E*I	& \RA & E*a	& \RA & E+E*a		\\
&	& \RA & E+I*a	& \RA & E+a*a	& \RA & I+a*a	& \RA & a+a*a
\end{array} $$
To be clear, ambiguity isn't an issue because there exists more than one derivation for a string,\footnote{Keep
in mind it's possible to have more than one derivation for a string such that they still result in the same parse tree.}
it is an issue solely due to the existence of more than one parse tree:

% a+a*a , ambiguous context-free constructions
%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
 \begin{asy}
 unitsize(1cm);

 real step = 0.90;

 //

 pair p00 = (0,0);

 pair p10 = segment(p00, "S", 1);

 pair p20 = segment(p10, "SL", step, 60);
 pair p21 = segment(p10, "S", step);
 pair p22 = segment(p10, "SR", step, 60);

 pair p30 = segment(p20, "S", step);
 pair p31 = segment(p22, "SL", step, 60);
 pair p32 = segment(p22, "S", step);
 pair p33 = segment(p22, "SR", step, 60);

 pair p40 = segment(p30, "S", step);
 pair p41 = segment(p31, "S", step);
 pair p42 = segment(p33, "S", step);

 pair p50 = segment(p41, "S", step);
 pair p51 = segment(p42, "S", step);

 //

 pair q00 = segment(p00, "E", 10);

 pair q10 = segment(q00, "S", 1);

 pair q20 = segment(q10, "SL", step, 60);
 pair q21 = segment(q10, "S", step);
 pair q22 = segment(q10, "SR", step, 60);

 pair q30 = segment(q20, "SL", step, 60);
 pair q31 = segment(q20, "S", step);
 pair q32 = segment(q20, "SR", step, 60);
 pair q33 = segment(q22, "S", step);

 pair q40 = segment(q30, "S", step);
 pair q41 = segment(q32, "S", step);
 pair q42 = segment(q33, "S", step);

 pair q50 = segment(q40, "S", step);
 pair q51 = segment(q41, "S", step);

 //

 pen stepcolor = gray;
 pen linecolor = heavygray;
 pen bordercolor = white;

 //
 
 draw(p10--p20, linecolor);
 draw(p10--p21, linecolor);
 draw(p10--p22, linecolor);

 draw(p20--p30, linecolor);
 draw(p22--p31, linecolor);
 draw(p22--p32, linecolor);
 draw(p22--p33, linecolor);

 draw(p30--p40, linecolor);
 draw(p31--p41, linecolor);
 draw(p33--p42, linecolor);

 draw(p41--p50, linecolor);
 draw(p42--p51, linecolor);

 //
 
 draw(q10--q20, linecolor);
 draw(q10--q21, linecolor);
 draw(q10--q22, linecolor);

 draw(q20--q30, linecolor);
 draw(q20--q31, linecolor);
 draw(q20--q32, linecolor);
 draw(q22--q33, linecolor);

 draw(q30--q40, linecolor);
 draw(q32--q41, linecolor);
 draw(q33--q42, linecolor);

 draw(q40--q50, linecolor);
 draw(q41--q51, linecolor);

 //
 
 label("$a+a*a$", shift(5,0)*p00);

 //

 safeLabel("$E$", p10, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$E$", p20, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$+$", p21, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$E$", p22, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$I$", p30, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$E$", p31, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$*$", p32, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$E$", p33, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$a$", p40, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$I$", p41, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$I$", p42, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$a$", p50, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$a$", p51, 0.25, 0.25, borderpen = bordercolor);

 //

 safeLabel("$E$", q10, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$E$", q20, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$*$", q21, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$E$", q22, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$E$", q30, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$+$", q31, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$E$", q32, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$I$", q33, 0.30, 0.30, borderpen = bordercolor);

 safeLabel("$I$", q40, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$I$", q41, 0.30, 0.30, borderpen = bordercolor);
 safeLabel("$a$", q42, 0.25, 0.25, borderpen = bordercolor);

 safeLabel("$a$", q50, 0.25, 0.25, borderpen = bordercolor);
 safeLabel("$a$", q51, 0.25, 0.25, borderpen = bordercolor);

 \end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
Any corresponding pushdown automata could tell us which one it used in \emph{its} derivation, but not which one was
used in the original.

The problem here is that parse trees implicitly represent the means by which we \emph{group} the terms of a string.
Returning to our canonical string, its terms can be grouped as $ a+(a*a) $ or $ (a+a)*a $, but with respect to
their intended meanings they are generally different:
$$ \tab[4cm] a+(a*a) \quad \neq \quad (a+a)*a \tab[2cm] \cbox{sometimes equal, but not usually.} $$
This also ties back to where we started in context-free grammar theory, with our ambiguous and \emph{motivating} composition:
$$ h \qdefeq \eq\ f\ g $$
which as we remember could be interpreted to represent two distinct functions:
$$ h\ - \qdefeq \eq\ (f\ -)\ (g\ -) \twoqquad \mbox{or} \twoqquad h\ - \qdefeq \eq\ (f\ g\ -)\ - $$

Ambiguity in grammars is an issue for us then because parse trees are inevitably what determine the order of evaluation
of the functions we inductively construct. We may be able to solve the problem locally with parentheses, but it's not
something we should simply assume as given within the larger discourse.

As for the nature of ambiguity within this larger discourse? It is in fact relative to the \emph{context-free languages}
that such grammars represent. Some languages are \strong{inherently ambiguous}, this being in the sense that all grammars
that generate them are themselves ambiguous. Unfortunately there's no easy way to differentiate these various kinds
of languages: It is a known result that there is no single universal algorithm to test for ambiguity of a language.

In terms of languages which aren't inherently ambiguous, by logical necessity at least one of their respective grammars
is safe for our evaluations, but it's certainly possible other grammars which generate the same language aren't. As there
is no universal algorithm to test languages, it also means there's no universal algorithm to translate an unsafe grammar
(of an otherwise unambiguous language) into a safer form.

All of this is to say any logical verifications or mitigating translations are a case by case, one ad-hoc proof at a time,
sort of situation. If we're especially clever we might find \emph{kinds} of context-free grammars which do share proofs
or translations, but that's just about as good as it gets. In anycase, this is an important issue to recognize and accept
as once we acknowledge such complexity we can begin to mitigate it. With that said, are there at least some best practices?

Yes. Generally speaking the first best approach when we can't reduce the complexity of a context-free language is to reduce
the complexity of the grammar we use as its interface. We do this by translating its productions into an alternative set
which are simpler in form but still generate the same language. There are four specific strategies:

\begin{enumerate}
\item We can reduce all $ \epsilon $-productions down to the single default start symbol production $ S \to \epsilon $.
\item We can get rid of the \emph{unit productions}, which are the rules that cast one variable into another $ U \to V $.
\item We can get rid of what are known as \emph{useless symbols}, which are otherwise present in some grammars
      but don't actually contribute to the derivations of any terminal strings.
\item We can translate a given grammar into what's called the \strong{Chomsky normal form} where the only bodies
      for their respective productions are either:
	\begin{enumerate}
	\item single terminals, for example $ H \to t $ for variable $ H $ and terminal $ t $.
	\item two catenated variables, for example $ H \to UV $ for variables $ H, U, V $.
	\end{enumerate}
\end{enumerate}

To be clear, these approaches alone don't mitigate ambiguity, rather they are the first steps toward creating
\strong{tiebreaking} strategies to disambiguate parse trees. Such clarified productions can then be used within single
serve proofs that the given tiebreakers do in fact create unambiguous grammars. Unfortunately there's also a tradeoff:
In practice the forms of individual productions may simplify, but in turn the total number of such productions often
increases.

I should mention there are other applications of the above clarifying strategies, for example the Chomsky normal form in
particular improves the performance of the parsing algorithms for its respective languages. In anycase, the implementation
of such grammatical translations as well as any resulting parser optimizations are beyond the scope of this essay,
for that I would refer you back to \cite{iatlc}.

Finally, there is one additional method worth mentioning here to mitigate ambiguity: We \emph{prevent} it from happening in
the first place---for which our induction grammar is then ideal. By specifying the derivation of a function rather than just
hand coding it, a compiler could create and store the parse tree as part of the definition. When the function shows up later
in the source code, the parse tree is already known thus preventing the ambiguity in the first place.

\subsubsection*{The Context-Free Pumping Lemma}

As with regular languages, context-free languages have their own version of the pumping lemma:

\begin{center}
\begin{minipage}{13cm}
Let $ \mathcal{L} $ be a context-free language with strings $ u,v,w,x,y,z \in \mathcal{L} $ such that $ uvwxy = z $
and  $ vx \neq \epsilon $. There exists a natural number $ n \in \mathbb{N} $ (dependent only on $ \mathcal{L} $)
such that if $ \length(z) \ge n $, and $ \length(vwx) \le n $, then for all $ k\in\mathbb{N} $ we have:
$$ uv^kwx^ky \in \mathcal{L} $$
\end{minipage}
\end{center}

The proof of this lemma is somewhat abstract, but reduced to its simplest idea it again rests on the
\emph{pigeonhole principle}. As broad outline of the proof we actually start with a grammar for the language
in question and translate it into its \emph{Chomsky normal form}. The parse trees end up translating into
\emph{binary trees} for which it then becomes easier to find a pigeonhole induced memory constraint---creating
the stated repetition.

Given that this schema of \emph{inducing repetition through the pigeonhole principle} shows up in both this and the
regular pumping lemma, I suspect that wherever one meets a repeating pattern within automata---or maybe even in life's
designs---there's probably some memory limitation hidden in its details that forces such symmetry in the first place.
This is an interesting thought, though it's neither here nor there.

\subsubsection*{Regular Languages as Context-Free Grammars}

As we're nearing the end of our \emph{context-free} discussion, let's look more closely at the relationship
between context-free induction and regular induction. In particular, context-free functions are an extension
of regular functions, which can be seen from the result that context-free languages are extensions of regular ones.

To understand this, we first might recall that regular expressions are constructed from \emph{catenation},
\emph{alternation}, and \emph{repetition}. As such, we only need describe these generic operators in a context-free
way, as I \emph{naively} do so here:\footnote{I describe this translation as naive because it would need to be proven
to generate exactly the regular expressions with alphabet $ \{c_1, \ldots, c_n\} $. As this is an essay only meant
to convey major ideas, I do not confirm this result here, and although I do not want to set a bad example I must
acknowledge I have not in fact formally verified this grammar myself. My reasoning is that this particular grammar
translates the recursive definition of regular expressions quite faithfully, and if nothing else at least contains
the language of regular expressions, which for the purpose of this subsubsection is more relevant.}
$$ \def\arraystretch{1.2}
\begin{array}{lclcl}
\mbox{characterization}		& \mbox{-} & S & \to & \epsilon\ |\ c_1\ |\ \ldots\ |\ c_n		\\
\mbox{parenthesization}		& \mbox{-} & P & \to & (E)						\\
\mbox{repetition}		& \mbox{-} & R & \to & E^*						\\
\mbox{catenation}		& \mbox{-} & C & \to & EE						\\
\mbox{alternation}		& \mbox{-} & A & \to & E\ \mbox{`$|$'}\ E				\\
\mbox{expression}		& \mbox{-} & E & \to & S\ |\ P\ |\ R\ |\ C\ |\ A
\end{array} $$
In this grammar $ E $ is the start symbol. Note the single quotes `$|$' enclosing the alternation bar? They're there
to distinguish between the bar symbol used within regular expressions, and the ones used in context-free productions.

As example of this regular grammar, we can now derive the following expression:
$$ \def\arraystretch{1.25}
\begin{array}{rclllllll}
E	& \RA & C		& \RA & EE		& \RA & PE		& \RA & (E)E			\\
	& \RA & (A)E		& \RA & (E|E)E		& \RA & (S|E)E		& \RA & (c_1|E)E		\\
	& \RA & (c_1|S)E	& \RA & (c_1|c_2)E	& \RA & (c_1|c_2)R	& \RA & (c_1|c_2)E^*		\\
	& \RA & (c_1|c_2)S^*	& \RA & (c_1|c_2)c_3^*
\end{array} $$
with $ c_1, c_2, c_3 $ being characters belonging to the underlying regular alphabet.

There is actually a second way to interpret regular expressions as context-free grammars: Each fully defined 
expression was originally meant to represent its own language of strings, and so could also be translated into
its own set of context-free productions representing that same language.

Let's break down and translate the major components of our current derivation $ (c_1|c_2)c_3^* $.
We start by observing this expression has a repetition $ c_3^* $, so we first translate it into the following productions:
$$ R \to Rc_3\ |\ \epsilon $$
This works because we can keep substituting $ R $ in the body until we've repeated the character $ c_3 $ the number of
times we wanted, then we can close it off with $ \epsilon $. Our expression also has a catenation, which we translate as:
$$ C \to AR $$
Finally, our expression has an alternation $ (c_1|\,c_2) $ for we can define the alternatives as individual productions:
$$ A \to c_1\ |\ c_2 $$
If we now define our start symbol as $ C $, this grammar can be summarized \emph{freely} as:
$$ \def\arraystretch{1.1}
\begin{array}{lclcl}
\mbox{repetition}		& \mbox{-} & R & \to & Rc_3\ |\ \epsilon			\\
\mbox{alternation}		& \mbox{-} & A & \to & c_1\ |\ c_2				\\
\mbox{catenation}		& \mbox{-} & C & \to & AR
\end{array} $$
which will generate the language matching the expression $ (c_1|c_2)c_3^* $.

Any other regular expressions can be translated to their respective context-free productions in similar ways.
In anycase, this outlines how regular induction operators can be reimplemented as context-free derivations.

\subsubsection*{Grammatical Paths and Context-Free Grammars}

We haven't gotten much into the function semantics of \cite{nikfs}, but there does seem to be an overlap between
grammatical path notation and context-free parse trees---both using trees to represent functions. Overall, untyped
grammatical path functions are universally computable and are thus the more general theory, but their tree oriented
design does still suggest a connection or two. It's beyond the scope of this essay to explore these in any serious way,
but I would still like to present the basic correspondences.

For starters, it's relatively easy to show that the definitions of the \emph{bodies} and \emph{signatures} of grammatical
path functions are derivable using context-free grammars. I won't formalize anything here,\footnote{If it were to be done,
one potential advantage in formalizing a context-free grammar for grammatical path components is that the derivations
would actually suggest how such functions could be evaluated, though a few tweaks would be needed to make recursion work.}
but the bodies of grammatical paths always start with a root function for which we then build more complex bodies by
substituting additional functions for their argument variables (creating compositions). This could be represented as
$ V \to B $.  Otherwise, we substitute actual argument values, which could be represented as $ V \to a $. For any of this
to work though, we'd need a set of predefined function primitives, but instead of giving them their own productions
(such as $ F \to f $) it would be better to embed them directly into body productions such as:
$$ B \quad \to \quad f\;\underbrace{V\ \ldots\ V}_{arity} $$
The assumption here is that any given function primitive will have a fixed \strong{arity} which we could then
tailor for its respective production. The productions of signatures could be theorized in a similar way.

On the flip side, showing a correspondence in the other direction---where we define context-free derivations using
grammatical paths---is actually a bit trickier. To walk us through it, we'll once again refer to our former example grammar:
$$ \def\arraystretch{1.2}
\begin{array}{lcccl}
\mbox{identifier}	& \mbox{-} & I & \to & a\ |\ b\ |\ Ia\ |\ Ib\ |\ I0\ |\ I1		\\
\mbox{factor}		& \mbox{-} & F & \to & I\ |\ (E)					\\
\mbox{term}		& \mbox{-} & T & \to & F\ |\ T*F					\\
\mbox{expression}	& \mbox{-} & E & \to & T\ |\ E+T
\end{array} $$
along with its example derivation:
$$ \def\arraystretch{1.1}
\begin{array}{rclllllll}
E & \RA & E+T	& \RA & T+T	& \RA & F+T	& \RA & I+T		\\
  & \RA & a+T	& \RA & a+T*F	& \RA & a+F*F	& \RA & a+I*F		\\
  & \RA & a+a*F	& \RA & a+a*I	& \RA & a+a*a
\end{array} $$

The first step in specifying context-free grammars as grammatical paths is to translate
their productions into something more along the lines of function definitions:
$$ \def\arraystretch{1}
\begin{array}{lclcl}
\mbox{identifier} & \mbox{-} & I(index) & := &
	\begin{array}[t]{l}\def\arraystretch{1.2}
	a\ |\ b\ |\ I(-)a\ |\ I(-)b\ |\ I(-)0\ |\ I(-)1			\\[-1ex]
	\mbox{\scriptsize $ index $}					\\
	\end{array}							\\[2ex]

\mbox{factor}	  & \mbox{-} & F(index) & := &
	\begin{array}[t]{l}\def\arraystretch{1.2}
	I(-)\ |\ (E(-))							\\[-1ex]
	\mbox{\scriptsize $ index $}					\\
	\end{array}							\\[2ex]

\mbox{term}	  & \mbox{-} & T(index) & := &
	\begin{array}[t]{l}\def\arraystretch{1.2}
	F(-)\ |\ T(-_1)*F(-_2)						\\[-1ex]
	\mbox{\scriptsize $ index $}					\\
	\end{array}							\\[2ex]

\mbox{expression} & \mbox{-} & E(index) & := &
	\begin{array}[t]{l}\def\arraystretch{1.2}
	T(-)\ |\ E(-_1)+T(-_2)						\\[-1ex]
	\mbox{\scriptsize $ index $}					\\
	\end{array}
\end{array} $$
Here we're also using the regular induction paradigm for our convenience.

As for our example derivation, it would then translate as follows:
$$ \def\arraystretch{1.5}
\begin{array}{rclllll}
E(1)	& \RA & E(0)+T(-)		& \RA & T(0)+T(-)		& \RA & F(0)+T(-)		\\
	& \RA & I(0)+T(-)		& \RA & a(-_x)+T(1)		& \RA & a(-_x)+T(0)*F(-)	\\
	& \RA & a(-_x)+F(0)*F(-)	& \RA & a(-_x)+I(0)*F(-)	& \RA & a(-_x)+a(-_y)*F(0)	\\
	& \RA & a(-_x)+a(-_y)*I(0)	& \RA & a(-_x)+a(-_y)*a(-_z)
\end{array} $$

Finally, if we were to translate this into a grammatical path equivalent, we would have:
% a+a*a , grammatical path construction
%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                  %
                                  %
\begin{center}
 \begin{asy}
 unitsize(1cm);

 pair p00 = (0,0);

 pair p10 = segment(p00, "S", 1.25);
 pair p11 = segment(p10, "E", 6);

 pair p20 = segment(p10, "S", 1);
 pair p30 = segment(p20, "S", 1);
 pair p40 = segment(p30, "S", 1);
 pair p50 = segment(p40, "S", 1);

 //

 real step = 0.5625;

 real angle1 = 70;
 real angle2 = 60;
 real angle3 = 45;

 real off1 = 0.06;
 real off2 = 0.04;
 real off3 = 0.03;

 pair q10 = segment(p10, "EU", 13.25, 0.4);
 pair lq10 = offset(q10, "W", off1);
 pair rq10 = offset(q10, "E", off1);

 pair q20 = segment(q10, "SL", step, angle1);
 pair lq20 = offset(q20, "W", off1);
 pair rq20 = offset(q20, "E", off1);
 pair q21 = segment(q10, "SR", step, angle1);

 pair q30 = segment(q20, "SL", step, angle1);
 pair lq30 = offset(q30, "W", off2);
 pair rq30 = offset(q30, "E", off2);
 pair q31 = segment(q20, "SR", step, angle1);
 pair lq31 = offset(q31, "W", off2);
 pair rq31 = offset(q31, "E", off2);

 pair q40 = segment(q30, "SL", step, angle2);
 pair lq40 = offset(q40, "W", off2);
 pair rq40 = offset(q40, "E", off2);
 pair q41 = segment(q30, "SR", step, angle2);
 pair q42 = segment(q31, "SL", step, angle2);
 pair lq42 = offset(q42, "W", off2);
 pair rq42 = offset(q42, "E", off2);
 pair q43 = segment(q31, "SR", step, angle2);

 pair q50 = segment(q40, "SL", step, angle2);
 pair lq250 = offset(q50, "W", off2);
 pair rq250 = offset(q50, "E", off2);
 pair lq350 = offset(q50, "W", off3);
 pair rq350 = offset(q50, "E", off3);
 pair q51 = segment(q40, "SR", step, angle2);
 pair q52 = segment(q42, "SL", step, angle2);
 pair lq52 = offset(q52, "W", off3);
 pair rq52 = offset(q52, "E", off3);
 pair q53 = segment(q42, "SR", step, angle2);
 pair lq53 = offset(q53, "W", off3);
 pair rq53 = offset(q53, "E", off3);

 pair q60 = segment(q50, "SL", step, angle3);
 pair lq60 = offset(q60, "W", off3);
 pair rq60 = offset(q60, "E", off3);
 pair q61 = segment(q50, "SR", step, angle3);
 pair q62 = segment(q52, "SL", step, angle3);
 pair lq62 = offset(q62, "W", off3);
 pair rq62 = offset(q62, "E", off3);
 pair q63 = segment(q52, "SR", step, angle3);
 pair q64 = segment(q53, "SL", step, angle3);
 pair lq64 = offset(q64, "W", off3);
 pair rq64 = offset(q64, "E", off3);
 pair q65 = segment(q53, "SR", step, angle3);

 pair q70 = segment(q60, "SL", step, angle3);
 pair lq70 = offset(q70, "W", off3);
 pair rq70 = offset(q70, "E", off3);
 pair q71 = segment(q60, "SR", step, angle3);
 pair q72 = segment(q62, "SL", step, angle3);
 pair lq72 = offset(q72, "W", off3);
 pair rq72 = offset(q72, "E", off3);
 pair q73 = segment(q62, "SR", step, angle3);
 pair q74 = segment(q64, "SL", step, angle3);
 pair lq74 = offset(q74, "W", off3);
 pair rq74 = offset(q74, "E", off3);
 pair q75 = segment(q64, "SR", step, angle3);

 pair q80 = segment(q70, "SR", step, angle3);
 pair q81 = segment(q72, "SL", step, angle3);
 pair lq81 = offset(q81, "W", off3);
 pair rq81 = offset(q81, "E", off3);
 pair q82 = segment(q72, "SR", step, angle3);
 pair q83 = segment(q74, "SR", step, angle3);

 pair q90 = segment(q81, "SR", step, angle3);

 //

 pen arrowcolor = mediumred + fontsize(8pt);

 draw(shift(-1.5,0)*p11--shift(1.5,0)*p11, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p20--shift(0.4,0.5)*p20, arrowcolor, Arrow);
 draw(shift(0.4,-0.5)*p40--shift(0.4,0.5)*p40, arrowcolor, Arrow);
  
 //

 pen stepcolor = gray + fontsize(8pt);
 pen fillcolor = lightgray;
 pen bordercolor = heavygray;
 pen textcolor = fontsize(10pt);

 //

 draw(lq10--lq20, bordercolor);
 draw(rq10--rq20, bordercolor);
 draw(q10--q21, bordercolor);

 draw(q20--q30, bordercolor);
 draw(q20--q31, bordercolor);

 draw(lq30--lq40, bordercolor);
 draw(rq30--rq40, bordercolor);
 draw(q30--q41, bordercolor);
 draw(lq31--lq42, bordercolor);
 draw(rq31--rq42, bordercolor);
 draw(q31--q43, bordercolor);

 draw(lq40--lq250, bordercolor);
 draw(rq40--rq250, bordercolor);
 draw(q40--q51, bordercolor);
 draw(q42--q52, bordercolor);
 draw(q42--q53, bordercolor);

 draw(lq350--lq60, bordercolor);
 draw(rq350--rq60, bordercolor);
 draw(q50--q61, bordercolor);
 draw(lq52--lq62, bordercolor);
 draw(rq52--rq62, bordercolor);
 draw(q52--q63, bordercolor);
 draw(lq53--lq64, bordercolor);
 draw(rq53--rq64, bordercolor);
 draw(q53--q65, bordercolor);

 draw(lq60--lq70, bordercolor);
 draw(rq60--rq70, bordercolor);
 draw(q60--q71, bordercolor);
 draw(lq62--lq72, bordercolor);
 draw(rq62--rq72, bordercolor);
 draw(q62--q73, bordercolor);
 draw(lq64--lq74, bordercolor);
 draw(rq64--rq74, bordercolor);
 draw(q64--q75, bordercolor);

 draw(q70--q80, bordercolor);
 draw(lq72--lq81, bordercolor);
 draw(rq72--rq81, bordercolor);
 draw(q72--q82, bordercolor);
 draw(q74--q83, bordercolor);

 draw(q81--q90, bordercolor);

 //
 
 label("$a(x)+a(y)*a(z)$", shift(5.5,0)*p00, E, fontsize(10pt));

 label("$(\ldots)$", p10, E, fontsize(10pt));

 label("applicate", p11, N, arrowcolor);

 label("duplicate", shift(0.5,-0.1)*p20, E, arrowcolor);

 label("$(E, T, F, I, a, +, *, x, y, z, 0, 1)$", p30, E, fontsize(10pt));

 label("prepare", shift(0.5,-0.1)*p40, E, arrowcolor);

 label("$(x,y,z)$", p50, E, fontsize(10pt));

 //

 real s = 0.175;

 label("$0$", shift(0.3,0)*q20, E, stepcolor);
 label("$1$", shift(-0.3,0)*q21, W, stepcolor);

 label("$0$", shift(0.2,0)*q30, E, stepcolor);
 label("$1$", shift(-0.2,0)*q31, W, stepcolor);

 label("$0$", shift(s,0)*q40, E, stepcolor);
 label("$1$", shift(-s,0)*q41, W, stepcolor);
 label("$0$", shift(s,0)*q42, E, stepcolor);
 label("$1$", shift(-s,0)*q43, W, stepcolor);

 label("$0$", shift(s,0)*q50, E, stepcolor);
 label("$1$", shift(-s,0)*q51, W, stepcolor);
 label("$0$", shift(s,0)*q52, E, stepcolor);
 label("$1$", shift(-s,0)*q53, W, stepcolor);

 label("$0$", shift(s,0)*q60, E, stepcolor);
 label("$1$", shift(-s,0)*q61, W, stepcolor);
 label("$0$", shift(s,0)*q62, E, stepcolor);
 label("$1$", shift(-s,0)*q63, W, stepcolor);
 label("$0$", shift(s,0)*q64, E, stepcolor);
 label("$1$", shift(-s,0)*q65, W, stepcolor);

 label("$0$", shift(s,0)*q70, E, stepcolor);
 label("$1$", shift(-s,0)*q71, W, stepcolor);
 label("$0$", shift(s,0)*q72, E, stepcolor);
 label("$1$", shift(-s,0)*q73, W, stepcolor);
 label("$0$", shift(s,0)*q74, E, stepcolor);
 label("$1$", shift(-s,0)*q75, W, stepcolor);

 label("$1$", shift(-s,0)*q80, W, stepcolor);
 label("$0$", shift(s,0)*q81, E, stepcolor);
 label("$1$", shift(-s,0)*q82, W, stepcolor);
 label("$1$", shift(-s,0)*q83, W, stepcolor);

 label("$1$", shift(-s,0)*q90, W, stepcolor);

 real r = 0.22;

 safeLabel("$E$", q10, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$+$", q20, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$1$", q21, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$E$", q30, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$T$", q31, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$T$", q40, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q41, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$*$", q42, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$1$", q43, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$F$", q50, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q51, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$T$", q52, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$F$", q53, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$I$", q60, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q61, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$F$", q62, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q63, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$I$", q64, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q65, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$a$", q70, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q71, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$I$", q72, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q73, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$a$", q74, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q75, r, r, textpen = textcolor, borderpen = bordercolor);

 safeLabel("$x$", q80, r, r, textpen = textcolor, borderpen = black, fillpen = fillcolor);
 safeLabel("$a$", q81, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$0$", q82, r, r, textpen = textcolor, borderpen = bordercolor);
 safeLabel("$z$", q83, r, r, textpen = textcolor, borderpen = black, fillpen = fillcolor);

 safeLabel("$y$", q90, r, r, textpen = textcolor, borderpen = black, fillpen = fillcolor);

 \end{asy}
\end{center}
                                  %
                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
The signature used in the \texttt{applicate} mapping is shortformed to $ (\ldots) $
for clarity, as it is otherwise quite large given the bijective nature of the mapping.

This demonstrated correspondence is actually quite intriguing: From a function semantics perspective, it suggests
context-free grammars are the logical end result of \strong{currying}. Admittedly I did not make such a connection
myself until I translated this derivation, but it makes sense actually: From a function semantics perspective both
recursion and \emph{currying} are the first natural extensions of composition, and so any induction paradigm
based off of composition would inevitably lead to their use as well.

\subsection*{Context-Sensitive Induction}

We are finally ready to move beyond context-free induction. If we were to continue following the conventional
narrative from automata theory, the next level of languages to explore would be \strong{context-sensitive languages}
with their corresponding \strong{linear bounded automata}.

For our purposes this is unnecessary. In the traditional theory linear bounded automata could be considered restricted forms
of Turing machines: They are \emph{bounded linearly} in the amount of memory available to them relative to the memory needed
for a given string input---and are otherwise built the same way. This means anything that can be derived as true for Turing
machines also holds for linear bounded automata with enough memory. This makes their pedagogical exposition mostly redundant.

To be fair, linear bounded automata are actually better approximations to the hardware implementations of Turing machines,
but we will admittedly overlook them here regardless and immediately move on to the next level of induction.

\subsection*{Register Induction}

We are now at the highest level of languages for our induction operators.

\subsubsection*{Turing Machines}

This level of languages is primarily represented by what are known as \emph{Turing machines}. These devices can still
be viewed as \emph{finite state machines}, except here the finite state is the set of \emph{instructions} (source code)
for the given machine. Otherwise, the machine has access to a potentially infinite supply of memory similar to that of
pushdown automata---but unlike such automata, Turing machine memory isn't a stack, it is \emph{random access}.

Turing machines are theoretically important for two major reasons: 1) They are known to be equipotent to the class of
\emph{computable functions} defined in mathematics---specifically $ \mu $-recursive functions. 2) As far as
equipotent machines go, they have a fairly intuitive and simple design which makes them ideal for mathematical proofs.

In particular, one of the foundational theorems of computing science is what's known as \strong{the halting problem}:
If we were given the \emph{instruction set} for any specific Turing machine, as well as some initial input (stored in its
memory system), there is no single other Turing machine that could tell us if our specific machine would eventually halt.

This theorem tells us that Turing machines have a fundamental limitation, but unlike regular or context-free languages
which also have limitations (due to their respective pumping lemmas), the halting problem isn't a limitation on which
languages of strings can be verified, it is a limitation on the representation of computable functions themselves.

Another way to understand this is to relate it back to the primary and secondary modelling approaches of design.
This result says that there's no primary \emph{constructive} definition of a computable function, and that
any such definition must be secondary. What this means is that Turing machines, or $ \mu $-recursive functions
are objects which form some primary model that properly contain the \emph{well behaved} computable functions.
These in turn have to be submodelled as the subspace of machines which actually do halt.

As a consequence, we do have a universal grammar, but we have to accept that some of the \emph{expressions} we create
with it might not be the idealized computable functions we seek. What's more: There's no universal way to prove such
expressions as one or the other. It's ad-hoc, one at a time.

\subsubsection*{Register Machines}

Although Turing machines are theoretically important they are also less practical than \emph{register machines}---which are
a known equivalent. It is for this reason we will focus on determining register induction grammars rather than Turing ones.

We can think of register machines as consisting of (potentially infinite) collections of \emph{registers}, along with
a \emph{stack} for convenience. Each machine is described by an instruction set called its \strong{controller}.
For our induction purposes we can interpret each register as a memory cell containing a single function,
though we're getting ahead of ourselves at this point.

Since we seek a grammar for register induction, let's start by observing a dual grammar for register machines.
The following is a well designed collection of instructions described in \cite{sicp} using LISP notation for building
such machines and their controllers:

$$ \def\arraystretch{1.3}
\tab[-3cm] \begin{array}{l}

(\mbox{reg}\ \angscphyp{register}{name})												\\

(\mbox{const}\ \angscphyp{constant}{value})												\\

(\mbox{assign}\ \angscphyp{register}{name}\ (\mbox{reg}\ \angscphyp{register}{name}))							\\

(\mbox{assign}\ \angscphyp{register}{name}\ (\mbox{const}\ \angscphyp{constant}{value}))						\\

(\mbox{assign}\ \angscphyp{register}{name}\ (\mbox{op}\ \angscphyp{operation}{name}\ \angscp{input_1}\ \ldots\ \angscp{input_n})	\\

(\mbox{assign}\ \angscphyp{register}{name}\ (\mbox{label}\ \angscphyp{label}{name}))							\\

(\mbox{test}\ (\mbox{op}\ \angscphyp{operation}{name})\ \angscp{input_1}\ \ldots\ \angscp{input_n})					\\

(\mbox{branch}\ (\mbox{label}\ \angscphyp{label}{name}))										\\

(\mbox{goto}\ (\mbox{label}\ \angscphyp{label}{name}))											\\

(\mbox{goto}\ (\mbox{reg}\ \angscphyp{register}{name}))											\\

(\mbox{save}\ \angscphyp{register}{name})												\\

(\mbox{restore}\ \angscphyp{register}{name})

\end{array} $$
If this looks a lot like the grammar for machine languages it's because it pretty much is: It is an abstraction of
the assembly languages one finds for actual hardware processors.

Such machine grammar is a good place to start, and it helps us obtain a better picture of what we will generally need for
our own induction grammar, but it's also somewhat removed from the underlying endoposes we have been working with for our
past induction operators.\footnote{Technically for our regular induction grammar we were using some \texttt{cposes} as
well, but keeping in line with this narrative that function evaluation corresponds with endopose operators, we could
readily reinterpret \texttt{distem}'s \texttt{cpose} here to be in its proper monadic form.} With that in mind,
we now turn to the final form of function induction grammars within this essay:

$$ \def\arraystretch{1.2}
\begin{array}{llll|lll}
\multicolumn{4}{l}{\bfmbox{memory}\ \ x}		\col[5ex] \bfmbox{composite}\ \ y	\\[1ex]

\bfmbox{closing}	\col[5ex] \bfmbox{call}		\col[5ex] \bfmbox{call}			\col[3ex] \col[5ex]
\bfmbox{open}		\col[5ex] \bfmbox{pass}		\col[5ex] \bfmbox{call}			\\

policy?\bnms{m}{0}	\col[5ex] break\bnms{m}{0}	\col[5ex] f\bnms{m}{0}			\col[3ex] \col[5ex]
policy?\bnms{c}{0}	\col[5ex] next\bnms{c}{0}	\col[5ex] f\bnms{c}{0}			\\
       
policy?\bnms{m}{1}	\col[5ex] break\bnms{m}{1}	\col[5ex] f\bnms{m}{1}			\col[3ex] \col[5ex]
policy?\bnms{c}{1}	\col[5ex] next\bnms{c}{1}	\col[5ex] f\bnms{c}{1}			\\
       
policy?\bnms{m}{2}	\col[5ex] break\bnms{m}{2}	\col[5ex] f\bnms{m}{2}			\col[3ex] \col[5ex]
policy?\bnms{c}{2}	\col[5ex] next\bnms{c}{2}	\col[5ex] f\bnms{c}{2}			\\

\tab[2.5ex] \vdots	\col[7.5ex] \vdots		\col[5.5ex] \vdots  			\col[3ex] \col[5ex]
\tab[2.5ex] \vdots	\col[7.5ex] \vdots		\col[5.5ex] \vdots  			\\

policy?\bnms{m}{n}	\col[5ex] break\bnms{m}{n}	\col[5ex] f\bnms{m}{n}			\col[3ex] \col[5ex]
policy?\bnms{c}{n}	\col[5ex] next\bnms{c}{n}	\col[5ex] f\bnms{c}{n}			\\[1ex]

\multicolumn{4}{l}{\bfmbox{closed}}			\col[5ex] \bfmbox{closed}		\\
\end{array} $$
In a lot of ways this grammatical form is quite straightforward: In appearance at least, it is a simple extension of
our \texttt{distem cpose} grammar, just doubled up. That would be the obvious interpretation, but is in fact misleading,
and for a few reasons.

For one, the parallel \texttt{distem}'s aren't independent here, they can actually communicate. This is to say they not
only build functions, they are able to pass the functions they're implicitly building (or parts thereof) directly to each
other. Aside from that, the other subtlety of this new grammatical form is the realization that we've now extended beyond
\texttt{distem} itself, and in doing so we've possibly opened up new design issues.

For example if you go back to the definition of \texttt{distem} there's more than one way to extend it---leading to
multiple alternative extensions all equally valid and potentially interesting. How do we decide which to use? What
kinds of nomenclatures do we equip such inventories of operators with? Fortunately for us, such issues are \emph{moot}:
There's a reasonably well known automata theorem which says a finite state automata equipped with \emph{two}
stacks is equipotent to a Turing machine.

Lo and behold, this is effectively what we have when we use the above grammatical form: The \strong{memory} is our intended
\emph{stack}, while the \strong{composite} is a set of registers which are used not only to hold the function we're building,
but which hold other objects as a secondary \emph{stack}.

Note, this equates to \emph{random access} memory specified within Turing machines: This works because with two stacks
we could access arbitrary locations within either of the individual stacks by simply shifting everything before
(the given location) onto the other stack, for which we could then read or write to that location, followed by
shifting everything back. In theory this is even how the above grammatical form works, but in practice we would
generally prefer to optimize toward random access performance.

Final note: I had previously introduced the \texttt{assign} and \texttt{alias} operators which allowed us to associate
variables with given values. In theory the memory system used within this two column construct could be implemented
as a reserved portion of the memory stack, simplifying this narrative design nicely as well.\footnote{In practice it
would be more performant to implement such an environmental memory system separately.}

\subsubsection*{Register Binds}

As for translating this grammatical form into its respective \texttt{cposes} (which could in turn be translated into proper
monadic endopositions), their translations are largely straightforward, though here I will use notation I had previously
introduced in the \emph{factorial function} subsubsection:
$$ \def\arraystretch{1}
\begin{array}{lcl}
f\,\ldp g_1,\ g_2\rdp				& := & \ldp f \circ g_1,\ f \circ g_2\rdp		\\
\ldp g_1,\ g_2\rdp\,f				& := & \ldp g_1 \circ f,\ g_2 \circ f\rdp		\\
\ldp f_1,\ f_2\rdp\ \bullet\ \ldp g_1,\ g_2\rdp	& := & \ldp f_1 \circ g_1,\ f_2 \circ g_2\rdp
\end{array} $$
As a reminder, this allows us to extend the idea of a function into what we'd call a \emph{bifunction}.
From there, the necessary \texttt{cpose} operators are fairly straightforward:

$$ \def\arraystretch{1.5}
\small
\begin{array}{lcl}
\tab[0ex] \langle\,policy?\bms{m},\ next\bnms{m}{1},\ next\bnms{m}{2},\ policy?\bms{c},\ next\bnms{c}{1},\ next\bnms{c}{2}\,\rangle

\col[-13ex] \defmcpose{open}{call call}{open}{call call} \col[2ex] \ldp\,cont\bms{m},\ cont\bms{c}\,\rdp		\\[3ex]

:=\,\ldp
\tab[1.5ex]
\distem(\,policy?\bms{m},\ cont\bms{m},\ next\bnms{m}{1},\ -\bms{m},\ cont\bms{m},\ next\bnms{m}{2},\ -\bms{m}\,)
\tab[3ex] ,														\\

\tab[6.75ex]
\distem(\,policy?\bms{c},\ cont\bms{c},\ next\bnms{c}{1},\ -\bms{c},\ cont\bms{c},\ next\bnms{c}{2},\ -\bms{c}\,)
\tab[8ex] \rdp
\end{array} $$
Keep in mind a quick combinatorial analysis suggests there would actually be $ 81 $ \texttt{distem} pairs given there
are $ 9 $ \texttt{distem cposes}. Moreover, we expect the two columns to communicate, for example $ next\bnms{m}{1} $
accepts $ -\bms{m} $ in the above, but we might want it to accept $ -\bms{c} $ as well. This means there are multiplicatively
more variations as well, too many to give here! Fortunately, as the signature problem has shown us we can instead create an
induction operator to do this work for us.

There is one subtlety though: If we want to be able to to shift chain compositions from one side to the other it would be best
to withhold applying the \texttt{force} operators until the final composition is returned. This means we'd secretly be working
with sequences of functions instead of compositions, but this changes little in the way of the theory of this essay.

Finally, with this induction grammar we can now consolidate the previously introduced paradigms: Regular induction
is straightforward, we just don't use the stack side. As for context-free induction? Upon first inspection it seems
more complicated since it doesn't match the vertical paradigm we developed for regular induction. Actually, it's only
a matter of putting restrictions on the columns of this new grammar: In particular we allow ourselves to be able to
pop from the front of the \texttt{memory} column, but we don't allow ourselves to pop or erase any functions from the
\texttt{composite}.

\subsubsection*{The Nature of Non-Halting Functions}

I thought I'd end this section with a mention about the \emph{non-halting threads} which are also part of this tapestry
of register functions. Such threads are now ``in the mix'' so to speak because of our need to accept secondary modelling
grammar for our register induction operators. Since the same grammar allows us to build both computable functions and
ones which don't halt, the question needs to be asked: What is it that actually makes these expressions differ?

For starters, let's we return to the version of the factorial function
that was implemented with quasi-regular induction grammar:
$$ \def\arraystretch{1.2}
\begin{array}{rcl}
n! & \qdefeq	& (1,n)\ [\ \car\ _\circ|_\circ\ \ldp\,\cdot\ ,\,\dec\ \cdr\rdp\,)^{n+1}		\\
   &		& \hspace{7.5ex}\scriptsize \isZero\ \cdr
\end{array} $$
We observed at the time that this was one of our first examples where function construction and evaluation were
no longer independent, noting that the repetition aspect of construction is now intermixed with the evaluation input.
This points to a deeper pattern about recursion, and about the nature of halting: Functions which halt do so in part
because of the input given to them, but also because of \emph{how their construction is intertwined with their evaluation}.

Such thinking potentially provides a best practice approach in determining whether a function halts or not: We start by
considering a hypothetical register function which builds new parts of itself for at least some of the steps of its
evaluation. Next, we would inventory those \emph{runtime} constructed parts for which we would interpret as machine
\emph{states}---accepting that such states might now be infinite. We would then use this information to determine
conditions that could create evaluative loops, and in particular unending loops.\footnote{This is not to say we have
a universal algorithm here---the halting problem negates that possibility---rather it is only meant to support a broader
range of strategies which could be used to mitigate the more complex cases of the halting problem, ones that were
previously inaccessible to existing analytic approaches.}

Actually, since we're talking best practices the better approach would be to first use the regular and context-free
pumping lemmas to rule out a given register function as being known to halt. Then we might try the just discussed
strategy. With that said, it should be acknowledged with humility here that the halting problem is not
an easy one to mitigate in general.

The easiest example to demonstrate the complexities of halting
is the \strong{Collatz conjecture}, where we define the function:
$$ \def\arraystretch{1.3}
\mbox{Collatz}(n) \qdefeq \left\{\begin{array}{ll}
\frac{n}{2}	& \qquad \mbox{if } n \mbox{ is even,}				\\
3n+1		& \qquad \mbox{otherwise.}
\end{array}\right. $$
The conjecture says that for all $ n\in\mathbb{N} $ this function always reaches the value $ 1 $. If we want this
to specifically be a conjecture about halting, we would first observe that by starting with value $ n = 1 $ and computing
the next few values we end up with the following sequence:
$$ 1, 4, 2, 1 $$
This function clearly doesn't halt, but it does repeat ad infinitum in a predictable way, and so we could just as
well modify a version of it that recursively breaks at the value $ 1 $ for every input. The underlying semantics
of the conjecture would not change. Either way---and this is the humbling point being made here---this conjecture
has been known since at least 1937 and in all that time no one has proven or disproven it.

\section*{Afterthoughts}

We've reached the end, and I'd like to share two more thematic ideas as my way of concluding this essay.

First, there was a lot to take in, in terms of this document being 52 pages. I'd like to summarize the major
concepts presented here, but instead of just rehashing what was already said let's interpret it from the perspective
of \emph{skill development}, and the stages one goes through when becoming language fluent as a programmer and designer:
\begin{enumerate}
\item When we learn programming initially, we learn grammar to build functions to manipulate objects, and that's good.
\item Next, we often take things to a ``higher order'' level with functional programming, where the functions we build
      can now manipulate other functions, and that's good too.
\item From there, we learn things like type theory which help us to get better at function design, as they guide us
      in making sure our functions are well formed---of course any experienced coder can tell you error messages
      and debuggers teach us these things as well.
\item Beyond that, we take abstraction to a new level with things like category theory, where we learn how to recognize
      and prove grammars which can be used to systematically build further grammars. For example if we have some initial
      grammar with some properties, we might have a theorem (monadic constructions come to mind) that tell us
      we (or our compilers) can then build other well-behaved functions or grammars.
\end{enumerate}
This sequence of developmental stages is missing a key component: If we look at all possible computable functions we can
build, even having theorems about how grammatical constructs relate and can be built from each other, we might still want
to know how such a space of computable functions can be organized, characterized, categorized, classified. Current
pedagogies don't always make this clear.

With that said, this narrative staging is not intended to put this theory of function induction as the peak of programming.
In fact it is most naturally an extension of what's known as \strong{concatenative programming} which in many ways is
considered a low level approach (before stage two) to building higher order functions.

This brings me to my second thematic idea: Most people don't code in assembly, and in practice people might not want to
code in what is otherwise low level register induction grammar either. In that case, these lower level grammars provide
important theoretical foundations, but in the long run we might want to return to higher level paradigms such as functional
programming which have more user-friendly interfaces.

Then again, we should take a step back and consider what that might mean: It would mean we are at a point of abstraction where
we are building induction grammars that build induction grammars! The absurd thing about it is that such a consideration
isn't even absurd any more. The conclusion here must be that none of this essay is the end of things, and all of it is
only the beginning.

Finally, I would like to acknowledge the theories (with their plethora of concepts, definitions, theorems, and proofs)
for which this essay could not exist without: Type Theory, Category Theory, Automata Theory, Lambda Calculus. These
endeavours represent the hard work of many very smart people over many years. Thank you.\\[0.25cm]

Pijariiqpunga.

\newpage

\section*{Appendix: CPose Optimizations}

\subsection*{Stem CPose}

\ \\
$$ \def\arraystretch{1.55}
\begin{array}{llrcl}
\hline																\\[-0.25cm]

\langle\,true?,\ break,\ next\,\rangle			& \spose{call call}							&
cont & \qdefeq						& \stem(\,true?,\ break,\ -_{arg},\ cont,\ next,\ -_{arg}\,)		\\

&&& \Downarrow															\\

\langle\,true?,\ break\,\rangle				& \spose{call id}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ -_{arg},\ cont,\ -_{arg}\,)			\\

\langle\,true?,\ next\,\rangle				& \spose{id call}							&
cont & \qdefeq						& \pend(\,true?,\ -_{arg},\ cont,\ next,\ -_{arg}\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ break,\ x\,\rangle			& \spose{call pass}							&
cont & \qdefeq						& \stem(\,true?,\ break,\ -_{arg},\ cont,\ -_{arg},\ x\,)		\\

&&& \Downarrow															\\

\langle\,true?,\ break\,\rangle				& \spose{call id}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ -_{arg},\ cont,\ -_{arg}\,)			\\

\langle\,true?,\ x\,\rangle				& \spose{id pass}							&
cont & \qdefeq						& \pend(\,true?,\ -_{arg},\ cont,\ -_{arg},\ x\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ break,\ next,\ x\,\rangle		& \spose{call pose}							&
cont & \qdefeq						& \stem(\,true?,\ break,\ -_w,\ cont,\ next,\ x\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ break,\ next\,\rangle			& \spose{call $-$ id}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ -_w,\ cont,\ next\,)			\\

\langle\,true?,\ break,\ x\,\rangle			& \spose{call id $-$}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ -_w,\ cont,\ x\,)				\\

\langle\,true?,\ next,\ x\,\rangle			& \spose{id pose}							&
cont & \qdefeq						& \pend(\,true?,\ -_w,\ cont,\ next,\ x\,)				\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ w,\ next\,\rangle			& \spose{pass call}							&
cont & \qdefeq						& \stem(\,true?,\ -_{arg},\ w,\ cont,\ next,\ -_{arg}\,)		\\

&&& \Downarrow															\\

\langle\,true?,\ w\,\rangle				& \spose{pass id}							&
cont & \qdefeq						& \dihold(\,true?,\ -_{arg},\ w,\ cont,\ -_{arg}\,)			\\

\langle\,true?,\ next\,\rangle				& \spose{id call}							&
cont & \qdefeq						& \pend(\,true?,\ -_{arg},\ cont,\ next,\ -_{arg}\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ w,\ x\,\rangle				& \spose{pass pass}							&
cont & \qdefeq						& \stem(\,true?,\ -_{arg},\ w,\ cont,\ -_{arg},\ x\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ w\,\rangle				& \spose{pass id}							&
cont & \qdefeq						& \dihold(\,true?,\ -_{arg},\ w,\ cont,\ -_{arg}\,)			\\

\langle\,true?,\ x\,\rangle				& \spose{id pass}							&
cont & \qdefeq						& \pend(\,true?,\ -_{arg},\ cont,\ -_{arg},\ x\,)			\\[0.5cm]

\hline
\end{array} $$

\vspace{\fill}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% next page

\newpage

$$ \def\arraystretch{1.55}
\begin{array}{llrcl}

\langle\,true?,\ w,\ next,\ x\,\rangle			& \spose{pass pose}							&
cont & \qdefeq						& \stem(\,true?,\ -_{break},\ w,\ cont,\ next,\ x\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ w,\ next\,\rangle			& \spose{pass $-$ id}							&
cont & \qdefeq						& \dihold(\,true?,\ -_{break},\ w,\ cont,\ next\,)			\\

\langle\,true?,\ w,\ x\,\rangle				& \spose{pass id $-$}							&
cont & \qdefeq						& \dihold(\,true?,\ -_{break},\ w,\ cont,\ x\,)				\\

\langle\,true?,\ next,\ x\,\rangle			& \spose{id pose}							&
cont & \qdefeq						& \pend(\,true?,\ -_{break},\ cont,\ next,\ x\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ break,\ w,\ next\,\rangle		& \spose{pose call}							&
cont & \qdefeq						& \stem(\,true?,\ break,\ w,\ cont,\ next,\ -_x\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ break,\ w\,\rangle			& \spose{pose id}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ w,\ cont,\ -_x\,)				\\

\langle\,true?,\ break,\ next\,\rangle			& \spose{$-$ id call}							&
cont & \qdefeq						& \pend(\,true?,\ break,\ cont,\ next,\ -_x\,)				\\

\langle\,true?,\ w,\ next\,\rangle			& \spose{id $-$ call}							&
cont & \qdefeq						& \pend(\,true?,\ w,\ cont,\ next,\ -_x\,)				\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ break,\ w,\ x\,\rangle			& \spose{pose pass}							&
cont & \qdefeq						& \stem(\,true?,\ break,\ w,\ cont,\ -_{next},\ x\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ break,\ w\,\rangle			& \spose{pose id}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ w,\ cont,\ -_{next}\,)			\\

\langle\,true?,\ break,\ x\,\rangle			& \spose{$-$ id pass}							&
cont & \qdefeq						& \pend(\,true?,\ break,\ cont,\ -_{next},\ x\,)			\\

\langle\,true?,\ w,\ x\,\rangle				& \spose{id $-$ pass}							&
cont & \qdefeq						& \pend(\,true?,\ w,\ cont,\ -_{next},\ x\,)				\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ break,\ w,\ next,\ x\,\rangle		& \spose{pose pose}							&
cont & \qdefeq						& \stem(\,true?,\ break,\ w,\ cont,\ next,\ x\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ break,\ w,\ next\,\rangle		& \spose{pose $-$ id}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ w,\ cont,\ next\,)				\\

\langle\,true?,\ break,\ w,\ x\,\rangle			& \spose{pose id $-$}							&
cont & \qdefeq						& \dihold(\,true?,\ break,\ w,\ cont,\ x\,)				\\

\langle\,true?,\ break,\ next,\ x\,\rangle		& \spose{$-$ id pose}							&
cont & \qdefeq						& \pend(\,true?,\ break,\ cont,\ next,\ x\,)				\\

\langle\,true?,\ w,\ next,\ x\,\rangle			& \spose{id $-$ pose}							&
cont & \qdefeq						& \pend(\,true?,\ w,\ cont,\ next,\ x\,)				\\

\langle\,true?,\ break,\ next\,\rangle			& \spose{$-$ id $-$ id}							&
cont & \qdefeq						& \hold(\,true?,\ break,\ cont,\ next\,)				\\

\langle\,true?,\ break,\ x\,\rangle			& \spose{$-$ id id $-$}							&
cont & \qdefeq						& \hold(\,true?,\ break,\ cont,\ x\,)					\\

\langle\,true?,\ w,\ next\,\rangle			& \spose{id $-$ $-$ id}							&
cont & \qdefeq						& \hold(\,true?,\ w,\ cont,\ next\,)					\\

\langle\,true?,\ w,\ x\,\rangle				& \spose{id $-$ id $-$}							&
cont & \qdefeq						& \hold(\,true?,\ w,\ cont,\ x\,)					\\[0.5cm]

\hline
\end{array} $$

\newpage

\subsection*{Costem CPose}

\ \\
$$ \def\arraystretch{1.55}
\begin{array}{llrcl}
\hline																\\[-0.25cm]

\langle\,true?,\ next,\ break\,\rangle			& \cpose{call call}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ next,\ -_{arg},\ break,\ -_{arg}\,)		\\

&&& \Downarrow															\\

\langle\,true?,\ next\,\rangle				& \cpose{call id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ -_{arg},\ -_{arg}\,)			\\

\langle\,true?,\ break\,\rangle				& \cpose{id call}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ -_{arg},\ break,\ -_{arg}\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next,\ w\,\rangle			& \cpose{call pass}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ next,\ -_{arg},\ -_{arg},\ w\,)		\\

&&& \Downarrow															\\

\langle\,true?,\ next\,\rangle				& \cpose{call id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ -_{arg},\ -_{arg}\,)			\\

\langle\,true?,\ w\,\rangle				& \cpose{id pass}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ -_{arg},\ -_{arg},\ w\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next,\ break,\ w\,\rangle		& \cpose{call pose}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ next,\ -_x,\ break,\ w\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ next,\ break\,\rangle			& \cpose{call $-$ id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ -_x,\ break\,)			\\

\langle\,true?,\ next,\ w\,\rangle			& \cpose{call id $-$}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ -_x,\ w\,)				\\

\langle\,true?,\ break,\ w\,\rangle			& \cpose{id pose}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ -_x,\ break,\ w\,)				\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ x,\ break\,\rangle			& \cpose{pass call}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ -_{arg},\ x,\ break,\ -_{arg}\,)		\\

&&& \Downarrow															\\

\langle\,true?,\ x\,\rangle				& \cpose{pass id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ -_{arg},\ x,\ -_{arg}\,)			\\

\langle\,true?,\ break\,\rangle				& \cpose{id call}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ -_{arg},\ break,\ -_{arg}\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ x,\ w\,\rangle				& \cpose{pass pass}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ -_{arg},\ x,\ -_{arg},\ w\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ x\,\rangle				& \cpose{pass id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ -_{arg},\ x,\ -_{arg}\,)			\\

\langle\,true?,\ w\,\rangle				& \cpose{id pass}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ -_{arg},\ -_{arg},\ w\,)			\\[0.5cm]

\hline
\end{array} $$

\vspace{\fill}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% next page

\newpage

$$ \def\arraystretch{1.55}
\begin{array}{llrcl}

\langle\,true?,\ x,\ break,\ w\,\rangle			& \cpose{pass pose}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ -_{next},\ x,\ break,\ w\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ x,\ break\,\rangle			& \cpose{pass $-$ id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ -_{next},\ x,\ break\,)			\\

\langle\,true?,\ x,\ w\,\rangle				& \cpose{pass id $-$}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ -_{next},\ x,\ w\,)				\\

\langle\,true?,\ break,\ w\,\rangle			& \cpose{id pose}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ -_{next},\ break,\ w\,)			\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next,\ x,\ break\,\rangle		& \cpose{pose call}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ next,\ x,\ break,\ -_w\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ next,\ x\,\rangle			& \cpose{pose id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ x,\ -_w\,)				\\

\langle\,true?,\ next,\ break\,\rangle			& \cpose{$-$ id call}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ next,\ break,\ -_w\,)			\\

\langle\,true?,\ x,\ break\,\rangle			& \cpose{id $-$ call}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ x,\ break,\ -_w\,)				\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next,\ x,\ w\,\rangle			& \cpose{pose pass}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ next,\ x,\ -_{break},\ w\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ next,\ x\,\rangle			& \cpose{pose id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ x,\ -_{break}\,)			\\

\langle\,true?,\ next,\ w\,\rangle			& \cpose{$-$ id pass}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ next,\ -_{break},\ w\,)			\\

\langle\,true?,\ x,\ w\,\rangle				& \cpose{id $-$ pass}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ x,\ -_{break},\ w\,)				\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next,\ x,\ break,\ w\,\rangle		& \cpose{pose pose}							&
cont & \qdefeq						& \costem(\,true?,\ cont,\ next,\ x,\ break,\ w\,)			\\

&&& \Downarrow															\\

\langle\,true?,\ next,\ x,\ break\,\rangle		& \cpose{pose $-$ id}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ x,\ break\,)				\\

\langle\,true?,\ next,\ x,\ w\,\rangle			& \cpose{pose id $-$}							&
cont & \qdefeq						& \copend(\,true?,\ cont,\ next,\ x,\ w\,)				\\

\langle\,true?,\ next,\ break,\ w\,\rangle		& \cpose{$-$ id pose}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ next,\ break,\ w\,)				\\

\langle\,true?,\ x,\ break,\ w\,\rangle			& \cpose{id $-$ pose}							&
cont & \qdefeq						& \dihold(\,true?,\ cont,\ x,\ break,\ w\,)				\\

\langle\,true?,\ next,\ break\,\rangle			& \cpose{$-$ id $-$ id}							&
cont & \qdefeq						& \cohold(\,true?,\ cont,\ next,\ break\,)				\\

\langle\,true?,\ next,\ w\,\rangle			& \cpose{$-$ id id $-$}							&
cont & \qdefeq						& \cohold(\,true?,\ cont,\ next,\ w\,)					\\

\langle\,true?,\ x,\ break\,\rangle			& \cpose{id $-$ $-$ id}							&
cont & \qdefeq						& \cohold(\,true?,\ cont,\ x,\ break\,)					\\

\langle\,true?,\ x,\ w\,\rangle				& \cpose{id $-$ id $-$}							&
cont & \qdefeq						& \cohold(\,true?,\ cont,\ x,\ w\,)					\\[0.5cm]

\hline
\end{array} $$

\newpage

\subsection*{Distem CPose}

\ \\
$$ \def\arraystretch{1.55}
\begin{array}{llrcl}
\hline																\\[-0.25cm]

\langle\,true?,\ next_1,\ next_2\,\rangle		& \dpose{call call}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ -_{arg},\ cont,\ next_2,\ -_{arg}\,)\\

&&& \Downarrow															\\

\langle\,true?,\ next_1\,\rangle			& \dpose{call id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ -_{arg},\ cont,\ -_{arg}\,)		\\

\langle\,true?,\ next_2\,\rangle			& \dpose{id call}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ -_{arg},\ cont,\ next_2,\ -_{arg}\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next_1,\ x_2\,\rangle			& \dpose{call pass}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ -_{arg},\ cont,\ -_{arg},\ x_2\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ next_1\,\rangle			& \dpose{call id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ -_{arg},\ cont,\ -_{arg}\,)		\\

\langle\,true?,\ x_2\,\rangle				& \dpose{id pass}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ -_{arg},\ cont,\ -_{arg},\ x_2\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next_1,\ next_2,\ x_2\,\rangle		& \dpose{call pose}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ -_{x_1},\ cont,\ next_2,\ x_2\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ next_1,\ next_2\,\rangle		& \dpose{call $-$ id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ -_{x_1},\ cont,\ next_2\,)		\\

\langle\,true?,\ next_1,\ x_2\,\rangle			& \dpose{call id $-$}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ -_{x_1},\ cont,\ x_2\,)		\\

\langle\,true?,\ next_2,\ x_2\,\rangle			& \dpose{id pose}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ -_{x_1},\ cont,\ next_2,\ x_2\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ x_1,\ next_2\,\rangle			& \dpose{pass call}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ -_{arg},\ x_1,\ cont,\ next_2,\ -_{arg}\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ x_1\,\rangle				& \dpose{pass id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ -_{arg},\ x_1,\ cont,\ -_{arg}\,)		\\

\langle\,true?,\ next_2\,\rangle			& \dpose{id call}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ -_{arg},\ cont,\ next_2,\ -_{arg}\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ x_1,\ x_2\,\rangle			& \dpose{pass pass}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ -_{arg},\ x_1,\ cont,\ -_{arg},\ x_2\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ x_1\,\rangle				& \dpose{pass id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ -_{arg},\ x_1,\ cont,\ -_{arg}\,)		\\

\langle\,true?,\ x_2\,\rangle				& \dpose{id pass}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ -_{arg},\ cont,\ -_{arg},\ x_2\,)		\\[0.5cm]

\hline
\end{array} $$

\vspace{\fill}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% next page

\newpage

$$ \def\arraystretch{1.55}
\begin{array}{llrcl}
\langle\,true?,\ x_1,\ next_2,\ x_2\,\rangle		& \dpose{pass pose}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ -_{next_1},\ x_1,\ cont,\ next_2,\ x_2\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ x_1,\ next_2\,\rangle			& \dpose{pass $-$ id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ -_{next_1},\ x_1,\ cont,\ next_2\,)		\\

\langle\,true?,\ x_1,\ x_2\,\rangle			& \dpose{pass id $-$}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ -_{next_1},\ x_1,\ cont,\ x_2\,)		\\

\langle\,true?,\ next_2,\ x_2\,\rangle			& \dpose{id pose}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ -_{x_1},\ cont,\ next_2,\ x_2\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next_1,\ x_1,\ next_2\,\rangle		& \dpose{pose call}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ next_2,\ -_{x_2}\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ next_1,\ x_1\,\rangle			& \dpose{pose id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ -_{x_2}\,)		\\

\langle\,true?,\ next_1,\ next_2\,\rangle		& \dpose{$-$ id call}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ next_1,\ cont,\ next_2,\ -_{x_2}\,)		\\

\langle\,true?,\ x_1,\ next_2\,\rangle			& \dpose{id $-$ call}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ x_1,\ cont,\ next_2,\ -_{x_2}\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next_1,\ x_1,\ x_2\,\rangle		& \dpose{pose pass}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ -_{next_2},\ x_2\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ next_1,\ x_1\,\rangle			& \dpose{pose id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ -_{next_2}\,)		\\

\langle\,true?,\ next_1,\ x_2\,\rangle			& \dpose{$-$ id pass}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ next_1,\ cont,\ -_{next_2},\ x_2\,)		\\

\langle\,true?,\ x_1,\ x_2\,\rangle			& \dpose{id $-$ pass}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ x_1,\ cont,\ -_{next_2},\ x_2\,)		\\[0.5cm]

\hline																\\[-0.25cm]

\langle\,true?,\ next_1,\ x_1,\ next_2,\ x_2\,\rangle	& \dpose{pose pose}							&
cont & \qdefeq						& \distem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ next_2,\ x_2\,)	\\

&&& \Downarrow															\\

\langle\,true?,\ next_1,\ x_1,\ next_2\,\rangle		& \dpose{pose $-$ id}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ next_2\,)		\\

\langle\,true?,\ next_1,\ x_1,\ x_2\,\rangle		& \dpose{pose id $-$}							&
cont & \qqeq						& \costem(\,true?,\ cont,\ next_1,\ x_1,\ cont,\ x_2\,)			\\

\langle\,true?,\ next_1,\ next_2,\ x_2\,\rangle		& \dpose{$-$ id pose}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ next_1,\ cont,\ next_2,\ x_2\,)		\\

\langle\,true?,\ x_1,\ next_2,\ x_2\,\rangle		& \dpose{id $-$ pose}							&
cont & \qqeq						& \stem(\,true?,\ cont,\ x_1,\ cont,\ next_2,\ x_2\,)			\\

\langle\,true?,\ next_1,\ next_2\,\rangle		& \dpose{$-$ id $-$ id}							&
cont & \qqeq						& \dihold(\,true?,\ cont,\ next_1,\ cont,\ next_2\,)			\\

\langle\,true?,\ next_1,\ x_2\,\rangle			& \dpose{$-$ id id $-$}							&
cont & \qqeq						& \dihold(\,true?,\ cont,\ next_1,\ cont,\ x_2\,)			\\

\langle\,true?,\ x_1,\ next_2\,\rangle			& \dpose{id $-$ $-$ id}							&
cont & \qqeq						& \dihold(\,true?,\ cont,\ x_1,\ cont,\ next_2\,)			\\

\langle\,true?,\ x_1,\ x_2\,\rangle			& \dpose{id $-$ id $-$}							&
cont & \qqeq						& \dihold(\,true?,\ cont,\ x_1,\ cont,\ x_2\,)				\\[0.5cm]

\hline
\end{array} $$

\newpage

\begin{thebibliography}{99}
\bibitem{hott} Homotopy Type Theory: Univalent Foundations of Mathematics. The Univalent Foundations Program (2013). 
\bibitem{nikfs} D.~Nikpayuk. Toward the Semantic Reconstruction of Mathematical Functions (2020).\\
    https://github.com/Daniel-Nikpayuk/LaTeX/blob/main/Mathematics/Essays/Function\%20Semantics/Version-Two/semantics.pdf
\bibitem{iatlc} J.E.~Hopcroft, R.~Motwani, J.D.~Ullman. Introduction to Automata Theory, Languages, and Computation
               (second edition). Addison-Wesley Publishing (2001).
\bibitem{sicp} H.~Abelson, G.J.~Sussman. Structure and Interpretation of Computer Programs (second edition).
               The Massachusetts Institute of Technology Press (1996).
\bibitem{ctic} E.~Riehl. Category Theory in Context. Dover Publications, Inc.~(2016).
\end{thebibliography}

\end{document}

